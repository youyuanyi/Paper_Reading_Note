# 网络模型笔记

## ResNet

### 论文地址

[ResNet]: file://E:/研究生学习/论文/1512.03385%20ResNet.pdf

### 期刊/会议

CVPR 2016

### Title

**Deep Residual Learning for Image Recognition**

### Author

**Kaiming He**; Xiangyu Zhang； Shaoqing Ren； Jian Sun

### Introduction

深度学习可以通过**加深网络层数**来**得到不同level（级别）的feature**，比如低级的视觉特征或者高级的语义特征。

#### 提出问题

**加深网络层数**会**导致梯度爆炸/梯度消失**的问题。尽管可以通过**挑选适合的初始化权重和归一化层（BN）来缓解**，使得数十层的网络能够收敛，但是**网络性能会趋向于饱和，并且会突然迅速下降**。而**原因不是因为层数增多，模型变复杂了导致的过拟合，而是由于训练误差和测试误差都变高了**。（对应Fig.4，可以看出训练误差和测试误差都很差)

一种解决思想是在一个效果还可以的**浅层网络后面添加Identity Mapping**（恒等映射：即输入为x，输出仍然为x），**理论上这样一个深的网络效果不应该比浅的网络效果差**。但是**实际情况中SGD是找不到这样的映射的**。

#### 提出方法

这篇文章提出了显示地构造identity mapping，将其称为**deep residual learning framework**。

<img src=".\img\resnet\残差块.png" alt="残差块" style="zoom:80%;" />

**此时我们希望模型要拟合的是H(x)，这样在新添加的层中，模型不是直接去学H(x)，而是去学F(x)=H(x)-x。x是上一层的输入。这样，新添加的层就不用去重新学习x中已经学到的内容，而是去学习x学到的东西和真实值之间的残差F(x)。这样就把优化目标从H(X)变为了H(x)-x**

在实现中，使用的就是**shortcut**，**它不会增加参数和模型复杂度**。



### Deep Residual Learning

#### 残差连接如何处理输入和输出的形状是不同的情况

- 第一种方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加。
- 第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做**1x1的卷积层**，这个卷积层的**特点是不学习任何空间维度的特征，主要是在通道维度上做融合和改变**。所以只要选取一个1x1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上

### Implemention细节

- 把短边随机采样到[256,480]，这样是为了增强网络的鲁棒性和泛化能力。这种随机采样的方式可以帮助网络**更好地适应不同尺寸的输入图像，从而提高其对于尺度变化的适应性。同时，这也是一种数据增强**
- **每一个pixel的均值都减掉了。其主要思想是将输入图像的每个像素减去一个全局平均值。这个全局平均值通常是在训练数据集上计算得到的，它代表了整个数据集图像像素的平均值。这样可以消除输入图像中的一些常见的光照变化，还有助于将输入数据的分布调整到一个更接近零均值的状态，这对于一些优化算法（如梯度下降）的稳定性和收敛速度都有积极的影响**
- 使用了**颜色的增强**（AlexNet上用的是PCA，现在我们所使用的是比较简单的RGB上面的，调节各个地方的亮度、饱和度等）
- 使用了**BN**（batch normalization）
- 所有的权重全部是跟另外一个paper中的一样（作者自己的另外一篇文章）。注意写论文的时候，尽量能够让别人不要去查找别的文献就能够知道你所做的事情
- **批量大小是56，学习率是0.1，然后每一次当错误率比较平的时候除以10**
- 模型训练了60*10^4个批量。建议最好不要写这种iteration，因为他跟批量大小是相关的，如果变了一个批量大小，他就会发生改变，所以现在一般会说迭代了多少遍数据，相对来说稳定一点
- weith decay=0.0001, 动量为0.9
- 这里没有使用dropout，因为没有全连接层，所以dropout没有太大作用
- 在测试的时候使用了标准的10个crop testing（给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后在每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。
- 采样的时候是在不同的分辨率上去做采样，这样在测试的时候做的工作量比较多，但是在实际过程中使用比较少

### Residual结构

<img src=".\img\resnet\ResNet-residual结构.png" alt="ResNet-residual结构" style="zoom:80%;" />

### Architectures

#### 总体结构

<img src=".\img\resnet\ResNet结构.png" alt="ResNet结构" style="zoom:80%;" />

#### ResNet-34 结构

<img src=".\img\resnet\ResNet-34.png" alt="ResNet结构" style="zoom:80%;" />

- 从layer2开始，stride=2。
- 虚线的残差分支，代表这个block的残差分支要采用1x1卷积核，将残差分支的通道数扩大4倍。同时虚线部分的主干分支负责将高和宽缩小一半，通道数变大4倍

#### ResNet-50结构

<img src=".\img\resnet\ResNet-50结构.png" alt="ResNet-50结构" style="zoom:80%;" />

### Experiments

#### 先比较了没有带残差的（即plain的）18-layer和34-layer

<img src=".\img\resnet\ResNet-plain.png" alt="ResNet-plain" style="zoom:80%;" />

- 细线代表训练误差，粗线代表验证误差。左图是没有使用残差结构的18-layer和34-layer。
- 因为训练集做了数据增强，相当于添加了噪声，而验证集没有做数据增强，所以一开始训练误差相较于验证误差较大。
- 图中，**误差断崖式下降的地方是因为学习率的下降**。
- 上图主要是想说明**在有残差连接的时候，34比28要好；另外对于34来说，有残差连接会好很多**；其次，有了残差连接以后，收敛速度会快很多，核心思想是说，**在所有的超参数都一定的情况下，有残差的连接收敛会快，而且后期会好**



#### 处理输入输出尺寸不同

<img src=".\img\resnet\ResNet挑选projection.png" alt="ResNet挑选projection" style="zoom:50%;" />

- 方案A表示填0
- 方案B表示在输入和输出不同时做投影
- 方案C表示全部做投影
- 由于B和C效果差不多，且B的计算复杂度更高，所以作者采用了方案C

#### Deeper Bottleneck Architecture

<img src=".\img\resnet\ResNet-deeper-resnet.png" alt="ResNet-deeper-resnet" style="zoom:80%;" />

- 如果要做50层以上的设计，则会引入bottleneck design
- 左图是之前的设计，当通道数是64的时候，通道数不会改变
- **如果要做到比较深的话，可以学到更多的模式，可以把通道数变得更大，右图从64变到了256**
- 当通道数变得更大的时候，**计算复杂度成平方关系增加，这里先通过1个1*1的卷积，将256维投影回到64维，然后再做通道数不变的卷积，然后再投影回256（将输入和输出的通道数进行匹配，便于进行对比）。**等价于先对特征维度降一次维，在降一次维的上面再做一个空间上的东西，然后再投影回去
- 虽然通道数是之前的4倍，但是在这种设计之下，二者的算法复杂度是差不多的

### 一些杂谈

- 在整个残差连接，如果后面新加上的层不能让模型变得更好的时候，是因为有残差连接的存在，**新加的那些层应该是不会学到任何东西，应该都是靠近0的**，这样就等价于就算是训练了1000层的ResNet，但是可能就前100层有用，后面的900层基本上因为没有什么东西可以学的，基本上就不会动了
- **ResNet在前面层学习数据主要的特征，通过shortcut传递给后面的层。越到后面，后面的层学习的东西就越少越次要了。**
- **如作者所说，不加残差连接的时候，理论上也能够学出一个有一个identity的东西，但是实际上做不到，因为没有引导整个网络这么走的话，其实理论上的结果它根本无法按这个方向优化，所以一定是得手动的把这个结果加进去，使得它更容易训练出一个简单的模型来拟合数据的情况下，等价于把模型的复杂度降低了**







## Transformer

### 论文地址

[Transformer]: file://E:/研究生学习/论文/1706.03762Transformer.pdf

### 期刊/会议

NeurIPS会议 2020

### Title

Attention Is All You Need

### Author

Ashish Vaswan

### Introduction

#### 介绍了RNN的缺陷

如果输入是一个sequence，那么RNN只能从左往右一个词一个词计算。对第t个词，会计算一个输出h_t。h_t是由前一个隐藏状态h_t-1和当前第t个词决定的。这样RNN就能保留之前的信息。

问题1：RNN缺少并行度，计算效率差

问题2：无法捕捉和保留长距离依赖。除非h_t很大，但这样也会导致内存开销特别大。

#### 早前工作

已经有过使用attention技术的工作，但大部分仍然是和RNN结合使用

#### 工作贡献

提出了Transformer，没有使用RNN和CNN，完全依赖于注意力机制来学习全局依赖关系，具有很好的并行度。



### Related Work

- 以前的工作主要用CNN作为basic building block来替代RNN，但仍然无法很好的捕捉长距离依赖。因为卷积计算的时候看一个比较小的窗口，例如一个 3 * 3 窗口，如果 2 个像素隔得比较远，需要用很多 3 * 3 的卷积层、一层一层的叠加上去，才能把隔得很远的 2个像素联系起来。计算量也随着两个像素位置距离的增大而显著变大。
- 在Transformer中，能一次看到所有的输入，和输入的长度无关
- 由于CNN具有多输出通道特点，每个通道可以学习不同的模式，所以在Transformer中使用了Multi-head Attention模拟这个效果。

讲述了和本文相关的工作（自己的或别人的），联系是什么，区别是什么

### Model

在大多数Encoder-Decoder模型中，模型的每一步都是自回归的（即输出是一个一个生成的），在生成下一个时将先前生成的符号用作附加输入。

这样Encoder可能可以一次看到整个句子，然后Decoder在解码时，只能一个一个的生成。

<img src=".\img\Transformer\Transformer模型结构.png" alt="Transformer模型结构" style="zoom:80%;" />

#### 为什么用LN而不是BN？

<img src=".\img\Transformer\BN原理.png" alt="BN原理" style="zoom:80%;" />

BN是把每一列（1个特征）放在一个mini-batch中，将均值变为0，方差变为1，具体做法为（该列向量 - mini-batch 该列向量的均值）/（mini-batch 该列向量的方差）。

BN还会学习一个λ和β，通过学习这两个参数将向量放缩成任意均值、方差的一个向量。



LN

LN是把每一行（1一个batch）的均值变为0，方差变为1.

LayerNorm 相当于把整个数据转置一次，放到 BatchNorm 里面出来的结果，再转置回去，基本上可以得到LayerNorm的结果（只针对二维输入）。 



但是输入通常为3维的（batch，长度n，特征数d)，此时切出来的结果分别为

<img src=".\img\Transformer\LN和BN对比.png" alt="LN和BN对比" style="zoom: 80%;" />

**对于BN，如果样本长度n变化较大，每次做mini-batch计算出来的均值和方差抖动较大**。且预测时**需要记录全局的均值和方差**，如果一个新的预测样本特别长，那么在训练时可能没见过，那么训练时计算的均值和方差就可能没用。

而对于LN，它是对每一个样本单独计算均值和方差，不需要存全局的均值和方差，会更稳定一些。



#### **为什么要用Masked-Multi Head Attention？**

因为Decoder是auto-regressive。当前时刻的输入集是之前一些时刻的输出。做预测时，decoder不能看到后民间的输出。因为attention会一次看到完整的输入，所以要加上Mask

#### Attention

一句话来说，values的权重是由query和对应的key计算得到。这样，key和value能保持不变，通过不同的query就能得到不同的weight，使得outputs不一样。

详情见李宏毅机器学习2022

#### Scaled Dot-Product Attention

queries和keys都是d_k维的，values是d_v维的。

计算query和所有keys的Dot-Product，然后除以根号d_k，最后使用softmax函数计算得到权重

##### 为什么使用内积

因为内积代表两个向量的余弦值大小，内积=1，表示两个向量平行，相似度很大；如果内积=0，则表示两个向量正交，相似度不大。

##### 实际操作

将所有的queries封装为一个矩阵Q，同理得到矩阵K和V
$$
Attention(Q,K,V)=softmax(\frac{QK^\top}{\sqrt{d_k}})V\\
假设Q是一个n*d_K的矩阵，K是一个m*d_k的矩阵，V是一个m*d_v的矩阵\\
QK^\top得到一个n*m的矩阵，每一行代表每一个query对每一个key的点积的值\\
除以\sqrt{d_k}是为了防止点击过大\\
最后乘上矩阵V，得到一个n*d_v的矩阵，每一行就是我们要的输出
$$

##### 为什么使用的是Dot-Product而不是Additive attention

Dot-Product计算起来更快，空间效率更好，因为它可以使用高度优化的矩阵乘法代码来实现

##### 为什么要除以根号d_k

因为过大的d_k值，会使得点积的值变得很大，从而将softmax函数推入梯度极小的区域。



#### Multi-Head Attention

对于每一个head，去匹配不同的模式，来表示不同的相关性，有点类似CN中的多输出通道机制。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。
$$
MultiHead(Q,K,V)=Contact(head_1,...,head_h)W^O\\
where\ head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)
$$
由于使用了残差结构，这就要保证不同输出的维度是一样的。所以文章作者将输出投影到64维（即d_model/h，h=8)

#### Transformer不同的Attention

<img src=".\img\Transformer\Transfomer不同的Attention.png" alt="Transfomer不同的Attention" style="zoom: 67%;" />

1. Encoder中的Multi-Head Attention：
   - 有三个输入，k，v，q，它们来自同一个输入（所以叫做自注意力）
   - 输入了n个query，每个query会得到一个输出，那么会有n个输出
   - 输出是value的加权和，权重是key和query的相似度
   - 输入和输出大小相同，都是d
   - 对于multi-head，由于投影的不同，可以学习到h个不一样的距离空间，使得输出变化。
2. Decoder中的Masked Multi-Head Attenion：
   - 和Encoder类似，只不过是Masked。因为对于Decoder，不应该看到t时刻以及之后的输入
3. Decoder中的第二个Multi-Head Attention:
   - 不再是自注意力了，Encoder的输出作为输入的K和V，上一层Decoder的输出作为Query
   - Encoder的输出是n个长为d的向量，上一层解码器的输出是m个长为d的向量
   - **实际上在做根据Decoder给的query，从Encoder的输出中提取出所需要的东西（所感兴趣的东西）**
   - **这个attention起到在encoder和decoder之间传递信息的作用**

#### Position-wise Feed-Forward Networks

对每一个position（每一个词）都做一次MLP，MLP是一样的
$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$
输入和输出维度d_model=512，W1将输入投影到2048维，W2重新投影到512

**Attention已经完成了输入的序列信息的汇聚，再通过MLP投影成所需的语义空间中的向量。所以可以对每一个Position单独使用MLP**

##### 和RNN的区别

**Transformer和RNN都是用MLP实现语义空间的转换**

不同在于，RNN是把上一个时刻的信息输出作为当前时刻的输入；Transformer是将整个序列的信息作为MLP的输入



#### Embeddings and Softmax

- 对于每一个input token和output token，学习一个**d维向量表示（d=512）**
- 编码器，解码器和softmax前面都需要一个embedding，**这三个embedding使用相同的权重，使得训练更简单。**
- 对权重除以了根号d。这是因为 **学 embedding 的时候，会把每一个向量的 L2 Norm 学的比较小。维度d大的话，学到的权重就很小。但之后还需要加上positional encoding，所以通过除以根号d，使embedding 和  positional encoding 的 scale 差不多，可以做加法。** 

#### Positional Encoding

Attenion是不会保留时序信息的。因为输出是value的加权和，权重来自query和key之间的距离，和序列信息无关。一句话把顺序任意打乱之后，attention 出来的结果都是一样的。 在处理时序数据的时候，一句话里面的词完全打乱，那么语义肯定会发生变化，但是 attention 不会处理这个情况，所以需要 加入时序信息

**对于每一个位置数字信息的值，使用周期不一样的sin和cos函数计算成一个长为512的向量来表示**

输入进入 embedding 层之后，那么对**每个词都会拿到一个长为 512 的向量。positional encoding （这个词在句子中的位置），返回一个长为 512 的向量，表示这个位置**，然后把 embeding 和 positional encodding 加起来就行了。相加后的结果仍然是顺序不变的



### Why Self Attention

#### 比较了Table 1

<img src=".\img\Transformer\Attention和其他结构比较.png" alt="Attention和其他结构比较" style="zoom:80%;" />

就计算复杂度而言，当序列长度 n 小于表示维度 d 时，自注意力层比循环层更快。

为了提高涉及很长序列的任务的计算性能，自注意力可以限制为仅考虑以相应输出位置为中心的输入序列中大小为 r 的邻域，但这会将最大路径长度增加到 O(n/r)。

实际上self-attention和CNN,RNN和时间复杂度差不多，但是Attention需要更多的数据才能达到和CNN、RNN一样的效果。

### Experiment

#### Tranning Data

WMT 2014 English-German dataset

WMT 2014 English-French dataset

#### Hardware and Schedule

base model：8块P100上训练了100,000 steps

big models：8块P100上训练了300,000 steps

#### Optimizer

Adam β1=0.9，β2=0.98，学习率是根据公式计算出来的，并使用了warm up技术。warmup_steps=4000

#### Regularization

##### Residual Dropout

- 对于每一个sub layer，在进入下一个sub layer和norm之前，会使用dropout
- 在输入embedding和positional encodings的和上也使用了dropout
- P_dropout=0.1

##### Label Smoothing

对于正确的词，置信度从1变为0.1

### Conclusion

- 提出了一个Transformer模型，用multi-head self-attention替换了RNN和CNN
- 在机器翻译任务上，Transformer训练更快
- 未来展望在图像，视频，音频中进一步研究



## VIT

### 论文地址

[vit]: file://E:/研究生学习/论文/vit.pdf

### Title

An Images Is Worth 16x16 Words:Transformers For Image Recognition At Scale

### Author

Google Research,Brain Team

### Abstract

Transformer已经在NLP中取得很好的发展，但在CV中应用有限。目前的CV里的attention主要是attention+CNN或者attention替换CNN部分模块但仍然保持CNN整体结构。

作者提出一个纯Transformer也可以在图像分类上取得很好的结果。

在大规模数据上训练的预训练VIT迁移到中小规模数据集上做分类时，可以媲美最好的CNN，且所需训练资源更少。

### Introduction

在NLP中，主流的方法是使用在大文本数据集上预训练模型，然后针对特定的任务数据集进行微调，且随着数据集和模型的增长，仍然没有出现饱和现象。

在CV中，CNN仍然占据主导地位。目前，已经有一些关于Attention的探索

- Wang et al.,2018：将ResNet的最后一个stage的14x14特征图拉平，此时序列长度为196，然后输入Transformer
- Ramachandran et al.,2019：提出孤立自注意力机制，用一个局部的window来控制Transformer的计算复杂度。
- Wang et al.,2020a：提出轴注意力机制。将2D的矩阵拆分为高和宽两个1D的向量。分别在高和宽两个dimension上做两次self-attention
- 以上技术理论上高效，但是无法在现代硬件上加速，因此很难训练一个大模型，可扩展性不高。
- 因此，state of the art仍然是以Resnet为basic的模型

**作者为了考虑Transformer的可扩展性，尽量不针对视觉任务而对Transformer进行修改，提出将图像分成patch（每个patch是16x16，一张224x224的图片就有14x14个patch，相当于一个维度为196x768的序列），将每一个patch经过一个FC Layer得到linear embedding sequence作为Transformer的输入。**



**由于Transformer和CNN相比缺少inductive biases（归纳偏置）。在CNN中inductive biases是locality 和平移不变性。**CNN的卷积核像一个template，同样的物体无论移动到哪里，遇到了相同的卷积核，则输出一致。

**CNN 有 locality 和 translation equivariance 归纳偏置--> CNN 有 很多先验信息 --> 需要较少的数据去学好一个模型。**

**Transformer 没有这些先验信息，只能 从更多的大量图片数据里，自己学习对视觉世界的感知。**

### Related Work

第一段讲了Transformer在NLP中的主流使用方法：BERT和GPT

第二段：

- 由于一张图片的pixel数量非常多，如果把所有pixel输入到Transformer中，那么这个计算量是远超Transformer在NLP中的使用情况的
- 提出了一些prior works对self-attention在CV中的研究

和VIT最接近的一项工作是Cordonnier(2020)提出的一个方法：把图像分成一个个patch，每个patch是2x2，然后使用self-attention。VIT和它的区别在于进一步提高了模型的scale。使用了大scale的patch，使得VIT能处理high resolution的images。

提出了一个VIT模型，除了初始的patch分块步骤，作者没有进将特定于图像的归纳偏置引入模型架构中。作者将一张图看作一个patches序列，使用标准的Transformer来做分类。在大数据集上的与训练效果很好，超过了state of the art

使用更多的额外数据集可以使得ViT超过state of the art

讲了别人做了哪些工作，你和他们的联系与区别是什么



### Method

#### VIT

<img src=".\img\VIT\VIT结构.png" alt="VIT结构" style="zoom:80%;" />

<img src=".\img\VIT\VIT细节结构.png" alt="VIT细节结构" style="zoom: 50%;" />

- 首先将一张图分成一个个patch，每个patch是16x16x3，一张224x224的图片就有14x14个patch（**在代码实现中，使用卷积核大小为16，stride=16，卷积核个数=768）：224x224x3->14x14x768->196x768。这样就变为14x14个patch，每个patch是原来16x16个pixel的特征提取**
- 将每一个patch通过Flatten变为序列
- 每一个patch通过Linear Projection操作得到一个特征patch embedding
- 将patch embedding和position embedding相加
- 为了实现分类，在Transformer Encoder的输入前加入了一个额外的Learnable class embedding
- 将以上输入到Transformer得到输出**（在Transformer Encoder前有一个Dropout层，后面有一个LN层）（训练ImageNet-21K时由Linear+tanh+Linear，迁移到自己的数据集时，只有一个零初始化的Linear）**<img src=".\img\VIT\fine-tuning.png" alt="fine-tuning" style="zoom: 50%;" />
- 将结果输入到MLP中进行分类



步骤1：将一张224x224x3的图片分成16x16的patch，每个patch大小为16x16x3，一共有14x14=196个patch。所以原来的224x224x3的图片就变成了196x768的序列。

步骤2：线性投射层其实就是一个MLP，维度为768x768(D)。第一个768是根据输入图像计算得到的，第二个**D=768可以根据Transformer的改变而变大或变小。**

步骤3：步骤二得到的输出为196x768。相当于有196个token，每个token维度为768。**这就完成了2D图像到1D序列的转换。**

步骤4：由于加入了一个1x768的class embedding，所以**总体输出维度为197x768**，即**Embedded Patches的输入为197x768的tensor**

步骤5：**加入位置编码信息（实际上是编号）**

步骤6：进入Transformer模块，先做Layer Norm

步骤7：做Multi-Head Attention。输入的Q,K,V维度都是197x768。不过由于多头自注意力机制的头个数**h=12**，所以**每个头输入的维度应该为197x(768/12)=197x64**

步骤8：**将12个头的输出contact起来，仍然得到一个197x768的输出，再过一层Layer Norm**

步骤9：过MLP：197x768 ->197x3072->197x768

步骤10：经过L个Transformer Block

#### 代码实现图

以ViT-B/16为例（如果是自己的数据集，可以不用Pre-Logits）

<img src=".\img\VIT\VIT-B-16的代码实现图.png" alt="VIT-B-16的代码实现图" style="zoom: 80%;" />

#### Class Token

往Patch Embedding中加入了一个**可学习的class token（即图中的0*)**，**这个token在Transformer的对应输出就当作是整个图像的特征y（即这个class token可以代表这张图片）。然后将y通过MLP进行分类预测**。



##### 为什么要加入class token

由于**Transformer**是为NLP任务设计的，它**期望输入数据中包含一个特殊的"类别"或"句子"标记，以指示所处理的文本属于哪个类别或句子**。**对于图像数据，没有明确的类别或句子标记，因此为了将图像数据与Transformer对话，ViT引入了一个特殊的向量，通常称为"类别标记"（class token）**

class token是一个额外的向量，它代表整个图像而不是特定的某个patch。它将与所有图像块的表示一起输入到Transformer模型。这样，**Transformer在处理每个图像块时都可以感知整个图像的信息，而不仅仅局限于特定的局部图像块**。**类别标记的添加有助于使Transformer模型能够理解整体图像的语义信息**，并在视觉任务中表现更好。

总的来说，就是**通过引入class token，使得图像数据能转换为Transformer可接受的序列格式，并且提供了一点额外的全局信息。**

#### Inductive bias

ViT相对于CNN缺少inductive bias。CNN具有locality（局部性）、two-dimensional neighborhood structure（二维邻域结构）和translation equivariance（平移不变性），这些信息贯穿于整个CNN网络中。然而在ViT中，只有MLP具有这些归纳信息，而self-attenion是全局的。

ViT仅仅在一开始的分patch阶段和fine-tuning修改position embeddings时使用到了图像的2D inductive bias。

#### Position Embeddings

仍然使用1D 的position embeddings

#### 公式描述

<img src=".\img\VIT\VIT过程的公式描述.png" alt="VIT过程的公式描述" style="zoom:80%;" />

1.将每一个patch Xp输入到Linear Projection，将输出和class embedding拼接

2.将拼接结果和Position Embeddings相加，得到Transformer的输入Z0

3.循环L个Transformer Block 

4.**将Z_L<sup>0</sup>（class token所对应的输出）作为整体图像的特征，输入到MLP中完成分类。**



#### Fine-tuning And Higher Resolution

-  在大数据集上预训练，在下游任务上进行fine-tuing
- 在微调时使用大分辨率的图像效果会提高(Touvron et al.,2019)，但这会导致VIT产生的序列长度增加(patch size不变，H*W/P会变大)。
- 尽管VIT可以处理任意长度的序列，但是**由于提前预训练好的position embeddings可能就没有用了**
- **因此作者在pre-trained的position embeddings中采用2D插值。**
- **只有position embeddings的2D插值和patch extraction使用了图像的2D inductive bias**

#### 为什么强调只在position embedding的2D interpolation和patch extraction使用了图像的2D inductive信息？

主要是为了**突出ViT在处理图像时的特殊性和优势**

- 因为这种做法使得**Transformer模型能够利用图像的局部和全局信息**，而**不会受到CNN中固定的局部感受野大小的限制。**
- ViT通过使用**2D插值的方式对位置编码进行计算**，以便**在局部区域和全局区域都能获取合适的位置信息**。这是因为**在2D插值过程中，可以通过对附近像素的加权平均来估算图像块的位置编码，从而捕获图像中的空间关系，使得Transformer很好地处理图像上下文**
- 在patch提取阶段，ViT并没有使用传统的固定形状的卷积滤波器，而是将图像切分成均匀的图像块，这样做的目的是**使Transformer模型能够处理不同大小的图像，而不需要调整网络结构或参数（即patch size不变，但是sequence长度增大减小不影响Transformer）。这样，ViT可以在训练和推理过程中处理任意大小的图像**
- **这两种方法使得ViT具有更好的可扩展性和适应性，可以处理各种尺寸的图像，并且能够更好地捕捉图像中的全局上下文信息。因此，ViT强调了它只在这两个阶段使用了图像的2D归纳信息，以强调其在视觉任务中的特殊优势。**

### Experiments

- 对比 ResNet, ViT, Hybrid ViT (CNN 特征图，不是图片直接 patch 化) 的 representation learning capabilities 表征学习能力。
- 为了了解每个模型预训练好到底需要多少数据，在不同大小的数据集上预训练，然后在很多 benchmark tasks 做测试。
- 考虑模型预训练的计算成本时，ViT 表现很好， SOTA + fewer resource 训练时间更少
- ViT 的自监督训练，可行，效果也还不错，有潜力；一年之后，MAE 用自监督训练 ViT 效果很好

#### Datasets

ImageNet-1K: 1000 classes, 1.3M images

ImageNet-21K: 21000 classes, 14M images

JFG-300: 303M images Google 不开源

下游任务：分类 CFIAR etc.

#### Model Variants

<img src=".\img\VIT\VIT Variants.png" alt="VIT Variants" style="zoom:80%;" />

**ViT-L/16代表着ViT-Large with 16x16 input patch size**



#### Metrics

微调上的准确率

few-shot：评估ViT在**小样本学习**（few-shot learning）任务上的表现的实验，并对其**泛化性能和适应性**进行测试

在ViT的few-shot实验中，通常采用的是一种叫做"**n-shot k-way**"的评估方式，其中**n代表每个类别的训练样本数量，k代表任务中的类别数量**。具体地说，给定n个样本的训练集，模型需要学会区分k个不同的类别。这个过程会多次重复，以评估模型在不同任务和数据集上的表现。

为了实现few-shot学习，通常会采用一些技术来辅助模型学习。例如，可以使用元学习（**meta-learning**）方法，如**MAML（Model-Agnostic Meta-Learning）或Prototypical Networks**，这些方法旨在帮助模型在少量样本上学习并快速适应新任务。此外，数据增强和特定的任务设计也可能用于增强模型的泛化性能

#### Comparison to state of the art

总结来说，就是ViT效果更好，预训练所需TPU核心天数更少

对于训练策略、优化器、学习率等其他影响因素的验证，作者在4.4节做了对比控制实验。

#### Pre-Train Data Requirements

##### 实验一

分别在ImageNet，ImageNet-21k和JFT-300M上进行了预训练

为了提高在小数据集上的性能，作者优化了weight decay,dropout和label smoothing三个参数。

试验结果表明，BiT在小数据集上比ViT更好，ViT在larger datasets更好

##### 实验二

分别在JFT-300M的9M，30M和90M子集上进行了实验。在这个实验中没有对小数据集进行额外的正则化。不过仍然使用了early-stopping来取得最佳精度。

在小数据集上，ResNets很有用，但是对于大数据集，直接从数据中学习相关模式的ViT更好。



#### Scaling study

- Vision Transformers 在性能/计算权衡方面优于 ResNets。要达到相同的性能，ViT 使用的计算量大约要少 2-4 倍（5 个数据集的平均值）
- 在计算预算较少的情况下，混合系统的性能略优于 ViT，但在较大的模型中，这种差异就会消失
- 仍然没有看到性能饱和的现象，模型仍然有进一步扩展和提高的空间。

#### 可视化

##### Patch embedding

<img src=".\img\VIT\RGB embedding filters.png" alt="RGB embedding filters" style="zoom:80%;" />

在Patch embedding阶段（即Linear Projection），可以看出attention学到的特征主要包含颜色和纹理，这些成分可以作为基函数去描述图像的底层结构

##### Positional embedding

<img src=".\img\VIT\pos embedding.png" alt="pos embedding" style="zoom:80%;" />

学到了**距离的概念**，可以看到自己和自己的位置处是黄色的，相似度最高，越远颜色就越蓝，即相似度越低。

此外，还学习到了行和列的规则，同行同列的颜色表示

##### Size of attended area

<img src=".\img\VIT\Mean area.png" alt="Mean area" style="zoom:80%;" />

- **attention distance就类似于CNN中的receptive field**
- 可以看出，对于attention，在浅层layer中就已经学习到了较远距离的信息了。
- 随着网络加深，head都趋向于更远的attention distance了
- **模型趋向于关注和图像分类语义相关的区域了**

#### Self-superivision

作者使用类似BERT的方法进行了自监督学习探索：将某些patch进行mask，然后进行学习预测。

通过这种自监督预训练，ViT-B/16在ImageNet上取得了79.9%的准确率，比从头开始训练显著提高了2%，但是相对于supervised pre-training仍然低4%。

这是一个未来工作的展望。

#### Fine Tune

- SGD with a momentum of 0.9
- 学习率小梯度搜索技术
- 迁移ViT模型到其他数据集上时，移除了最后的两个linear layers，用一个零初始化的linear layer代替，输出维度为分类个数。这种方法比直接重新初始化最后的layer更具有鲁棒性。

#### Transformer Shape

ViT进行了对模型深度、宽度以及patch size缩放对性能影响的实验

- 缩放深度：通过增加模型的深度（即增加层数），可以明显地改进性能，尤其在增加了64层之前可以明显观察到这种改进。然而，在增加到16层之后，性能的改进变得有限。这表明增加模型深度对性能改进是有帮助的，但随着深度增加，性能的提升会逐渐减弱。
- 缩放宽度：增加模型的宽度（即增加每层的隐藏单元数）似乎对性能的改进影响较小。换句话说，增加隐藏单元数并不像增加深度那样显著改善性能。
- 缩小patch size：**通过减小图像块的大小，有效地增加了模型的序列长度。这样的操作在不引入额外参数的情况下，表现出惊人的鲁棒性改进**。换句话说，更小的图像块能够带来较好的性能提升，并且不会增加太多模型参数的数量。

#### 为什么缩小patch size可以提高ViT的鲁棒性？

- 当ViT减小patch size时，每个patch的pixel数减少，这意味着**patch的维度减少，那么此时patch的数量就会增加，相当于增加了输入序列的长度**。
- 在Transformer中，**序列长度决定了每个head在计算时可以考虑的位置距离。**
- Vit的position embedding**允许Transformer模型处理任意长度的输入序列**，并能够捕捉远距离位置之间的关系。因此，当减小patch size大小，从而增加输入序列的长度时，ViT仍然可以处理更长的上下文关系，而**不会引入额外的模型参数。**
- **对于Transformer，更长的输入序列长度意味着可以为模型提供更多全局信息，使得模型能够更好地理解图像的语义和结构。**

#### 三种不同位置编码的消融实验

- 无位置编码（No Positional Encoding）：在这个消融实验中，作者移除了位置编码，不将位置信息引入Transformer模型中。**这相当于将图像数据看作一系列无序的token，而不考虑其空间关系**。通过这个实验，作者想要验证位置编码对Transformer在处理图像数据时的重要性。
- 1D 位置编码：在这个实验中，作者使用1D位置编码，而不是原始的2D位置编码。1D位置编码是在图像块**序列维度**上独立地应用的，以一维方式捕捉位置信息。
- 原始的2D位置编码（Original 2D Positional Encoding）：这是ViT中默认和原始使用的位置编码。它涉及使用2D插值来生成位置嵌入，捕捉图像块之间的2D空间关系。
- 相对位置编码：对于给定的图像块对（其中一个作为查询，另一个作为注意机制中的键/值），我们有一个偏移量（pq − pk），其中每个偏移量都关联着一个嵌入（embedding）。然后，论文中进行额外的注意力运算，其中使用原始查询（查询内容），但是使用相对位置嵌入作为键。然后，将**相对注意力的逻辑值作为偏置项，并添加到主要注意力（基于内容的注意力）的逻辑值上，然后再应用softmax操作**

#### Attention maps

<img src=".\img\VIT\attention maps.png" alt="attention maps" style="zoom:80%;" />

Attention maps是通过计算Transformer模型的注意力权重得到的。具体来说，这些attention maps显示了每个图像块与其他图像块之间的注意力分布。

注意力分布决定了每个图像块在处理其他图像块时的重要性。在Transformer中，**注意力是通过计算查询（query）和键（key）之间的相似度来得到的**。**较高的相似度意味着两个图像块之间的关联更强，因此，查询会更多地关注这些键**。

**为了获得注意力权重，通常会使用softmax函数将相似度值归一化为概率分布。这样，每个查询对所有键的注意力分布之和为1，这确保了每个图像块都能获得其他图像块的信息，同时保持了全局的一致性。**

注意力分布决定了Transformer中自注意力机制的输出。**在ViT中，这些自注意力权重可以直接可视化并显示为attention maps，以便了解模型在处理图像时对不同图像块之间的关注程度。**

在Figure 9中，ViT论文展示了不同层次的attention maps。这些maps显示了每个图像块与其他图像块的注意力分布，以及它们在不同层次的Transformer中的处理过程。这样的可视化有助于理解ViT模型如何在不同层次上学习图像中的信息，并捕捉图像块之间的关联

### Conclusion

- 提出了一个VIT模型
- 除了patch extraction step和position embedding，没有引入特定图像的归纳偏置，即没有使用到图像的2D信息。
- 在大数据集上pre-train可以取得超越state of art的性能
- 训练更cheap

展望

- VIT在检测和分割中的应用
- 探索自监督预训练方法



## Swin Transformer

### 论文地址

[Swin]: file://E:/研究生学习/论文/swin.pdf

### 期刊/会议

ICCV 2021

### Title

Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

### Author

Ze Liu, Yutong Lin, Yue Cao

### Abstract

#### 研究背景与问题

Transformer在CV中应用仍然存在两个问题

1.视觉实体规模尺度的变化

2.图像的resolution太大，计算量太大

#### 所提模型特点

提出了Shifted window（移动窗口）方法，将自注意力计算限制在非重叠的局部窗口，降低了序列长度。同时还允许cross-window connection（跨窗口连接），变相的带来了全局建模能力。

这种**分层架构**具有在各种尺度上建模的灵活性，并且具有相对于图像大小的线性计算复杂性，这为提出Swin V2做铺垫，可以在更大的分辨率图像上做预训练。

### Introduction

在CV中，仍然是以CNN为backbone的模型占主导地位，人们仍然通过扩大模型尺寸，使用更复杂的结构来提升CNN的性能。

引入Transformer到CV中，由于ViT已经做了这一点，作者指出可以将Transformer作为CV里所有任务通用的backbone。

#### Figure1

<img src=".\img\Swin\图1.png" alt="图1" style="zoom:67%;" />

对于**ViT**，**每一层Transformer block都是16倍下采样率，图像特征图是单一低分辨率**的。对于**多尺度的任务**（分割、检测等）**不太适用**。此外，**ViT的自注意力始终是在整张图上进行了计算，所以是一个全局建模，它的计算复杂度和图像关系成平方倍增长。**

对于Swin-T，它的特点：

- 它构建了**不同层级的特征图**。通过提出**patch merging（相邻的4个小patch合成1个大patch）**的方法，**在更深的layers中可以获得更大的感受野**。这样就获取了图像的多尺寸信息，就能**将这些多尺度特征图用于检测或者分割任务**中。

- **将self-attention限制在在局部小窗口中，其计算复杂度和图像大小是线性关系**。这一点利用了CNN的**locality的Inductive bias**（**局部性的先验知识**），即**同一个物体的不同部位**或者**语义相近的不同物体**会**大概率出现在相连的地方**，所以**在一个小范围local中计算的自注意力是够用的**。
- **使用shifted-window使得不同window之间的patch进行交互，提高模型的全局建模能力**。

#### Figure2

Swin-T设计的关键就是shifted window。

<img src=".\img\Swin\shifted window.png" alt="shifted window" style="zoom:50%;" />

**将每个window往右下角移动了2个patch。然后在新的window中再次分为四方格。这样使得window与window之间进行互动。因为原本的self-attention是限制在每个window中的，并且window之间是互不重叠的，因此它是无法和其他window中的patch产生关联的，这样就失去了全局建模能力。**

此外，**通过使用patch merging技术，在Transformer的后几层中，每个patch的感受野都变得很大了。再通过shift window，所谓的window local self-attention也就变相于全局self-attention了**。



**Swin-T利用了更多的视觉中的先验知识。在模型大一统上，仍然是ViT用的更好。ViT可以不加任何先验知识，直接利用Transformer在CV和NLP领域都用的很好，这样模型就能共享参数。**



### Related work

#### CNN and variants

AlexNet，VGG，GoogleNet，ResNet，DenseNet，HRNet，EfficientNet，深度卷积，可变性卷积

#### Self-attention based backbone architectures

一些工作使用self-attention替代CNN中的一部分结构，但是计算复杂度仍然很高。

#### Self-attention/Transformers to complement CNNs

使用self-attention的较好的长距离依赖建模能力来增强CNN

#### Transformer based vision backbone

ViT

DeiT

不适用于密集型的视觉任务中，且当图片分辨率很大时，计算复杂度会平方倍增长

### Method

#### Overall Atchitecture

<img src=".\img\Swin\Swin-T结构.png" alt="Swin-T结构" style="zoom:80%;" />

- 假设输入图片尺寸是224x224x3。首先对图片进行**Patch Partition**。
- Patch Partition步骤中，**每个patch的size是4x4**。所以Patch Partition后就得到了**56x56x48**（56=224/4；48=4x4x3，即56x56个patch，每个patch_size=4x4）的特征图。
- 输入**Linear Embedding**，得到结果为**56x56xC（在Swin-T中C=96），展平后就变为3136x96**
- 由于3136这个序列对于Transformer来说太长了，所以Swin引入了window，只在window内做self-attention。**对于每个window，默认的patch数量是7x7=49个，所以序列长度只有49。**
- 通过**Swin Transformer Block计算的输出仍然为56x56x96**。
- 执行**Patch Merging**操作<img src=".\img\Swin\Patch Merging示意图.png" alt="Patch Merging示意图" style="zoom:80%;" />

​	假设输入一个tensor为HxWxC，**如果下采样的倍数为2，那么Patch Merging就会每隔1个点选一个点进行Merging**，得到4个tensor，**输出的高宽缩小1倍，通道数变为4倍**。而**为了保持输出通道数是输入通道数的2倍 ，用了一个1x1的卷积将通道数缩小为2倍**。

- **执行完Patch Merging操作后，输出特征图变为28x28x192（即变成28x28个patch了，每个patch的size为8x8x3)**
- 重复多次，**得到7x7x768的特征图（即7x7个patch，每个patch的size为16x16x3）**
- **由于Swin-T没有像ViT那样加入CLS Token。所以它执行了全局平均pooling操作，将7x7取平均拉直为1，去做分类。**



#### Shifted Window based Self-Attention

##### Self-attention in non-overlapped windows

<img src=".\img\Swin\self-attention in window.png" alt="self-attention in window" style="zoom:80%;" />

**假设输入tensor为56x56x96，那么图中每一个黄色格子就是一个window。每一个window包含MXM个patch（M默认=7）。**

##### MSA和W-MSA复杂度计算

**标准MSA的计算复杂度分析图**

<img src=".\img\Swin\MSA复杂度计算.png" alt="MSA复杂度计算" style="zoom:80%;" />

- 如果输入是hw x C的tensor，那么q，k，v就是分别乘以3个C x C的系数矩阵得到的，每一个计算复杂度为hw x C<sup>2</sup>，那么计算复杂度就为3hwC<sup>2</sup>。
- 在计算Attention这一步，q和k<sup>T</sup>点乘，相当于乘上一个C x hw的矩阵，所以计算复杂度就为(hw)<sup>2</sup>C
- Attention和v的乘积，就是hw x hw * hw x C，那么计算复杂度为(hw)<sup>2</sup>C
- Proj层，hw x C * C x C，那么计算复杂度为hwC<sup>2</sup>
- 所以MSA总的计算复杂度就为4hwC<sup>2</sup>+2(hw)<sup>2</sup>C
- **公式表示**<img src=".\img\Swin\W-MSA复杂度分析.png" alt="W-MSA复杂度分析" style="zoom:80%;" />

**对于W-MSA，此时h和w就变为M，那么带入MSA公式（1），可得一个窗口的MSA的计算复杂度**
$$
4M^2C^2+2M^4C
$$
**那么对于Swin-t，一共有hw/(MxM)个窗口，那么乘上以上公式，可得公式（2）**
$$
4hwC^2+2M^2hwC
$$
**对于MSA，复杂度随着hw的增大而呈平方关系倍数增大；而对于W-MSA，当M是一个固定值时，复杂度随着hw的增大而呈线性关系增大。**

**这样虽然减少了内存开销和计算量，但是这样的局部自注意力无法做到全局建模和通信**



**Swin Transformer的三个切入点**

- 为了解决CV中**层级式的问题**（不同尺度），提出了**Patch Merging操作**，像CNN一样**构建不同尺度的Transformer**。
- 为了**减少计算复杂度**，提出了**基于window的self-attention**操作。
- 为了**全局建模**和**不同window之间patch的交互**，提出了**shifted-window**的方法。





##### Shifted window partitioning in successive blocks

介绍了shifted window，主要是围绕图2介绍了交替使用W-MSA和SW-MSA的过程。

- **W-MSA负责将特征图划分为不同的窗口，例如8x8的特征图均匀划分为2x2=4个window，每个window有4x4(M=4)=16个patch**
- **SW-MSA将上一层的输入往右下角移动floor{M/2}个pixels（其实是patch，不是window)**



##### Efficient batch computation for shifted configuration

使用shifted window带来了2个问题：

1.窗口数量增加了

2.每个窗口的patch数量可能不一样。

**这样就无法把这些window压成一个batch去快速做self-attention**



**解决方法**

<img src=".\img\Swin\Batch swmsa.png" alt="Batch swmsa" style="zoom:80%;" />

- 移位之后进行**cyclic shift，这样就保证了窗口数量不变**，**每个窗口的patch数量一致。**
- 但也产生了**新问题**，A,B,C三个窗口的有一部分patch是从原来相隔较远的window中移动过来的，它们之间不应该去做self-attention，所以做了一个**Masked MSA，让不相干的patch不进行self-attention计算（即图中一个window内不同颜色的patch不应该做self-attention**）。
- 做完Masked MSA后，将图重新**reverse cyclic shift**回去，为了**保持原来图片的相对位置和语义信息不变。**如果不把循环位移还原的话，那相当于在做Transformer的操作之中，一直在把图片往右下角移，不停的往右下角移，这样图片的语义信息很有可能就被破坏掉了。

##### Masked MSA

<img src=".\img\Swin\Mask msa.png" alt="Mask msa" style="zoom:80%;" />



**虽然相隔较远的patch可能在某些层级上没有直接的self-attention联系，然后不对它们做self-attention可能会损失一部分全局信息。但是由于多尺度的存在，patch size会逐渐增加，到后面的layer中patch已经有了较大的感受野了（例如16x16），再利用shifted window融合不同window间patch的交互，仍然能捕捉到较好的全局信息。全局信息仍然能够在不同层级间传递和融合，使得模型整体上保留了一定程度的全局self-attention能力。这种折中的设计使得Swin Transformer在处理大尺寸图像时成为可能。**



#### Relative position bias

<img src=".\img\Swin\relative pos idx.png" alt="relative pos idx" style="zoom:80%;" />
$$
Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V\\
B∈R^{M^2 * M^2};Q,K,V∈R^{M^2 * d}
$$


**相对位置索引，将4个图展平为行。但是作者实现时是用的一元索引。**

**二元转一元过程：**

1.偏移从0开始，行、列标加上M-1

2.行标乘上2M-1

3.行、列标相加

<img src="E:\深度学习\网络模型笔记\img\Swin\relative pos index.png" alt="relative pos index" style="zoom:80%;" />

**relative position bias table**

每一个head的table

```python
 self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]
```

bias个数为(2M-1)X(2M-1)

<img src=".\img\Swin\pos 论文.png" alt="pos 论文" style="zoom:50%;" />

<img src=".\img\Swin\relative pos table.png" alt="relative pos table" style="zoom:80%;" />

### Experiment

分别比较了Swin和其他state of art在ImageNet-1K 图像分类，COCO 目标检测和ADE20K语义分割任务上的性能。然后给出了消融实验。

#### Image Classfication on ImageNet 1K

##### Regular ImageNet-1K training

- AdamW optimizer
- 300 epochs using a cosine decay learning rate
- 20 epochs of linear warm-up
- batch size = 1024
- initial learning rate of 0.001
- weight decay = 0.05
- gradient clipping with a max norm of 1
- 正则化技术：RandAugment [17], Mixup [77], Cutmix [75], random erasing [82] and stochastic depth
- 对于更大的分辨率输入，在pre-train基础上微调训练30个epochs，学习率固定为10^-5，weight decay=10^-8



##### Pre-training on ImageNet-22K

- AdamW optimizer

- 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up

- batch size = 4096

- initial learning rate = 0.001

- weight decay = 0.01

-  In ImageNet-1K fine-tuning, we train the models for 30 epochs with a batch size of 1024, a constant learning rate of 10−5 , and a weight decay of 10−8

  

### Conclusion

1.所提的Swin-T可以构建层级式的特征图，并且计算复杂度相对于图像size是线性的。

2.希望 Swin Transformer 在各种视觉问题上的强大表现将促进视觉和语言信号的统一建模。

3.作为 Swin Transformer 的关键要素，基于shifted window的自注意力在视觉问题上被证明是有效且高效的，我们也期待研究其在自然语言处理中的使用



## GAN

### 论文地址

[GAN]: file://E:/研究生学习/论文/NIPS-2014-generative-adversarial-nets-Paper.pdf

### 期刊/会议



### Title

**Generative Adversarial Nets**

生成式对抗网络



### Author

Goodfellow



### Abstract

提出了一个新的框架 通过对抗的过程 来估计一个生成模型，同时训练2个模型。一个是生成模型G，G负责抓取数据分布；一个是判别模型D，负责估计样本是来自训练数据还是G生成的。

训练G是为了尽可能让D犯错。在任何函数空间中，都有一个唯一解，使得G能够生成的东西接近训练数据的分布，D基本上无法分辨。

在训练或生成样本期间不需要任何马尔可夫链或展开的近似推理网络。

### Introduction

深度学习在辨别模型中已经取得了很大进展，主要依赖于反向传播和dropout算法。

深度生成模型的影响较小，因为**难以近似最大似然估计**和**相关策略中出现的许多棘手的概率计算**，并且由于**难以在生成环境中利用分段线性单元的优势。**

介绍了GAN框架是什么

在GAN框架下的G和D都是一个MLP，G的输入是一个随机的noise（从高斯分布中取样）。G把这个noise映射到一个数据分布。



### Related work

#### 之前的工作

大多数都是去构造出一个分布函数，给这个函数一些参数使得它可以学习，并通过最大似然函数来训练。这样的问题是计算难度比较大，特别是高维的情况。

介绍了别人的工作：VAE

之前也有用辨别模型来训练生成模型：NCE。但NCE使用的损失函数更复杂，以至于在求解性能上没有GAN好。

介绍了GAN和对抗样本(Adversarial examples)的区别

Adversarial example：构造一些假的样本，跟真的样本很像，从而测试算法的稳定性。



### Adversarial nets

参考李宏毅的机器学习那部分讲解的笔记



### Theoretical Results

有一个全局最优解使得p_g = p_data

![理论1](.\img\GAN\理论1.png)

最终D_G会等于1/2



## BERT

### 论文地址

[BERT]: file://E:/研究生学习/论文/1810.04805BERT.pdf

### 期刊/会议



### Title

BERT:Pre-training of Deep Bidirectional Transformers Language Understanding

### Author

Jacob Devlin,Ming-Wei Chang

### Abstract

提出了BERT。BERT旨在通过**在所有层**中**联合调节左右上下文**来**预训练未标记文本的深度双向表示**

因此，BERT在fine-tune时只需要加一个额外的输出层即可获得很好的结果。

### Introduction

预训练语言模型对NLP的任务是很重要的，包括句子层面的任务：句子关系分析；词元层面的任务：命名实体识别，问答系统。

预训练模型在应用于特征表示的下游任务时，一般有两类方法：

- **feature_based：以ELMo为代表，针对每个任务构造相关的网络，把预训练的表示作为额外的特征，和输入一起进入网络中。ELMo使用了双向信息，但是它是基于RNN的，不适合fine-tuning**
- fine-tuning：以GPT为代表。把预训练好的模型用于下游任务上进行微调。**使用了Transformer的架构，但只能处理单向的信息**

这两种方法**使用相同的目标函数**，并且使用的是**单向的语言模型**来学习特征表示。

单向的语言模型限制了结构的选择。



**BERT随机mask输入的一些token（MLM），目标是预测出这些被masked的token。这样，MLM就能融合文本左右的信息。此外，作者还做了“next sentence prediction”的任务。**



### Related Work

#### Unsupervise Feature-based Approaches

介绍了word embeddings以及它的更粗粒度的扩展：sentence embeddings,paragraph embeddings

介绍了ELMo：双向语言模型

 

#### Unsupervised Fine-tuning Approaches

GPT

####  Transfer Learning from Supervised Data

在有标号的任务上进行迁移学习。

### BERT

#### BERT的两个步骤：

- pre-training：自监督训练，在没有标号的数据上进行了训练

- fine-tuning：用BERT预训练模型在有标号的数据上进行训练

<img src=".\img\BERT\BERT两阶段.png" alt="BERT两阶段" style="zoom:80%;" />

#### 模型结构

BERT模型就是一个**多层的双向的Transformer编码器**。

##### Input/Output表示

BERT的输入可以是一个句子，也可以是一个句子对

使用了**WordPiece embeddings**。

- 首先使用**[CLS]**这个token。该token在每个sequence的**第一个位置**，**它的最终隐藏状态对应汇聚了序列的特征表示**
- 用**[SEP]**分开不同的sentences，然后给每个token添加一个可学习embedding

<img src=".\img\BERT\BERT输入组合.png" alt="BERT输入组合" style="zoom:80%;" />

**BERT的输入包括token embeddings，segment embeddings（区分这个token属于哪个sentence）和Position Embeddings**



#### Pre-training BERT

随机MASK 15%的tokens，然后只预测这些被masked的words是什么，而不是恢复整个序列。

对于这15%的mask住的tokens，对其中的80%替换成[MASK]符号，10%替换成随机words，10%什么都不变



#### Fine-tuning BERT

BERT把整个sentence pair都放进去了，所以self-attention能看到两端的信息。

针对下游任务，只需要设计输入和输出

### Experiments



### Conclusion

无监督的预训练模型是许多语言理解系统任务中不可或缺的一部分。这样能使得训练资源不多的任务也能享受深度神经网络。BERT主要就是把前人的结果拓展到深的双向结构，使得一个pre-train模型能处理不同的NLP任务。

## MAE

### 论文地址

[MAE]: file://E:/研究生学习/论文/2111.06377MAE.pdf

### 期刊/会议

### Title

**Masked** Autoencoders Are **Scalable** Vision Learners

带掩码的自编码器是一个可拓展的视觉学习器

**取标题时（Scalable和Efficient二选一）**

### Author

Kaiming He，Xinlei Chen —— Facebook AI Research

### Abstract

MAE的方法：随机盖住输入图片的一些patches，然后重构这些patches

#### 两个核心的设计：

- 一个非对称的encoder-decoder结构，encoder只操作那些可见的patches（没有被masked）。一个轻量的decoder，负责从latent representation和mask tokens中恢复原始图片
- mask大量的patches可以得到一个非显在的且有意义的自监督任务
- 合并这两个设计可以高校训练大模型。

### Introduction

#### 示意图

<img src=".\img\MAE\MAE示意图.png" alt="MAE示意图" style="zoom: 80%;" />

- 一张图片分成patches
- 将大量的patches盖住（灰色格子）
- 没有被盖住的patches被输入到encoders中得到特征
- 将盖住的patches和特征拼在一块（按原图顺序）
- 输入decoder进行重构恢复
- 在应用于下游任务时，只需要encoders，不需要decoder

CV已经在监督学习上可以训练大模型了，但在自监督学习上仍然没有很好的办法。

举例了NLP领域中基于自监督学习的大模型（GPT、BERT）

#### 是什么使得masked autoencoders在CV和NLP领域上处理也不一样呢

- 卷积神经网络的卷积窗口不好将mask tokens或者位置编码放入网络中（即**CNN无法将这个特定的mask tokens提出来，导致这个mask tokens在之后难以还原**）
- **信息密度差距**。在语言中，一个词有很丰富的语义和密集的信息。对于图片，由于像素之间存在冗余，所以一个被mask的patch可以通过附近的patch进行插值来恢复。作者就使用非常高比率将很多块给mask住。使得模型去学习全局的信息，而不是关注于局部的恢复
- decoder不同。在NLP中还原的是词，语义层面较高，使用一个MLP就能还原。而对于图片，还原的是pixel，是低层次的语义层面，仅靠一个MLP是不够的

**这篇文章的Introduction不仅讲了做了什么，还重点以问答方式讲了为什么这么做**

### Related Work

#### Masked language modeling

BERT GPT

#### Autoencoding

DAE

#### Masked image encoding

iGPT

BEiT

#### Self-supervised learning

Constrastive learning（对比学习），主要依靠数据增强

#### 总结

没有强调MAE和这些工作的区别，在写作时应该强调区别

### Approach

#### Masking

- 将图片分patches
- 随机采样一些patches进行保留，其余的全部mask

#### MAE encoder

一个ViT，但仅用于可见的patch，大量减少了训练开销

#### MAE decoder

输入是latent representation+mask token

所有的mask token是一个共享的、可学习的vector，它代表了要预测的missing patch

加入了positional embedding

用于其他任务时，不需要使用decoder

#### Reconstruction target

decoder的最后一层时一个linear layer。如果时是16x16，linear layer会投影到256的空间，然后reshape为重构图片。

损失函数为MSE

只在masked patches上计算loss





### Conclusion

作者的工作在ImageNet数据集上，通过一个自编码器可以学习到可以媲美有标号的效果

尽管图片的语义信息没有NLP那样明显，但是MAE仍然能取得很好的效果，它确实能够学习到一个隐藏的比较好的语义表达。

### 总结

- 利用ViT来做跟BERT一样的自监督学习
- ViT已经做过这个事了，MAE进行了拓展
  - MASK的块要更多，使得剩下的那些块，块与块之间的冗余度没那么大，使整个任务变得更加复杂
  - 使用Transformer的decoder，使得恢复信息更加简单
- 加上ViT之后的各种技术，使得MAE的训练更加鲁棒
- **简单的想法+非常好的结果+详细的实验**





## FGSM

### 论文地址

[fgsm]: file://E:/研究生学习/论文/研0/1412.6572_fgsm_goodfellow.pdf

### 期刊/会议

2015 ICLR

### Title

Explaining and harnessing adversarial examples

解释和利用对抗性样本

### Author

Ian.J.Goodfellow@google



### 论文主要创新点

- 创造出一种用于深度网络的通用攻击方法，同时针对该方法创立了对应的防御方案
- 作者通过分析现代神经网络内部结构，发现现有的网络结构大多数是线性或近似线性，而这样的网络对于样本的细微差别会有很大的反应。

### Abstract

包括神经网络在内的几种机器学习模型**很容易对adversarial examples进行错误分类**。这些对抗样本是**通过对数据集中的样本应用较小但蓄意的会导致最坏情况的扰动而形成的输入**，因此，**被扰动的输入导致模型以高置信度输出错误的答案**。

早期解释这种现象主要关注于非线性和过拟合。相反，**我们认为神经网络容易受到对抗性扰动的主要原因是它们的线性特性。**

这种解释得到新的定量结果的支持，同时对有关它们的最有趣的事实进行了首次解释：**它们在体系结构和训练集之间的概括**。而且，这种观点产生了一种**简单快速的生成对抗样本的方法**。使用这种方法**为对抗训练提供示例**，我们减少了MNIST数据集上maxout网络的测试集错误

### Introduction

Szegedy等人做出了一个有趣的发现：几种机器学习模型（包括最新的神经网络）**容易受到对抗样本的攻击**。也就是说，这些机器学习模型对这些样本进行了错误分类，这**些样本与从数据分布中得出的正确分类的样本仅稍有不同**。在许多情况下，在训练数据的不同子集上训练的、具有不同体系结构的各种模型都会将对抗样本错误分类。这表明**对抗样本暴露了我们训练算法中的基本盲点**。

这些对抗样本的原因仍然是一个谜团。一种推测性解释是由于深度神经网络的极端非线性性，也许是由于模型平均不足和纯监督学习问题的正则化不足所致。我们证明这些推测性假设是不必要的。**高维空间中的线性行为足以引起对抗样本**。这种观点**使我们能够设计一种快速生成对抗样本的方法**，从而使对抗性训练切实可行。我们表明，**对抗性训练可以提供dropout以外的正则化收益**。通用的正则化策略（例如dropout、预训练和模型平均）并不能显着降低模型对付对抗样本的脆弱性，但改用非线性模型族（如RBF网络）可以做到。

我们的解释表明，**设计模型因其线性而易于训练，而设计模型则利用非线性效应来抵抗对抗性扰动，这之间存在根本的矛盾**。从长远来看，**通过设计可以成功地训练更多非线性模型的更强大的优化方法，可以避免这种折衷**。

### Related work

Szegedy等人展示了神经网络和相关模型的多种有趣特性，与本文最相关的是：

- Box-constrained L-BFGS可以可靠地找到对抗样本；
- 在某些数据集（例如ImageNet）上，**对抗样本与原始样本是如此接近，以至于人眼无法区分这些差异**；
- 在许多情况下，**在训练数据的不同子集上训练的、具有不同体系结构的各种模型都会将对抗样本错误分类**；
- Shallow softmax回归模型也容易受到对抗样本的影响；
- 对抗样本的训练可以使模型正规化，但是，由于需要在内部循环中进行昂贵的约束优化，因此这在当时不切实际；

**现代机器学习模型——potemkin village（表面上很厉害，但是内部很脆弱）**

尽管线性分类器具有相同的问题，但通常将这些结果特别解释为深度网络中的缺陷。我们认为对这一缺陷的了解是修复它的机会。



### The Linear Explanation of Adversarial Examples

许多问题中，单个输入特征的精度受到限制。 例如，**数字图像通常每个像素仅使用8位，因此它们会丢弃低于动态范围1/255的所有信息**。

**如果扰动 η 的每个元素都小于特征的精度，分类器对输入x的响应与对抗输入 x~=x+η 的响应是不合理的**
$$
W_T\tilde{x}=W^Tx+W^T\eta
$$

$$
对抗性扰动会导致增长W^T\eta，通过赋予\eta=sign(w)，可以在\eta上受到最大范数约束的情况下最大化此增量\\
$$

**如果简单线性模型的输入具有足够的维度，则可以具有对抗样本。**



#### 总结

在该小节中，作者探讨了当输入数据空间被线性划分时，为什么机器学习模型容易受到对抗性示例的影响。具体来说，**如果一个机器学习模型是由一组线性函数组成的，那么输入空间会被分割成一系列的决策区域。然后，通过微小的扰动，可以将输入数据移动到不同的决策区域，从而导致模型产生不正确的输出**

这种现象在一些基于线性模型或者在输入空间被线性划分的情况下尤为明显。然而，即使是深度神经网络等复杂模型，**当其在高维空间中变得足够线性时，也可能容易受到对抗性示例的攻击**



### Linear Perturbation of Non-Linear Models

线性模型的廉价的分析性扰动也会破坏神经网络。

#### 最优最大范数约束下的扰动

**optimal max-norm constrained pertubation**
$$
\eta=\varepsilon sign(\nabla_xJ(\theta,x,y))\\
\theta为模型参数，x为模型输入，y为和x关联的target，J(\theta,x,y)是loss函数
$$
**作者将其称为生成对抗样本的”快速梯度符号方法“(fast gradient sign method,FGSM)**

<img src=".\img\FGSM\figure1.png" alt="figure1" style="zoom:80%;" />

### 补充：Attack Approach

#### 基本原理

<img src=".\img\FGSM\attack approach2.png" alt="attack approach2" style="zoom:33%;" />

<img src=".\img\FGSM\attack approach.png" alt="attack approach" style="zoom:33%;" />

- 和训练一个模型一样，在每个iteration中都会计算梯度
- 不过此时的梯度不是模型的参数对于loss的梯度，而是输入的x（很长的vector）对于loss的梯度
- 不过x是有限制的，这里是d(x<sup>0</sup>,x)<=ε，因此需要判断
- 假设用的限制d是L-∞，x0在图中的位置，那么x_t的位置只能在蓝色框之内
- 当x_t的位置超出蓝色框时，就需要fix拉回来

#### 基于梯度的攻击方法很多，但它们都基于两个基本思路

- 对约束标准做改动，如L2-norm，L-infinity等

- 对优化方案做改动

<img src=".\img\FGSM\base_idea.png" alt="base_idea" style="zoom:80%;" />



### FGSM原理

#### FGSM基本过程

FGSM方法通过计算输入数据的损失函数对输入数据的梯度，然后用符号函数sign将梯度值变为+1或者-1。将输入按照梯度方向加上一定的扰动，从而生成对抗样本。这些对抗样本人眼很难看出，但是很容易引起神经网络错误分类的高置信度。

#### 如何对样本添加噪音从而造成网络误分类

对于一个样本来说，其分类结果是由深度网络中大量参数和激活函数的形式所决定的，**如果以某种方式改变样本使得激活函数朝着反方向变化**，那么这种变化会形成“雪球效应”使得分类器改变最终的分类概率。

<img src=".\img\FGSM\fgsm示意图.png" alt="fgsm示意图" style="zoom:50%;" />

- FGSM将网络参数θ保持不变，损失函数L对原始输入x<sup>0</sup>进行求导，若值为正数，则取1；值为负数，则取-1，得到Δx
- 这样梯度要么为1，要么为-1，再将学习率替换为扰动约束ε（一个很大的值）
- 扰动约束ε为L-infinity时，扰动空间就是一个正方形
- 这样得到的x就一定落在四个角落上，一步到位，不会超出限制



#### 为什么这样做有攻击效果

就结果而言，攻击成功就是模型分类错误，**就模型而言，就是加了扰动的样本使得模型的loss增大**。 而**所有基于梯度的攻击方法都是基于让loss增大这一点来做的**

FGSM产生的对抗样本之所以能够使得模型误分类，是因为它利用了深度神经网络中的**线性性质和梯度信息**来**引导输入数据的微小扰动**，从而改变模型的预测结果。

FGSM的**核心思想**是在**保持对抗样本与原始样本相似性**的前提下，**找到能够最大程度地改变模型预测结果的最小扰动。**

#### 为什么不直接使用导数，而要用符号函数求得其方向

1. FGSM是典型的无穷范数攻击，那么我们**在限制扰动程度的时候，只需要使得最大的扰动的绝对值不超过某个阀值即可**。 而我们**对输入的梯度，对于大于阀值的部分我们直接clip到阀值**，对于小于阀值的部分，既然对于每个像素扰动方向只有+-两个方向，而现在方向已经定了，那么为什么不让其扰动的程度尽量大呢？ 因此**对于小于阀值的部分我们就直接给其提升到阀值，这样一来，相当于我们给梯度加了一个符号函数了**
2. 由于FGSM这个求导更新只进行一次，如果直接按值更新的话，可能生成的扰动改变就很小，无法达到攻击的目的，因此我们只需要知道这个扰动大概的方向

#### FGSM进一步解释

FGSM的原作者在论文中提到，神经网络之所以会受到FGSM的攻击是因为：

1. **扰动造成的影响在神经网络当中会像滚雪球一样越来越大，对于线性模型越是如此**。 而目前神经网络中倾向于使用Relu这种类线性的激活函数，使得网络整体趋近于线性。 
2. **输入的维度越大，模型越容易受到攻击**。

<img src=".\img\FGSM\fgsm-1.png" alt="fgsm-1" style="zoom:50%;" />

<img src="E:\深度学习\网络模型笔记\img\FGSM\fgsm-2.png" alt="fgsm-2" style="zoom:50%;" />

可以看到，对于一个简单的线性分类器，loss对于x的导数取符号函数后即w，**即使每个特征仅仅改变0.5，分类器对x的分类结果由以0.9526的置信概率判断为0变成以0.88的置信概率判断为1.**　　　



#### FGSM存在的问题

- 如果**激活函数是非线性函数**，那么，会导致**在ϵ的范围内，找不到合适的扰动骗过分类器**。其原因在于：线性函数或近似线性函数对于X的梯度是基本恒定的，因此只要沿着梯度方向，一定可以以最小的代价最快的速度增大loss，但是，**非线性函数对于每一点的梯度都不一样**。
- 对于界限ϵ，需要自己确定

#### FGSM的改良

- Iterative FGSM
- PGD：PGD是FGSM的一种迭代版本。它对抗性地优化输入数据，进行**多次迭代**，**每次都在一定范围内向着梯度的方向移动一小步**。PGD通常比FGSM更强大，因为它可以生成更具挑战性的对抗性样本。
- DeepFool：DeepFool是另一种有效的对抗性样本生成方法。它利用线性化的方式来寻找最小扰动，使得模型的输出从正确分类到错误分类。
- CW攻击：Carlini和Wagner攻击是一种强大的对抗性样本生成方法。它通过最小化添加的扰动的范数，并在约束条件下找到最小扰动，以使得模型误分类。

### Adersarial Training of Linear Models Versus Weight Decay

**线性模型与权重衰减的对抗训练**



### Adersarial Training of Deep Networks

Szegedy等人(2014b)表明，通过**训练对抗样本和干净样本的混合，可以对神经网络进行一定程度的正则化**。关于对抗样本的训练与其他数据增强方案有所不同。通常，人们会**使用转换（例如预期在测试集中实际发生的转换）来扩充数据**。相反，这种形式的数据增强使用了不太可能自然发生的输入，但暴露了以模型概念化其决策功能的方式的缺陷。

作者使用了两个方法提高了性能

- 加大模型容量，每层使用1600个单元。
- 在对抗性验证集错误上使用了early stopping，使用此标准来选择要训练的时期数，然后我们对所有60000个示例进行了再训练。使用随机种子生成器的五种不同种子进行五次不同的训练，这些随机数生成器用于选择小样本训练样本，初始化模型权重，并生成dropout掩码。

**模型还具有了对抗对抗样本的能力**。在没有对抗训练的情况下，基于快速梯度符号法的同类样本中，这种模型的错误率为89.4％。经过对抗训练，错误率降至17.9％。

对抗样本可在两个模型之间转移，但**对抗性训练的模型显示出更高的鲁棒性**。通过原始模型生成的对抗样本在对抗训练模型上的错误率为19.6％，而通过新模型生成的对抗样本在**原始模型（没有进行对抗训练**）上的错误率为40.9％。

当经过对抗训练的模型对一个对抗样本进行了错误分类时，其预测confidence仍然很高。错误分类示例的平均置信度为81.4％

 当数据受到对抗样本的干扰时，对抗训练过程可以看作是使最坏情况的错误最小化。

讨论了在输入加扰动还是在隐藏层加扰动比较好，作者认为如果模型具有对抗对抗样本的能力，那么在隐藏层中加入扰动也能起到正则化的作用，不然的话还是在输入加扰动更好。

### Conclusion

1. 对抗样本可以解释为高维点积的属性，它们是模型过于线性而不是非线性的结果；
2. 可以将对抗性示例在不同模型之间的泛化解释为：对抗性扰动与模型的权重向量高度一致，并且不同模型在训练以执行相同任务时会学习相似的功能；
3. 最重要的是扰动的方向，而不是空间中的特定点。空间中没有充满对抗样本的口袋，这些对抗样本像有理数一样精确地贴图了实数；
4. 因为这是最重要的方向，所以对抗性扰动会在不同的干净示例中进行概括；
5. 我们介绍了一系列用于生成对抗样本的快速方法；
6. 我们已经证明，对抗训练可以导致正则化； 比dropout更正则化；
7. 我们进行的控制实验无法使用更简单但效率更低的正则器（包括L1权重衰减和添加噪声）来重现此效果；
8. 易于优化的模型很容易受到干扰；
9. 线性模型缺乏抵抗对抗性扰动的能力。仅应训练具有隐藏层的结构（适用通用逼近定理），以抵抗对抗性扰动；
10. RBF网络可抵抗对抗样本；
11. 受过训练以模拟输入分布的模型不能抵抗对抗样本；
12. 集成模型不能抵抗对抗样本；



## PGD

### 论文地址

[pgd]: file://E:/研究生学习/论文/研0/1706.06083_pgd.pdf

### 期刊/会议

ICLR 2018

### Title

 **Towards Deep Learning Models Resistant to Adversarial Attacks**



### Author

MIT



### Abstract

为了解决神经网络易受对抗样本攻击的问题，作者**通过鲁棒优化的角度研究了神经网络的对抗鲁棒性。**

本文提出的方法提供了**一个广阔、统一的观点来看待对抗样本**的问题。

这些方法使作者可以**可靠的训练网络**。

特别的，本文提出的方法**在某种程度上提出了一种可以防御住任意攻击思路**，使其对各种对抗性攻击的抵抗力大大提高

提出了针对一阶攻击的安全概念

### Introduction

怎样训练一个对对抗攻击有较好鲁棒性的模型

如何训练对对抗样本具有鲁棒性的深度神经网络是一个比较重要的问题。现在有相当多的工作为对抗性环境提出各种攻击和防御机制：

- 防御性蒸馏（defensive distillation）
- 特征压缩（feature squeezing）
- 其他对抗样本检测方法

我们永远无法确定给定的攻击会在上下文中找到“最具对抗性”的例子，或者特定的防御机制会阻止某些定义明确的对抗性攻击的存在（**即不能明确的知道这些方法的适用性和适用范围**）



作者从**鲁棒优化的角度研究神经网络的对抗鲁棒性**。使用一个**鞍点（min-max）公式**来严格描述对抗鲁棒性问题。这种表述使我们能够**精确地确定我们想要实现的安全保证的类型**，即**我们想要抵抗的广泛攻击类型或对哪类攻击方法适用（与仅防御特定的已知攻击相反）**。这种表述还使我们**能够将攻击和防御都放到一个通用的理论框架中**（不同于以往的用于对抗样本上）。**对抗训练直接对应于优化该鞍点问题**。

#### 主要贡献

- **给出了一种提高模型鲁棒性的方案**。尽管提出的鞍点问题是个非凸性和非凹性的问题，但它的**根本的优化问题是可解决的**。特别是，我们提供了有力的证据，证明**一阶方法可以可靠地解决此问题**。本文使用的是`Projeccted Gradient Descent（PGD）`方法，这是一种**利用局部一阶信息求解**的优化方法（**局部线性是成立的**）
- **模型容量对对抗鲁棒性起着重要的作用**。**为了可靠地抵抗强大的对抗攻击，网络的模型容量需要大大的大于仅有干净样本情况下所需要的容量**。这表明，**对抗鲁棒模型的边界可能比正常模型的边界更加复杂**。
- **基于PGD对抗训练给出了一些结果与结论**，我们在MNIST和CIFAR10上训练了网络，这些网络可抵抗各种对抗性攻击。我们的方法**以优化上述鞍点公式为基础**，并**将 PGD 作为可靠的一阶对抗攻击**



### An Optimization View on Adversarial

#### 对抗鲁棒性的优化视角

假设data服从D分布，标签y同理，loss function L(θ,x,y)是cross entropy，θ是model parameters。训练模型就是求解一个最优参数θ，使得loss function最小化，即最小化经验风险(Empirical risk minimization，ERM)
$$
E_{(x,y)\sim D[L(x,y,\theta)]}
$$
尽管ERM在很多任务上取得了成功，但是却不能增加模型的鲁棒性。

为了提高鲁棒性，需要适当增强ERM范式。我们的方法不是采用直接专注于提高针对特定攻击的鲁棒性的方法，而是**提出了一个具有通用性的范式**。

- 第一步是指定攻击模型，即**精确定义我们的模型应该抵抗的攻击（用于生成对抗样本）**。对于每个数据点 x ，我们引入一组允许的扰动 S⊆R<sup>d</sup> ，以正规化算法的操纵力。
- 对风险期望进行修正，不同于ERM，而是首先要基于原始干净样本生成对抗样本，然后再基于对抗样本求解风险期望，如下所示

#### min-max最优化框架

接下来，通过结合算法来修改结构风险的定义，在输入中加入扰动，这就产生了以下鞍点问题（2.1）：

<img src=".\img\PGD\鞍点问题.png" alt="鞍点问题" style="zoom:80%;" />

我们的观点源于将鞍点问题视为**内部最大化问题和外部最小化问题的组合**。

- **内部最大化旨在找到给定数据样本x的对抗样本δ（δ∈S），以获得尽可能高的loss（这正是攻击神经网络所期望的：loss越大，说明对抗攻击越有效）。**
- **外部最小化是旨在找到模型参数θ，以使得内部攻击问题给定的`对抗损失`最小化（即优化网络，基于内部对抗样本最小化损失函数，这正是使用对抗训练技术训练鲁棒性模型所期望的）。**
- 其次，**该范式也对鲁棒模型应该满足的条件提出了明确的目标**（**通过优化损失函数实现**）。特别地，**当通过该范式求解得到的网络参数有消失的风险时，可能正说明此时的模型具有对抗鲁棒性。**

本文**在深度神经网络的背景下研究了该鞍点问题的结构**，然后，这些调查将我们带到了一种训练技术上，**该训练技术可以产生对各种对抗性攻击具有高度抵抗力的模型**。

相关工作上，攻击部分回顾了FGSM和I-FGSM，防御部分回顾了对抗训练。

#### PGD和BIM的区别

**Kurakin等人(2016)提出的BIM方法是FGSM的迭代版本. **

**Athalye等人（2018）发现的PGD攻击是BIM的变体**, **它以均匀的随机噪音作为初始化**,  并且是最强大的一阶攻击方法之一. 

**PGD只是比BIM添加了一个初始的随机扰动了**。



### Towards Universally Robust Network

#### 面向通用的鲁棒网络

当前关于对抗样本的工作通常集中于特定的防御机制，或针对这种防御的攻击。公式（2.1）的一个重要特征是，**获得较小的对抗损失即可保证没有攻击可以欺骗网络**。**根据定义，因为这样训练得到的模型对所有的干扰`loss`都很小，所以模型不存在“对抗扰动”是可能的**。

但是，对于求解该范式存在一定的问题，因为**不管是内层的最大化优化问题还是外层的最小化优化问题，都是一个`nonconvex`问题**。本文的一个很重要工作就是给出了如何求解该问题的方法。最后，**实验也表明，只要训练的网络足够大，模型的鲁棒就能大大提高**。

不幸的是，尽管由鞍点问题提供的总体保证显然是有用的，但**我们能否在合理的时间内找到一个好的解决方案尚不清楚**。**解决鞍点问题（2.1）涉及同时解决非凸外部最小化问题和非凹内部最大化问题**。

#### 对抗样本的前景展望

为了更详细地了解内部最大化问题，作者研究了MNIST和CIFAR10上多个模型的局部最大值。作者使用的是PGD方法（一种大规模约束优化的标准方法）

因为**一般的神经网络训练得到的模型具有很多的局部最小点**，**而理论上，要求解（2）式，需要得到内部优化问题的最大值。**为了得到更多的关于训练模型的`landscape`信息，本文选择从不同的起始点出发进行探索。

作者的实验表明，**从一阶方法的角度来看，内部最大化问题是可以解决的**。因为最终要将 `x′` 约束在`xi+S` 内。在`xi+S`会分布有很多的初始点，因此也将导致很多的局部最大值点。但是最后这些极大值点都会收敛到一个较集中的`loss values`（收敛接近）。这也验证了一个观点：**虽然神经网络是个高度非凸的问题，具有很多的局部极小值点，但是这些极小值是很相似的，也即该方法仍然可解**

##### 一些实验观察到的现象

- 作者观察到，**当对 x+S 内随机选择的起始点执行PGD攻击时，得到的损失会以相当一致的方式增加，并迅速达到平稳状态**。下图是从MNIST和CIFAR10评估数据集创建对抗样本时，交叉熵损失值的变化。

<img src=".\img\PGD\loss变化.png" alt="loss变化" style="zoom:80%;" />

- **每次运行都在同一自然样本周围的L-∞球中的均匀随机点处开始。几次迭代后，攻击算法损失平稳**。**优化轨迹和最终损失值也相当聚集**，尤其是在CIFAR10上。而且，**经过对抗训练的网络上的最终损失值明显小于其标准对应网络上的损失；**

- 进一步**研究最大值的集中度**，我们观察到，**在大量的随机初始化（random restarts）训练后，最终迭代的损失分布很集中，而没有极端的异常值**。下图是对于MNIST和CIFAR10评估数据集中的五个样本，由交叉熵损失给出的局部最大值的值。对于每个样本，我们从样本周围的 L-∞ 球中的105个均匀随机点开始投影梯度下降（PGD），然后迭代PGD直到损失达到平稳。**蓝色直方图对应于标准网络上的损失值，而红色直方图对应于经过对抗训练得到的loss。 对于经过对抗训练的网络，损失明显较小，并且最终损失值非常集中，没有任何异常值。**<img src=".\img\PGD\loss集中问题.png" alt="loss集中问题" style="zoom:80%;" />
- 为了证明最大值明显不同，我们还测量了它们之间的L2 距离和角度，并观察到**距离分布接近 L∞ 球中两个随机点之间的预期距离**，**并且角度接近 90∘** 。**沿着局部最大值之间的线段，损失值是凸的，在端点处达到最大值，并在中间减小一个恒定因子。但是对于整个段，其损失值要比随机点的损失值高得多**。
- 最后，我们观察到最大值的分布表明最近开发的对抗样本子空间视图不能完全捕获攻击的丰富性。尤其是，我们观察到**对抗性扰动的负内积与样本的梯度有关**，并且随着扰动规模的增加，与梯度方向的总体相关性会降低。



#### 一阶攻击方法

**通过上面的实验发现：**不管是正常训练的模型还是对抗训练的模型，最终的`loss`都是很接近（集中）的，这个集中现象说明了一个问题：即**对于所有的一阶攻击方法，只要对`PGD`鲁棒，则对其它所有的方法也鲁棒。**

**一阶攻击方法**是对抗性攻击中的一种类型，它**只使用了输入样本的一阶梯度信息**。**在一阶攻击中，攻击者仅考虑损失函数对输入样本的一阶导数，即关于输入样本的梯度**。例如FGSM

 我们的实验表明，**对于正常训练的网络和对抗训练的网络**，**PGD发现的局部最大值都具有相似的损失值**。这种集中现象提出了一个有趣的问题，即**针对PGD攻击的鲁棒性会产生针对所有一阶攻击算法的鲁棒性，即仅依赖一阶信息的攻击**。**只要对手仅使用损失函数相对于输入的梯度，我们就可以推测，它不会找到比PGD更好的局部最大值**。

**我们的实验表明，使用一阶方法很难找到这样更好的局部最大值：即使大量随机重启也找不到具有明显不同损失值的函数值**

**在某种意义上，依赖于一阶信息的攻击对于当前的深度学习实践来说是普遍的**

**如果我们训练网络使其对PGD 对抗具有强大的抵抗力，那么它将对包括所有当前方法的广泛攻击具有强大的抵抗力**。

同时，这个结论也具有一般性，即不仅适用于白盒问题， 也同样适用于黑盒攻击问题。

对攻击的迁移性也进行了分析。实验观察到：通过增加模型的容量且强化训练对手（使用`FGSM`或`PGD`训练，而不是对抗训练）能提高模型对迁移性攻击的抵抗能力。

#### 对抗训练的下降方向

前面的讨论表明，通过应用PGD可以成功解决内部优化问题。接下来讨论了公式2.1外部最小化问题，即**找到使“对抗性损失”（内部最大化问题的值）最小的模型参数**。

首先，在神经网络训练过程中，最小化损失函数（用于训练）的方法是`SGD`。对于（2）式给出的鞍点问题，通常直接求解是比较困难的，但是`Danskin's theorem`指出：

`**对内层最大值求梯度，确实就是求解梯度的下降方向。**`

**计算外部问题的梯度 ∇θρ(θ) 的一种自然方法是在内部问题的最大值处计算损失函数的梯度。这相当于用输入点的对应对抗性扰动代替输入点，并通常在扰动的输入上训练网络**。



<img src=".\img\PGD\外部问题loss变化.png" alt="外部问题loss变化" style="zoom:80%;" />

训练过程中对抗样本的交叉熵损失。这些图显示了在针对PGD攻击训练MNIST和CIFAR10网络期间，训练样本中的攻击损失如何演变。CIFAR10图中的急剧下降对应于训练步长的减小。这些图说明，我们可以不断降低鞍点公式（2.1）的内部问题的值，从而产生越来越强大的分类器。

### Network Capacity and Adversarial Robustness

#### 网络容量和对抗鲁棒性

 成功地从等式（2.1）解决问题不足以保证稳健而准确的分类。

对于训练神经网络而言，最终评价训练过程/网络性能的指标归根结底是`loss value`，即最终模型对对抗样本的损失。**一般地，认为一个小的`loss`说明训练出来的模型是较好的模型，并且也能对对抗样本表现出鲁棒性**。

对抗性强健的深度学习模型。对于可能扰动的固定集合 S ，问题的值完全取决于我们正在学习的分类器的体系结构。因此，**模型的架构能力成为影响其整体性能的主要因素**。在高层次上，以鲁棒的方式对样本进行分类需要更强大的分类器，因为对抗样本的存在将问题的决策边界更改为更复杂的样本

<img src=".\img\PGD\模型容量.png" alt="模型容量" style="zoom:80%;" />

- 对于原始简单的样本，最左侧情况，此时决策边界也简单，则需要使用的模型架构/容量也简单；
- 有对抗样本的情况，如中间图所示（ ℓ∞ ），此时如果仍然使用线性分类器，则已经不能很好的说明问题，图中的红点为此时的对抗样本；
- 需要更复杂的边界/模型，如最右边所示，才能表征问题
- 同时，还能说明一些问题，即**对抗训练出来的模型在测试集上的分类精度会有所下降**。根据上面的分析，**对抗训练得到的模型边界更加复杂，则对于干净样本来说模型此时是过拟合状态，即模型在测试集上的精度有所下降。**

#### **以下对不同模型容量在不同对抗样本下的训练结果进行实验：**

- 对于`MNIST`数据集，原始模型是2个卷积核（通道数分别是2、4）“+”一个全连接层（64神经元），卷积核后接 2×2 的最大池化， ε=0.3 。为了分析模型容量对结果的影响，每一个将卷积核的通道数和全连接层的神经元翻倍；

- 对于`CIFAR10`数据集，使用的是`wideResNet-32`模型

  模型说明：使用的wideResNet-32，如下所示：

  <img src=".\img\PGD\wideresnet_32.png" alt="wideresnet_32" style="zoom:80%;" />

  共有三种类型的`block`，`map size`分别为32、16、8，通道数分别为160、320、640（原始`ResNet-32`是16、32、64）.每种类型`block`内是5个卷积单元。

  **该网络使用PGD攻击时的ε=8.0 / 255** 

<img src=".\img\PGD\模型容量实验结果.png" alt="模型容量实验结果" style="zoom:80%;" />

- 对于Standard training，简单的模型就够了，不需要wide（可能过拟合）
- 在FGSM训练中，对`FGSM`对抗样本分类准确率最高；对`PGD`样本无效；正常样本会先下降；但是随着模型容量的增加又会趋于高精度
- 如果是PGD traning，那么它对PGD和FGSM都具有较好的对抗鲁棒性，且随着模型容量的增大，对抗鲁棒性提升。
- 正常训练的loss变化不大，但是`FGSM`和`PGD`的`loss`随着模型容量的增大会逐渐减小。

作者观察到的一些现象：

- **仅通过改变模型的容量就能有效提高模型的鲁棒性**。例如对于最左侧图片，对于正常训练模型，随着模型容量的增大，对`FGSM`对抗样本的精度也增高了
- **使用`FGSM`训练得到的模型不能提高模型的鲁棒性（对于扰动较大的情况）**。当使用FGSM生成的对抗样本训练网络时，我们观察到**网络对这些对抗样本过拟合**。这种行为被称为**标签泄漏(label leaking)**，它源于以下事实：**攻击算法产生了非常有限的一组对抗样本，网络会过拟合**。在扰动较小时，生成的对抗样本和`PGD`生成的很像，所以对`PGD`也有效，但是对于大扰动就失效
- **容量小的模型（weak models）可能并不能学到有效的结果**。**如果使用容量较小的模型去进行对抗训练（特别是`PGD`对抗训练），可能会最终使得模型学不到任何有价值的东西**。对于中间两幅图，特别是第三幅图，在模型容量较小时，模型对三类样本分类效果都很差。这种情况下可能会使得模型仅仅对一些固定的类别有效果（即使正常训练下该模型能有较好的性能），训练过程中，模型会牺牲对正常样本的分类效果来换取对对抗样本的分类精度，但因为模型容量较小，最终可能什么都没学到；
- 随着容量的增加，鞍点问题的损失减小。固定攻击的模型并对其进行训练，随着容量的增加，（2.1）的值会下降，这表明该模型可以更好地拟合对应的对抗样本
- 更大的容量和更强大的攻击算法会降低可转移性。要么增加网络的容量，要么对内部优化问题使用更强大的方法，都会降低所传递对抗性输入的有效性。**实验表明，随着网络容量的增加，训练网络和迁移网络间的梯度相关性越来越低。**

### Experiment：Adversarially Robust Deep Learning Models

 到目前为止，我们的实验表明，我们需要关注两个关键要素：**a）训练足够高的容量的网络，b）使用最强大的攻击算法**（PGD)

对于MNIST和CIFAR10，选择的攻击算法将是随机梯度下降（PGD），从围绕自然实例的随机扰动开始。这对应于我们的“完整”一阶攻击的概念，**该算法可以仅使用一阶信息就可以有效地最大化样本的损失**。由于我们正在为多个epoch训练模型，因此每批多次重新启动PGD并没有好处。下次遇到每个样本时，都将选择新的开始。

 在针对该攻击方法进行训练时，**我们观察到对抗样本的训练损失不断减少。此行为表明我们确实在训练过程中成功解决了最初的优化问题**

下图所示，分别是适用PGD在MNIST和CIFAR10上的训练loss曲线

<img src=".\img\PGD\外部问题loss变化.png" alt="外部问题loss变化" style="zoom:80%;" />

两种情况下train loss都会持续下降，表明训练过程确实有效

#### 所对比的模型

<img src=".\img\PGD\不同的模型.png" alt="不同的模型" style="zoom:80%;" />

- `A`表示使用`PGD`下的白盒攻击，使用不同的迭代步和`restarts`；
- `A'`表示模型和`A`一样，但是使用黑盒攻击方法进行攻击；
- 使用`PGD`作为`adversary`对抗训练时候的一些参数（`CIFAR10`）：
- steps：7
- α ：2/（2.0/255 = 0.0078）
- ε ：8/（8.0/255 = 0.03）
- 在测试阶段，仍然是`PGD`进行攻击，最终的扰动大小仍是 ε=8 ，但是迭代步分别为7、20。**其中20迭代步认为是最强攻击方式**。

#### MNIST上的实验

- 40 iterations of PGD
- step size = 0.01
- ε=0.3
- 两层卷积结构（分别有32和64个filters），每层跟随一个2x2 的最大池化，最后的fully connected layer 有1024个单元

<img src=".\img\PGD\MNIST上的实验.png" alt="MNIST上的实验" style="zoom:80%;" />

#### CIFAR10上的实验

- orginal ResNet and its 10x wider variant
- 7 steps of size 2
- ε = 8
- 对于最厉害的adversary，用了20 steps的PGD

<img src=".\img\PGD\CIFAR10上的实验.png" alt="CIFAR10上的实验" style="zoom:80%;" />

#### 一些结论

- 对于`MNIST`数据集，若直接使用干净样本训练，则在`evaluation set`上的精度可达`99.2%`，但是使用`FGSM`攻击准确率就下降到了6.4%。在对抗训练下，干净样本的分类精度降了一点点（`99.2%`-98.8%）,但是对攻击方法鲁棒性大大提高。
- 对于`CIFAR10`数据集，对抗模型在自然样本上的分类精度是`87.3%`，对抗训练后模型的精度虽没有像`MNIST`那么明显，但是也大大提高了；
- 模型使用的是`7-PGD`对抗训练得到，使用不同`K-PGD`进行攻击时，表现不同。其中`7-PGD`攻击分类精度有`50.0%`，使用`20-PGD`攻击时下降到`45.8%`;
- 通过更复杂的模型（容量），效果应该能进一步提高。

#### 补充了L-∞和L-2对比的实验

为了进一步对对抗训练模型的鲁棒性进行分析，本文又进行了两个实验，**分别是在测试阶段，对 ℓ∞ 下不同攻击扰动值 ε大小对分类精度的影响和 ℓ2 范数下模型的鲁棒性进行评估**

L-∞训练出来的模型更加鲁棒

<img src=".\img\PGD\L2和L∞对比.png" alt="L2和L∞对比" style="zoom:80%;" />

- **在ℓ2-bounded约束下，使用的不再是模型梯度的sign，而直接是梯度的方向，且对步长大小进行归一化**。最终，每次`PGD`攻击都使用100迭代步，同时设置步长为 2.5⋅ε/100 。这样仍然满足最终的大小是小于单步步长与迭代步数的乘积和（ 100⋅(2.5⋅ε/100)>ε
- 对于(a)、(b)图，主要比较其中的蓝色线和棕色线，即`PGD`对抗训练和`PGD`标准攻击。在`MNIST`数据集下，模型的表现在扰动超过某个阈值后立刻（大幅度）下降（`CIFAR10`是缓慢下降）。但是正常训练模型和对抗训练模型下降的阈值不同。**这种阈值不同现象可能是因为对抗训练根据给定的扰动不同而学习到了不同的新阈值。**
- 对于(c)、(d)图，两种格式范数约束下，随着扰动的增大（表示攻击强度变大），精度逐步下降（(a)、(c)图中的红虚竖线分别表示扰动是0.3和8的情况）；
- 此外，对于 ℓ2 约束下对`MNIST`的攻击，发现`PGD`方法，即使使用较大的ε 也不能攻击成功。
- 以下图为例，虽然最终分类错误了，但是已经不能较对抗样本，因为人眼已经能够识别出来。（也有文献指出，`PGD`在ℓ2约束下表现出了`overestimating`的性质）。这可能是因为学习到的网络将梯度信息隐藏了，这样`PGD`方法不再那么有效。若使用`decision-based`的方法进行攻击，则模型表现的较ℓ2约束下更脆弱<img src=".\img\PGD\l2的overfit.png" alt="l2的overfit" style="zoom:80%;" />

### Conclusion

- 本文的工作说明了神经网络，使用对抗训练可以抵抗对抗样本现象；
- 本文的实验也说明了一个问题，即使最终的训练任务是个非凸、多局部极值点的问题，但是问题仍然是可解的，因为这些极值点比较集中；
- 本文的方法/实验在`MNIST`数据集上表现很好，但是在`CIFAR10`上表现的不是太好，但已经能说明问题了。





## CW

### 论文地址

[cw]: file://E:/研究生学习/论文/研0/1608.04644_c&w.pdf



### 期刊/会议

2017 IEEE Symposium on Security and Privacy



### Title

Towards Evaluating the Robustness of Neural Networks



### Author

Nicholas Carlini, David Wagner

University of California, Berkeley

### Abstract

Defensive distillation是近来一项能增强模型鲁棒性的工作，能使得目前的攻击成功找到对抗样本的概率从95%下降到0.5%。

作者提出了3种攻击方法，证明defensive distillation并不能增强模型鲁棒性。这三种新的攻击算法以100％的概率在蒸馏和未蒸馏的神经网络上均成功。攻击是针对文献中先前使用的三个距离度量量身定制的

此外，我们建议在一个简单的可转移性测试中使用高可信度的对抗样本，我们证明该示例也可以用于打破防御性蒸馏。我们希望我们的攻击将被用作未来防御尝试的标准，以创建能够抵抗对抗样本的神经网络

### Introduction

**防御性蒸馏**是最近提出的一种针对神经网络对抗样本而提出的防御措施。初步分析证明是很有前途的：防御性蒸馏击败了现有的攻击算法，并将其成功概率从95％降低到0.5％。防御性蒸馏可以应用于任何前馈神经网络，只需要一个简单的重新训练步骤，并且是目前唯一针对对抗样本提供强大安全保证的防御措施之一。

通常，可以采用**两种不同的方法来评估神经网络的健壮性**：

- **尝试证明神经网络的下界，即尝试确定神经网络在不同情况下的最低性能水平。该方法虽然合理，但实际上很难实施，因为它需要对各种情况进行近似处理**。
- **构造证明上限的攻击，即网络容易受到何种程度的攻击。但是，该方法使用的攻击可能因为攻击的强度不足或者频繁失败而导致上限的评估失效**

 作者**创建了一组攻击，可用于构建神经网络鲁棒性的上限**。我们构造了三个新的攻击（在三个先前使用的距离度量标准下： L-0,L-2,L-∞ ），这些攻击成功地为防御性蒸馏网络上100％的图像找到了对抗样本。

**此外，作者提出的攻击带来的总失真更小。下图是在MNIST和CIFAR10数据集上，在防御蒸馏性网络上生成的对抗样本**：

<img src=".\img\CW\生成的对抗样本.png" alt="生成的对抗样本" style="zoom:80%;" />

甚至在ImageNet分类上，可以通过**仅更改每个像素的最低位**来导致 Inception v3  网络错误地对图像进行分类。这种变化是无法通过肉眼察觉的

#### **本文的主要贡献**

作者提出了一种新的攻击方法（业界常称之为**CW**），可以以几乎100%的成功率攻破适应性防御，并**成为检验防御模型效果的一个新的baseline**

- **针对 L0,L2,L∞ 的距离度量引入了三种新的攻击方式**。我们的攻击比以前的方法明显更有效。我们的 L0 攻击是第一个可能导致ImageNet数据集上的目标错误分类的攻击。
- 利用**这三种攻击方法攻破了防御性蒸馏**。
- 提出**使用高置信度的对抗性样本**在一个**简单的可传递性测试中（即迁移攻击，通过将已经生成的对抗性样本应用于另一个模型，并观察这些样本是否仍然具有对抗性质，从而测试对抗性在不同模型之间是否传递）**来评估防御方法，同时这种方法也成功攻破了防御性蒸馏。。
- 系统地**评估了不同损失函数对攻击效果的影响，发现损失函数的选择影响巨大。**



### Related Work

#### Threat Model

关键问题就变成了**我们必须增加多少失真才能导致分类发生变化**

我们在本文中假设对抗可以完全访问神经网络，包括模型结构和所有参数，并且可以白盒方式使用。这是一个保守且现实的假设：先前的工作表明，在黑盒访问目标模型的情况下，可以训练替代模型，然后通过攻击替代模型，我们可以将这些攻击转移到目标模型。

考虑到这些威胁，已经进行了各种尝试来构建能够增强神经网络健壮性的防御措施，**这些措施被定义为找到最接近原始输入的对抗样本的难易程度的一种度量**。

#### Neural Networks and Notation

就讲了下基础的神经网络和softmax函数

#### Adversarial Examples

在Target attack中，有三种方法来选择target class:

- 平均情况：在不是正确标签的标签之间随机选择目标类别
- 最好情况：对所有不正确的类别进行攻击，并报告最不难攻击的目标类别
- 最坏的情况：对所有不正确的类别进行攻击，并报告最难攻击的目标类别

#### Distance Metrics

用来衡量对抗样本和真是样本的近似都，有三种常见的L_p norms

- L0度量定义的是图像中被更改的像素个数，也是防御性蒸馏主要讨论的度量
- L2度量是干净样本和对抗样本之间的欧氏距离；
- L∞代表对抗样本中像素变化最大的值

#### Defensive Distillation

这种方法的核心思想是在训练神经网络时，将模型的预测输出转化为概率分布，并将这些“软化”的概率分布作为新的标签，然后使用这些标签来重新训练网络。

##### 主要步骤

1. **Softened Labeling**: 在原始的训练集上，使用已经训练好的神经网络来生成一个“软化”的标签分布。这意味着不再将一个样本的标签仅仅视为一个类别，而是**将每个类别的概率分布作为标签**。**这种“软化”的标签更能够表达模型对输入的不确定性**。
2. **Re-Training**: 使用这些“软化”的标签来重新训练神经网络。**在这个阶段，网络会被要求尽量优化以匹配“软化”的概率分布，而不仅仅是对输入样本进行硬分类**。这有助于网络学会更多的数据结构和特征，使其对于微小的扰动更具鲁棒性。
3. **Inference**: 在测试阶段，通过对网络的输出进行“软化”（即计算概率分布），可以减少对抗性攻击的成功率。攻击者通常会试图通过在输入数据中添加微小的扰动来欺骗网络，但**由于网络输出已经被“软化”，攻击者需要在更高维度的标签空间上进行攻击，从而变得更加困难**。

 为了防御性地蒸馏神经网络，首先要以标准方式在训练数据上训练具有相同架构的网络。当**我们在训练该网络时计算softmax时，将其替换为更平滑的softmax版本（将对数除以某个常数T）**。训练结束时，通过在每个训练实例上评估该网络并获取网络的输出标签来生成软训练标签

然后，**丢弃第一个网络并仅使用软训练标签**。**通过这些标签，训练第二个网络，而不是在原始训练标签上进行训练**。这训练了第二个模型，使其表现得像第一个模型，并且软标签传达了第一个模型学到的其他隐藏知识。

此处的关键见解是，通过进行训练以匹配第一个网络，我们有望避免针对任何训练数据进行过拟合。如果存在神经网络的原因是因为神经网络是高度非线性的并且在对抗样本所在的地方具有“盲点”，那么防止这种类型的过度拟合可能会消除那些盲点。



### Attack Algorithms

#### L-BFGS

(L-BFGS，Limited-momry Broyden-Fletcher-Goldfarb-Shanno)是一种**基于梯度**的优化方法，常用于**解决无约束非线性优化问题**。在L-BFGS攻击中，攻击者试图**通过最小化损失函数来找到使得网络在对抗性样本上产生错误分类的输入**。攻击者通过**迭代**地**调整输入样本中的扰动**，**使得网络的预测结果与攻击者所期望的目标类别之间的损失最小化（L2距离）**。

##### 主要步骤

1. **选择目标样本**: 攻击者选择一个目标样本，希望将其误分类为其他类别。
2. **计算损失函数梯度**: 攻击者**计算目标样本上的损失函数关于输入数据的梯度**。这需要对目标神经网络进行前向传播以计算预测结果，并进行反向传播以计算梯度。
3. **设置初始扰动**: 攻击者为目标样本添加一个初始扰动，通常是一个小的随机噪声。
4. **使用L-BFGS算法进行优化**: 攻击者使用L-BFGS算法来最小化损失函数。这涉及**迭代地更新输入数据中的扰动，以减小损失**。
5. **生成对抗性样本**: 经过多次迭代，L-BFGS算法找到的**最优扰动会被添加到目标样本上，从而生成一个对抗性样本**。这个对抗性样本在保持视觉上相似性的同时，会导致神经网络产生错误的预测。

#### Fast Gradient Sign

参考FGSM论文，在此不再赘述

#### JSMA

Papernot等人介绍了一种在 L0 距离下优化的攻击，称为基于雅可比的显着性图攻击（JSMA）。

JSMA攻击的核心思想是**在输入数据中找到对分类结果影响最大的特征**，然后**对这些特征进行微小的扰动**，以使神经网络产生错误的分类。攻击的过程可以看作是在输入空间中寻找最有效的扰动，以最大程度地改变网络的预测

##### 主要步骤

1. **选择目标样本**: 攻击者选择一个目标样本，希望将其误分类为错误的类别。
2. **计算Jacobian矩阵**: 攻击者计算目标神经网络在目标样本上的**Jacobian矩阵，该矩阵描述了网络输出相对于输入特征的变化率。**
3. **选择影响最大的特征**: 攻击者**根据Jacobian矩阵的信息，选择对分类结果影响最大的一些特征作为攻击目标**。这些特征对于误分类起到关键作用。
4. **生成扰动**: 攻击者根据选择的特征和Jacobian矩阵信息，**生成一个微小的扰动，使目标样本在选择的特征上发生变化**。这个扰动的大小和方向都是根据目标函数来确定的，以使分类结果产生错误。
5. **重复迭代**: 重复上述步骤，直到目标样本被误分类为错误的类别，或者达到预定的迭代次数。
6. **生成对抗性样本**: 最终生成的带有微小扰动的样本会导致神经网络错误地将其分类为攻击者所期望的错误类别。

JSMA在倒数计算中使用倒数第二层Z的输出logits：未使用softmax F的输出。我们将此称为JSMA-Z攻击。但是，当作者将此攻击应用于其防御性蒸馏网络时，他们会修改攻击，使其使用F而不是Z。换句话说，**他们的计算使用softmax(F)的输出而不是logits(Z)。我们将此修改称为JSMA-F攻击**。当图像具有多个颜色通道（例如RGB）时，此攻击将针对每个独立更改的颜色通道的 L0 差异视为1（因此，如果一个像素的所有三个颜色通道均发生更改，则 L0 范数将为3）。



#### Deepfool

Deepfool攻击的核心思想是在输入数据中**添加一个最小的扰动**，以使神经网络产生错误的分类结果。这个扰动被设计为**将输入数据从其真实分类决策边界推离，使其更接近于错误的分类决策边界**。攻击的过程可以看作是**在一个多维特征空间中寻找最小的扰动，使得样本从一个类别的决策区域穿越到另一个类别的决策区域**。

##### 主要步骤

1. **选择目标样本**: 攻击者选择一个目标样本，希望将其误分类为错误的类别。
2. **计算梯度和边界**: 攻击者计算目标样本**在当前分类决策边界上的梯度**。然后，**找到距离目标样本最近的错误分类决策边界**。
3. **生成扰动**: 攻击者根据计算得到的梯度信息和边界信息，**生成一个微小的扰动，将目标样本从正确分类决策边界推离**，使其更接近错误分类决策边界。
4. **重复迭代**: **重复上述步骤，直到目标样本被误分类为错误的类别，或者达到预定的迭代次数**。
5. **生成对抗性样本**: 最终生成的带有微小扰动的样本会导致神经网络错误地将其分类为攻击者所期望的错误类别

Deepfool攻击的优点之一是它可以生成相对较小的扰动，同时保持对人类视觉来说是不可感知的。然而，它可能不如一些其他攻击方法在生成强大的对抗性样本方面效果好，因此在实际应用中可能需要与其他防御方法结合使用。此外，Deepfool攻击也需要一定程度的计算资源和时间来进行迭代计算。



#### 区别

1. **优化策略**：
   - PGD：PGD使用**梯度下降**的方法来找到使**损失函数最大化**的对抗性扰动。它通过**迭代地在扰动的方向上移动一小步，然后将扰动投影回一个预定范围内，以确保生成的对抗性样本不会过分远离原始样本**。
   - L-BFGS：L-BFGS采用的是一种**更复杂**的优化策略，旨在**寻找损失函数的全局最小值**。它在每次迭代中使用一些历史信息来更新扰动，通常需要更多计算资源。
   - JSMA：JSMA攻击旨在最大程度地改变网络输出中的目标类别的概率，以**使目标样本误分类为攻击者所期望的目标类别**。**它关注特定类别的概率差异，以导致错误的分类**。
   - Deepfool：Deepfool是一种迭代的攻击方法，但它的**优化目标是最小化网络对目标样本的分类置信度，使其误分类为目标类别，而不是最大化损失函数**。
2. **迭代过程**：
   - PGD：PGD通常进行多次迭代，**每次迭代根据当前的梯度信息朝着损失函数最大化的方向来调整扰动的大小和方向**。每一次迭代都尝试更进一步地扰动数据，从而逐步构建更强的对抗性样本。
   - L-BFGS：L-BFGS通常也需要多次迭代，但**每次迭代都会尝试更新扰动以最小化损失函数**。由于**L-BFGS使用了历史信息**，它在某些情况下可能会更快地收敛。
   - JSMA：JSMA是一种迭代的攻击方法，它在**每次迭代中选择对目标类别和其他类别影响最大的特征，并计算扰动以最大程度地改变网络输出**。它在寻找扰动方向和大小时采用了一种启发式策略
   - Deepfool：Deepfool在每次迭代中通过计算梯度信息来更新扰动，使其朝着**使目标样本尽可能接近错误分类决策边界的方向**移动。
3. **效果与适用性**：
   - PGD：PGD通常被认为是一种较为通用和强大的对抗性攻击方法，适用于各种神经网络结构和数据分布。它在生成有效的对抗性样本方面表现出色。
   - L-BFGS：L-BFGS在某些情况下可能会更快地找到一些最优解，但也可能在某些情况下受限于网络结构和数据分布。由于其计算复杂性较高，它可能不适用于所有场景。
   - Deepfool：Deepfool方法试图使目标样本在最小的扰动下误分类为目标类别，但可能在某些情况下不如其他攻击方法那么强大或通用。
   - JSMA：JSMA攻击需要计算目标样本的Jacobian矩阵，涉及一定的计算复杂性。它可能需要更多的计算资源和时间来进行迭代计算

### Experimental Setup

#### 模型结构和参数

<img src=".\img\CW\模型结构和参数.png" alt="模型结构和参数" style="zoom:60%;" />

- SGD优化器：momentum = 0.9
- 在CIFAR-10上明显过拟合
- 在ImageNet上用的是pre-trained模型：Inception v3，接受299x299x3的输入图片



### Our Approach

#### 对抗攻击优化函数的最基本形式

<img src=".\img\CW\公式1.png" alt="公式1" style="zoom:67%;" />

- 图像x是固定的，我们的目标是找到一个小的扰动δ，使得D(x,x+δ)最小。
- D是一种距离度量，作者尝试了L0，L2和L∞
- 我们通过将其公式化为可以由现有优化算法解决的适当优化实例来解决此问题。有很多可能的方法可以做到这一点。我们探索了表述的空间，并根据经验确定了哪些表述可导致最有效的攻击

#### 目标函数

因为约束C(x+δ)=t是高度非线性，现有算法难以直接求解上述公式。因此，需要定义关于分类的损失函数f，使得当且仅当f(x+δ)<=0时，C(x+δ)=t成立，从而联立D与f。

<img src=".\img\CW\目标函数.png" alt="目标函数" style="zoom:67%;" />

- c是一个常数
- f(x+δ)是目标函数，它有以下7种选择
- <img src=".\img\CW\目标函数的选择.png" alt="目标函数的选择" style="zoom:67%;" />

在用Lp范数实例化度量距离D之后，问题变为：给定x，找到可求解的δ

![最终问题](.\img\CW\最终问题.png)

根据经验，超参数c的最佳方法是在满足解f(x<sup>*</sup>)<0成立的前提下，选择最小值，因为这样可以使得训练过程中，梯度下降算法同时优化函数里的两个项，而不是做“有偏”的优化。

作者用损失函数f6进行了实验，将变量c在0.01到100的log空间内均匀地取值，统计攻击成功率及扰动的L2距离，如下图所示：

<img src=".\img\CW\c的取值.png" alt="c的取值" style="zoom:80%;" />

- 当c>1时，增大c几乎不会再提高攻击的成功率
- 但是扰动的L2距离会不断增加
- 因此在攻击成功的前提下，应尽可能选择c的最小值。

#### 盒约束（像素值上下界）

数字图像能表示的像素值范围是有限的，在**经过归一化之后，像素值应该在[0,1]的范围内**。为了确保修改产生有效的图像，对δ进行了约束：**对于所有i，我们必须使 0≤xi+δi≤1** 。 在优化文献中，这称为`盒约束`

如果在攻击生成之后，直接把像素值裁剪到[0,1]的范围内，那显而易见，会破坏形成的攻击效果。为此，我们研究了解决此问题的三种不同方法:

<img src=".\img\CW\三种约束方法.png" alt="三种约束方法" style="zoom:50%;" />

- **PGD**：在攻击的每一个迭代步完成后，都将像素裁剪到[0,1]的范围内，那么算法就会尝试在这个范围内寻找成功攻击的点。但是这样做的缺点是，每一轮迭代完成后，由于裁剪操作的存在，实际上输入下一个迭代步的图像是已经发生了变化的，已经有研究发现，在优化步骤比较复杂的梯度下降算法（例如加动量的梯度下降）中，这样的裁剪操作会导致很差的效果。
- **Clipped gradient descent**：既然不想在迭代步之间修改x + δ的值，那么就把限制加在别的地方，比如损失函数f的定义域这里，将损失函数f替代为**f(min(max (x + δ,0),1))** ，但是这样又会带来另一个问题，设想这样一个场景：假如在某个迭代步中，x + δ的范围超出了1的限制，但是在下一个迭代步中，该处的像素值回调到[0,1]之间并且攻击效果提升了，那么由于损失函数的限制，在第一个迭代步中，梯度为0，因此实际上没有任何攻击扰动的变化，那么后一个迭代步的攻击效果提升也就自然无法被找到了。
- **Change of variables**：为了解决上述的两个问题，本文提出了换元的方法，通过引入新的变量`w`使得扰动的表达式变为`δ = (tanh(w_i)+1)/2 - x_i`，由于tanh函数本身的值域范围为[-1,1]，这样就保证了x + δ的范围在[0,1]之间，同时，因为tanh函数在全定义域内导数均不为0，也就克服了Clipped gradient descent方法的缺点。这样做还有另一个好处，由于像素值的限制靠的是损失函数本身的值域而非人工截断，那么就可以引入一些不支持人工截断的优化器，例如Adam，作者通过实验发现Adam在保证攻击效果的前提下实现了更快的收敛速度。

#### 方法的评估

作者将7种损失函数和3种设置“box constrain”的方式逐一进行组合，测试攻击的成功率以及扰动的L2距离

<img src=".\img\CW\不同目标函数f和盒约束方法的评估.png" alt="不同目标函数f和盒约束方法的评估" style="zoom:80%;" />

为了选择最佳的c，我们对c执行20次二分搜索迭代。对于每个选定的c值，我们使用Adam优化器运行10000次梯度下降迭代。

- 最佳目标函数和最差目标函数之间存在质量差异的三个因素。
- 处理盒约束的方法的选择不会对结果的质量产生明显的影响。

#####  为什么有些损失函数比其他损失函数更好？

- 当c=0时，图像扰动的这一项将不会产生任何梯度，那么**迭代的过程对攻击的效果不会有任何改进**。
- **较大的c通常会导致梯度下降的初始步骤过于贪婪地执行，仅沿最容易减小f的方向行进，而忽略了D损失，因此导致梯度下降找到了次优解**。换句话说就是：**当c的值非常大时，由于该项对目标函数的值影响过大，相比之下距离项对目标函数的影响会显得很小，因此算法会采取“贪心”的策略，即只优化扰动项，这样就会导致优化问题最终陷入次优解，影响最终的效果**。这意味着对于损失函数f1和f4，没有好的常数c在整个梯度下降搜索过程中都有用。由于常数c权衡距离项和损失函数项的相对重要性，为了使固定常数c有用，这两个项的相对值应保持近似相等。这两个损失函数不是这种情况



#### 离散化

计算的时候图像往往都是[0,1]之间的实数，但是存储为图像的时候往往是[0,255]的离散整数，这其实也是一个约束条件。为了解决这个连续的约束问题，一般做法是取 255∗(xi+δi) 最近的整数。

在对抗性示例生成过程中，为了使像素值满足离散性要求，我们会对像素值进行四舍五入，从而使得对抗性示例的质量稍微下降。**如果我们需要恢复攻击的质量，我们会在由离散解定义的格子上执行贪婪搜索，逐个像素地改变像素值**。在我们的所有攻击中，贪婪搜索从未失败过。

以往的研究很大程度上忽略了整数约束。例如，当使用快速梯度符号攻击（ε = 0.1，即将像素值改变10%）时，离散化很少影响攻击的成功率。然而，在我们的研究中，我们能够找到对图像进行更小变化的攻击，因此不能忽略离散化效应。我们始终确保生成有效的图像；在报告我们攻击的成功率时，它们总是包括了离散化后处理的攻击。



### Our three attacks

#### L2 Attack

##### 基于L2距离的攻击的目标函数

<img src=".\img\CW\L2 attack.png" alt="L2 attack" style="zoom:80%;" />

- f是best objective function
- 参数k使得solver能找到对抗实例x'，它被以高置信度分类为类别t
- 为了解决梯度下降容易陷入“次优解”的问题，作者在原始图像的r 领域（r 对应已发现的扰动最小的攻击样本）里采样多个点作为初始图像，在多个初始图像的基础上进行攻击扰动的生成。

##### L2 Attack生成的对抗样本

<img src=".\img\CW\L2生成的对抗样本.png" alt="L2生成的对抗样本" style="zoom:80%;" />

##### 多个起点梯度下降

梯度下降的主要问题是：其贪婪搜索不能保证找到最佳解，并且可能陷入局部最小值。为了解决这个问题，我们在接近原始图像的多个随机起始点上进行梯度下降，每个起始点运行固定次数的迭代。我们从距离最近的已发现的对抗性示例中均匀随机采样点，该距离由半径 r 的球定义，其中 r 是迄今为止找到的最接近的对抗性示例。**从多个起始点开始减少了梯度下降陷入糟糕局部最小值的可能性。**

#### L0 Attack

**L0距离度量是不可微的**，因此不适用于标准梯度下降。从L0的定义来看，是在修改的像素数量上进行操作，因此取而代之的是，我们使用一种迭代算法，该算法在**每次迭代**中都会**基于L2距离来生成攻击样本、计算损失函数值，获得扰动对应的梯度g，随后基于梯度来找到对目标函数值影响最小的像素点，将该像素点排除在外，对其他像素进行梯度反传更新，不断迭代，直到获得一个最小的像素子集合（终止条件是L2的攻击方式无法再找到有效的攻击样本）**

在每一个迭代步中，基于L2的攻击方式是从原始图像开始来生成扰动的，为了加快攻击生成的速度，作者对该设置做了一定的修改，**在每个迭代步中，攻击生成的初始图像为上一个迭代步的最终图像**。之所以可以这样做，**是因为修改k 个像素与修改k + 1 像素在生成对抗样本的方式上是非常接近的。**

##### L0 Attack生成的对抗样本

<img src=".\img\CW\L0生成的对抗样本.png" alt="L0生成的对抗样本" style="zoom:80%;" />

肉眼上可见区别，L0效果不如L2

#### L∞ Attack

L∞度量不是完全可微的，标准梯度下降对其表现不佳。

作者尝试过优化以下的目标函数，但是发现效果非常不理想，原因可想而知，**按照L ∞ 距离含义，这样的优化只会去惩罚扰动量最大的像素**。那么就有可能出现这样一种情况：有两个像素点，分别是`δi = 0.5`和`δ j = 0.5 − ϵ` ，那么L ∞正则化就会惩罚`δ i` 点，而正则化项在`δ j `点处的梯度将会是0，该点即便扰动量很大也不会被惩罚。因此，在后续的迭代步中，`δ j = 0.5 + ϵ′′`可能会慢慢大于`δ i = 0.5 − ϵ′ `，如此反复，两个点会在`δ i = δ j = 0.5` 的基线附近来回波动，不会有任何有效的更新。


为了解决这个问题，作者基于L 2 距离设计了一个适应于L ∞ 优化的正则化项。我们**将目标函数中的 L2 项替换为对超过 τ 的任何项的惩罚**（最初为 1，在每次迭代中减少）。**这可以防止振荡，因为该损失项同时惩罚所有大值**。具体来说，在每次迭代中我们解决

<img src=".\img\CW\L∞ attack.png" alt="L∞ attack" style="zoom:80%;" />

**正则化项将会惩罚所有超过τ 的像素点，在每个迭代轮次完成后，如果所有像素的像素值均小于τ，那么就会对τ 进行衰减，衰减因子是0.9。否则，就停止攻击样本的搜索过程**



##### L∞生成的对抗样本

<img src=".\img\CW\L∞生成的对抗样本.png" alt="L∞生成的对抗样本" style="zoom:67%;" />

大多数改变是肉眼无法可见的



### Attack Evaluation

#### 对比方法

- Deepfool
- FGSM：如果没有找到ε可以生产目标类别，那么就会失败
- Iterative gradient sign
- JSMA：用的是CleverHans的版本。但无法在ImageNet训练，因为pixels计算量太大了

**如果攻击产生了带有正确目标标签的对抗样本，那么我们将报告成功，无论需要进行多少更改**。失败表示攻击完全无法成功的情况。我们在CIFAR和MNSIT的测试集中评估了前1000张图像。在ImageNet上，我们报告了最初由Inception v3正确分类的1000张图像。在ImageNet上，我们通过随机选择100个目标类别（10％）来估计最佳情况和最坏情况的结果

#### 在MNIST和CIFAR-10上的测试结果

<img src=".\img\CW\在MNIST和CIFAR10上的测试结果.png" alt="在MNIST和CIFAR10上的测试结果" style="zoom:80%;" />

- 对于每种距离度量方法，我们的攻击找到的对抗样本相较于先前提出的最佳gong'jigongji 更近。
- 我们的攻击从来不会失败
- 尤其是L0和L2攻击，找到的对抗样本的失真率相比于best previouly attacks下降了2倍和10倍，成功率是100%
- 我们的L∞攻击可以和先前的方法相比比较接近，成功率更高

#### 在ImageNet上的测试结果

<img src=".\img\CW\在ImageNet上的测试结果.png" alt="在ImageNet上的测试结果" style="zoom:80%;" />

- L∞的成功率特别高，因此可以通过仅翻转最低bit的每个pixel，就能将图像的label改为任意想要的target。这种改变是不可能检测到的。

#### 生成合成数字

有了我们的targeted adversary，我们可以从任何我们想要的图像开始，**找到每个给定目标的对抗样本**。利用这一点，在**图 6 中，我们显示了对于每个距离度量，将全黑图像分类为每个数字所需的最小扰动**

<img src=".\img\CW\生成合成数字.png" alt="生成合成数字" style="zoom:80%;" />

#### 运行时分析

由于不同的算法采用了并行或串行的方式，作者无法给出确切的数字

不过，**No attack takes longer than a few minutes to run on any given instance**



### Evaluating defensive distillation

最初提出蒸馏是一种将大型模型（教师）简化为较小的蒸馏模型的方法。在较高的层次上，蒸馏首先通过以标准方式在训练集上训练教师模型来进行。然后，我们使用老师为训练集中的每个实例加上软标签（老师网络的输出向量）。例如，尽管手写数字7的硬标签将其分类为七个，软标签可能会说其80％的可能性为7，而20％的可能性为7。然后，我们在老师的软标签上训练提炼的模型，而不是在训练集中的硬标签上训练提炼的模型。蒸馏可以潜在地提高测试集的准确性以及较小模型学习预测硬标签的速率。

 防御性蒸馏使用蒸馏以提高神经网络的鲁棒性，但有两个重大变化。**首先，教师模型和蒸馏模型的大小相同，防御性蒸馏不会产生更小的模型。其次，更重要的是，防御性蒸馏使用较大的蒸馏温度（如下所述）来迫使蒸馏后的模型对其预测具有更高置信度**。

##### 防御性蒸馏训练过程如下

1. 通过在训练阶段将softmax的温度设置为T来训练网络，即教师网络。
2. 通过将教师网络应用于训练集中的每个实例来计算软标签，并再次评估温度T时的softmax。
3. 使用温度为T的softmax在软标签上训练蒸馏网络（形状与教师网络相同的网络）。
4. 最后，在测试时间运行蒸馏网络（以对新输入进行分类）时，请使用温度1。

#### 现有攻击的脆弱性

我们简要**研究了现有攻击在蒸馏网络上失败的原因**，并发现现有攻击非常脆弱，即使存在对抗样本也很容易无法找到。

**FGSM失败的原因和L-BGFS，Deepfool一样：梯度几乎始终为零**。具体来说，当训练模型时，真实标签类别对应的置信度会趋近于1.0，其他类别的置信度会趋近于0，而当采用防御性蒸馏的方法时，**因为温度T的存在，真实标签类别对应的logits值会是温度为1时的T倍**，作者通过实验证实了这点。**那么，其他类别的logits值将会非常非常小，对应的梯度同样非常小**，事实上，**在大部分情况下，梯度小到32-bit的数据无法表示，只能取0。这样攻击生成所能利用的梯度就几乎不存在，自然无法攻击成功，这就是防御性蒸馏生效的原因**。

#### 使用我们的攻击

<img src=".\img\CW\作者方法在防御性蒸馏上的效果.png" alt="作者方法在防御性蒸馏上的效果" style="zoom:80%;" />

- 攻击成功率100%
- 相较于之前没有防御蒸馏的模型，L0和L2攻击的性能只稍微下降了一些。L∞攻击的效果几乎一样

#### 温度的作用

<img src=".\img\CW\温度的作用.png" alt="温度的作用" style="zoom:80%;" />

先前的attack基本都随着蒸馏温度的上文，攻击成功率下降

作者的方法，L2 attack基本上不受温度变化的影响。

#### 可迁移性

最近的工作表明，即使使用不同的训练数据集对它们进行训练，甚至使用完全不同的算法，一个模型的对抗样本也通常会转换为不同模型的对抗性（即，有关神经网络的对抗样本会转移到随机森林中）。

因此，任何能够对付对抗样本的防御措施都必须以某种方式破坏这种可转移性。否则，我们可以在易于攻击的模型上运行我们的攻击算法，然后将这些对抗样本转移到难于攻击的模型上。

即使防御性蒸馏对我们更强的攻击没有强大的作用，我们还是通过将攻击从标准模型转移到防御性蒸馏模型来证明了蒸馏的第二次突破。

**防御方法必须证明自己能够防御迁移性攻击，才能够证明防御的有效性。否则，只要在易攻破的模型上生成攻击样本，再迁移到防御模型上就可以攻击成功了。本文的作者利用迁移性攻击的方式再次攻破了防御性蒸馏，如下式所示，作者实现迁移性攻击的方式是搜索高置信度的攻击样本**。

<img src=".\img\CW\L2公式.png" alt="L2公式" style="zoom:80%;" />

 其中，κ 用于标定攻击样本的置信度，κ的值越高，生成的攻击越强。作者在MNIST数据集上进行迁移性试验，首先在未经过防御的模型上进行迁移性攻击，发现随着κ 值的升高，攻击成功率不断升高，在κ = 20 后保持在100%的成功率。这说明搜索高置信度的攻击样本有利于提高攻击成功率，作者随后对防御性蒸馏进行迁移性攻击，发现攻击依旧生效，这样就再一次攻破了防御性蒸馏。


我们的基准实验使用在MNIST上训练的两个模型，每个模型在一半的训练数据上进行训练。我们发现，可传递性的成功率从κ=0线性增加到κ=20，然后在κ≈20的情况下稳定在接近100％的成功率，因此显然**增加κ会增加可成功转移攻击的可能性**。然后，我们仅运行相同的实验，而是使用防御性蒸馏训练第二个模型，并发现对抗样本确实可以转移。这为我们提供了另一种攻击方法，可用于在蒸馏网络上查找对抗样本。然而，有趣的是，与之前只需要κ=20的方法相比，在不安全模型和蒸馏模型之间的可转移性成功率仅在κ=40时才达到100％成功。

<img src=".\img\CW\迁移.png" alt="迁移" style="zoom:80%;" />

### Conclusion

**本文的作者通过对损失函数以及像素值clip方式的改进，提出了一种新的攻击方式，常称之为CW攻击，一举攻破了当时SOTA的防御方式——防御性蒸馏，推动了对抗攻防的演进。这也成为一个新的benchmark，用于衡量后续所有对抗防御方面的新方法的性能。**

 对抗样本的存在限制了可以应用深度学习的领域。构建对付对抗样本的防御是一个开放的问题。为了解决该问题，提出了防御性蒸馏作为提高任意神经网络的鲁棒性的通用程序。

 在本文中，我们提出了强大的攻击，可以打败防御蒸馏，这表明我们的攻击可以更广泛地用于评估潜在防御的效果。通过系统地评估许多可能的攻击方法，我们选择了一种能够比所有现有方法始终找到更好的对抗样本的方法。**我们使用此评估作为我们三个 L0,L2,L∞ 攻击的基础。**

 我们鼓励创建防御的人员执行我们在本文中使用的两种评估方法：

1. 使用强大的攻击（例如本文提出的攻击）直接评估安全模型的鲁棒性。 由于阻止我们的L2攻击的防御将阻止我们的其他攻击，因此**防御者应确保针对L2距离度量建立稳健性**。
2. **通过在非安全模型上构建高可信度对抗样本，并显示它们无法转移到安全模型来证明可传递性失败。**

