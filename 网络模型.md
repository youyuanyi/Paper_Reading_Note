# 网络模型笔记

## ResNet

### 论文地址

[ResNet]: file://E:/研究生学习/论文/1512.03385%20ResNet.pdf

### 期刊/会议

CVPR 2016

### Title

**Deep Residual Learning for Image Recognition**

### Author

**Kaiming He**; Xiangyu Zhang； Shaoqing Ren； Jian Sun

### Introduction

深度学习可以通过**加深网络层数**来**得到不同level（级别）的feature**，比如低级的视觉特征或者高级的语义特征。

#### 提出问题

**加深网络层数**会**导致梯度爆炸/梯度消失**的问题。尽管可以通过**挑选适合的初始化权重和归一化层（BN）来缓解**，使得数十层的网络能够收敛，但是**网络性能会趋向于饱和，并且会突然迅速下降**。而**原因不是因为层数增多，模型变复杂了导致的过拟合，而是由于训练误差和测试误差都变高了**。（对应Fig.4，可以看出训练误差和测试误差都很差)

一种解决思想是在一个效果还可以的**浅层网络后面添加Identity Mapping**（恒等映射：即输入为x，输出仍然为x），**理论上这样一个深的网络效果不应该比浅的网络效果差**。但是**实际情况中SGD是找不到这样的映射的**。

#### 提出方法

这篇文章提出了显示地构造identity mapping，将其称为**deep residual learning framework**。

![残差块](.\img\resnet\残差块.png)

**此时我们希望模型要拟合的是H(x)，这样在新添加的层中，模型不是直接去学H(x)，而是去学F(x)=H(x)-x。x是上一层的输入。这样，新添加的层就不用去重新学习x中已经学到的内容，而是去学习x学到的东西和真实值之间的残差F(x)。这样就把优化目标从H(X)变为了H(x)-x**

在实现中，使用的就是**shortcut**，**它不会增加参数和模型复杂度**。



### Deep Residual Learning

#### 残差连接如何处理输入和输出的形状是不同的情况

- 第一种方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加。
- 第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做**1x1的卷积层**，这个卷积层的**特点是不学习任何空间维度的特征，主要是在通道维度上做融合和改变**。所以只要选取一个1x1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上

### Implemention细节

- 把短边随机采样到[256,480]，这样是为了增强网络的鲁棒性和泛化能力。这种随机采样的方式可以帮助网络**更好地适应不同尺寸的输入图像，从而提高其对于尺度变化的适应性。同时，这也是一种数据增强**
- **每一个pixel的均值都减掉了。其主要思想是将输入图像的每个像素减去一个全局平均值。这个全局平均值通常是在训练数据集上计算得到的，它代表了整个数据集图像像素的平均值。这样可以消除输入图像中的一些常见的光照变化，还有助于将输入数据的分布调整到一个更接近零均值的状态，这对于一些优化算法（如梯度下降）的稳定性和收敛速度都有积极的影响**
- 使用了**颜色的增强**（AlexNet上用的是PCA，现在我们所使用的是比较简单的RGB上面的，调节各个地方的亮度、饱和度等）
- 使用了**BN**（batch normalization）
- 所有的权重全部是跟另外一个paper中的一样（作者自己的另外一篇文章）。注意写论文的时候，尽量能够让别人不要去查找别的文献就能够知道你所做的事情
- **批量大小是56，学习率是0.1，然后每一次当错误率比较平的时候除以10**
- 模型训练了60*10^4个批量。建议最好不要写这种iteration，因为他跟批量大小是相关的，如果变了一个批量大小，他就会发生改变，所以现在一般会说迭代了多少遍数据，相对来说稳定一点
- weith decay=0.0001, 动量为0.9
- 这里没有使用dropout，因为没有全连接层，所以dropout没有太大作用
- 在测试的时候使用了标准的10个crop testing（给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后在每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。
- 采样的时候是在不同的分辨率上去做采样，这样在测试的时候做的工作量比较多，但是在实际过程中使用比较少

### Residual结构

![ResNet-residual结构](.\img\resnet\ResNet-residual结构.png)

### Architectures

#### 总体结构

![ResNet结构](.\img\resnet\ResNet结构.png)

#### ResNet-34 结构

![ResNet结构](.\img\resnet\ResNet-34.png)

- 从layer2开始，stride=2。
- 虚线的残差分支，代表这个block的残差分支要采用1x1卷积核，将残差分支的通道数扩大4倍。同时虚线部分的主干分支负责将高和宽缩小一半，通道数变大4倍

#### ResNet-50结构

![ResNet-50结构](.\img\resnet\ResNet-50结构.png)

### Experiments

#### 先比较了没有带残差的（即plain的）18-layer和34-layer

![ResNet-plain](.\img\resnet\ResNet-plain.png)

- 细线代表训练误差，粗线代表验证误差。左图是没有使用残差结构的18-layer和34-layer。
- 因为训练集做了数据增强，相当于添加了噪声，而验证集没有做数据增强，所以一开始训练误差相较于验证误差较大。
- 图中，**误差断崖式下降的地方是因为学习率的下降**。
- 上图主要是想说明**在有残差连接的时候，34比28要好；另外对于34来说，有残差连接会好很多**；其次，有了残差连接以后，收敛速度会快很多，核心思想是说，**在所有的超参数都一定的情况下，有残差的连接收敛会快，而且后期会好**



#### 处理输入输出尺寸不同

<img src=".\img\resnet\ResNet挑选projection.png" alt="ResNet挑选projection" style="zoom:50%;" />

- 方案A表示填0
- 方案B表示在输入和输出不同时做投影
- 方案C表示全部做投影
- 由于B和C效果差不多，且B的计算复杂度更高，所以作者采用了方案C

#### Deeper Bottleneck Architecture

![ResNet-deeper-resnet](.\img\resnet\ResNet-deeper-resnet.png)

- 如果要做50层以上的设计，则会引入bottleneck design
- 左图是之前的设计，当通道数是64的时候，通道数不会改变
- **如果要做到比较深的话，可以学到更多的模式，可以把通道数变得更大，右图从64变到了256**
- 当通道数变得更大的时候，**计算复杂度成平方关系增加，这里先通过1个1*1的卷积，将256维投影回到64维，然后再做通道数不变的卷积，然后再投影回256（将输入和输出的通道数进行匹配，便于进行对比）。**等价于先对特征维度降一次维，在降一次维的上面再做一个空间上的东西，然后再投影回去
- 虽然通道数是之前的4倍，但是在这种设计之下，二者的算法复杂度是差不多的

### 一些杂谈

- 在整个残差连接，如果后面新加上的层不能让模型变得更好的时候，是因为有残差连接的存在，**新加的那些层应该是不会学到任何东西，应该都是靠近0的**，这样就等价于就算是训练了1000层的ResNet，但是可能就前100层有用，后面的900层基本上因为没有什么东西可以学的，基本上就不会动了
- **ResNet在前面层学习数据主要的特征，通过shortcut传递给后面的层。越到后面，后面的层学习的东西就越少越次要了。**
- **如作者所说，不加残差连接的时候，理论上也能够学出一个有一个identity的东西，但是实际上做不到，因为没有引导整个网络这么走的话，其实理论上的结果它根本无法按这个方向优化，所以一定是得手动的把这个结果加进去，使得它更容易训练出一个简单的模型来拟合数据的情况下，等价于把模型的复杂度降低了**







## Transformer

### 论文地址

[Transformer]: file://E:/研究生学习/论文/1706.03762Transformer.pdf

### 期刊/会议

NeurIPS会议 2020

### Title

Attention Is All You Need

### Author

Ashish Vaswan

### Introduction

#### 介绍了RNN的缺陷

如果输入是一个sequence，那么RNN只能从左往右一个词一个词计算。对第t个词，会计算一个输出h_t。h_t是由前一个隐藏状态h_t-1和当前第t个词决定的。这样RNN就能保留之前的信息。

问题1：RNN缺少并行度，计算效率差

问题2：无法捕捉和保留长距离依赖。除非h_t很大，但这样也会导致内存开销特别大。

#### 早前工作

已经有过使用attention技术的工作，但大部分仍然是和RNN结合使用

#### 工作贡献

提出了Transformer，没有使用RNN和CNN，完全依赖于注意力机制来学习全局依赖关系，具有很好的并行度。



### Related Work

- 以前的工作主要用CNN作为basic building block来替代RNN，但仍然无法很好的捕捉长距离依赖。因为卷积计算的时候看一个比较小的窗口，例如一个 3 * 3 窗口，如果 2 个像素隔得比较远，需要用很多 3 * 3 的卷积层、一层一层的叠加上去，才能把隔得很远的 2个像素联系起来。计算量也随着两个像素位置距离的增大而显著变大。
- 在Transformer中，能一次看到所有的输入，和输入的长度无关
- 由于CNN具有多输出通道特点，每个通道可以学习不同的模式，所以在Transformer中使用了Multi-head Attention模拟这个效果。

讲述了和本文相关的工作（自己的或别人的），联系是什么，区别是什么

### Model

在大多数Encoder-Decoder模型中，模型的每一步都是自回归的（即输出是一个一个生成的），在生成下一个时将先前生成的符号用作附加输入。

这样Encoder可能可以一次看到整个句子，然后Decoder在解码时，只能一个一个的生成。

![Transformer模型结构](.\img\Transformer\Transformer模型结构.png)

#### 为什么用LN而不是BN？

![BN原理](.\img\Transformer\BN原理.png)

BN是把每一列（1个特征）放在一个mini-batch中，将均值变为0，方差变为1，具体做法为（该列向量 - mini-batch 该列向量的均值）/（mini-batch 该列向量的方差）。

BN还会学习一个λ和β，通过学习这两个参数将向量放缩成任意均值、方差的一个向量。



LN

LN是把每一行（1一个batch）的均值变为0，方差变为1.

LayerNorm 相当于把整个数据转置一次，放到 BatchNorm 里面出来的结果，再转置回去，基本上可以得到LayerNorm的结果（只针对二维输入）。 



但是输入通常为3维的（batch，长度n，特征数d)，此时切出来的结果分别为

<img src=".\img\Transformer\LN和BN对比.png" alt="LN和BN对比" style="zoom: 33%;" />

**对于BN，如果样本长度n变化较大，每次做mini-batch计算出来的均值和方差抖动较大**。且预测时**需要记录全局的均值和方差**，如果一个新的预测样本特别长，那么在训练时可能没见过，那么训练时计算的均值和方差就可能没用。

而对于LN，它是对每一个样本单独计算均值和方差，不需要存全局的均值和方差，会更稳定一些。



#### **为什么要用Masked-Multi Head Attention？**

因为Decoder是auto-regressive。当前时刻的输入集是之前一些时刻的输出。做预测时，decoder不能看到后民间的输出。因为attention会一次看到完整的输入，所以要加上Mask

#### Attention

一句话来说，values的权重是由query和对应的key计算得到。这样，key和value能保持不变，通过不同的query就能得到不同的weight，使得outputs不一样。

详情见李宏毅机器学习2022

#### Scaled Dot-Product Attention

queries和keys都是d_k维的，values是d_v维的。

计算query和所有keys的Dot-Product，然后除以根号d_k，最后使用softmax函数计算得到权重

##### 为什么使用内积

因为内积代表两个向量的余弦值大小，内积=1，表示两个向量平行，相似度很大；如果内积=0，则表示两个向量正交，相似度不大。

##### 实际操作

将所有的queries封装为一个矩阵Q，同理得到矩阵K和V
$$
Attention(Q,K,V)=softmax(\frac{QK^\top}{\sqrt{d_k}})V\\
假设Q是一个n*d_K的矩阵，K是一个m*d_k的矩阵，V是一个m*d_v的矩阵\\
QK^\top得到一个n*m的矩阵，每一行代表每一个query对每一个key的点积的值\\
除以\sqrt{d_k}是为了防止点击过大\\
最后乘上矩阵V，得到一个n*d_v的矩阵，每一行就是我们要的输出
$$

##### 为什么使用的是Dot-Product而不是Additive attention

Dot-Product计算起来更快，空间效率更好，因为它可以使用高度优化的矩阵乘法代码来实现

##### 为什么要除以根号d_k

因为过大的d_k值，会使得点积的值变得很大，从而将softmax函数推入梯度极小的区域。



#### Multi-Head Attention

对于每一个head，去匹配不同的模式，来表示不同的相关性，有点类似CN中的多输出通道机制。多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。
$$
MultiHead(Q,K,V)=Contact(head_1,...,head_h)W^O\\
where\ head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)
$$
由于使用了残差结构，这就要保证不同输出的维度是一样的。所以文章作者将输出投影到64维（即d_model/h，h=8)

#### Transformer不同的Attention

<img src=".\img\Transformer\Transfomer不同的Attention.png" alt="Transfomer不同的Attention" style="zoom: 67%;" />

1. Encoder中的Multi-Head Attention：
   - 有三个输入，k，v，q，它们来自同一个输入（所以叫做自注意力）
   - 输入了n个query，每个query会得到一个输出，那么会有n个输出
   - 输出是value的加权和，权重是key和query的相似度
   - 输入和输出大小相同，都是d
   - 对于multi-head，由于投影的不同，可以学习到h个不一样的距离空间，使得输出变化。
2. Decoder中的Masked Multi-Head Attenion：
   - 和Encoder类似，只不过是Masked。因为对于Decoder，不应该看到t时刻以及之后的输入
3. Decoder中的第二个Multi-Head Attention:
   - 不再是自注意力了，Encoder的输出作为输入的K和V，上一层Decoder的输出作为Query
   - Encoder的输出是n个长为d的向量，上一层解码器的输出是m个长为d的向量
   - **实际上在做根据Decoder给的query，从Encoder的输出中提取出所需要的东西（所感兴趣的东西）**
   - **这个attention起到在encoder和decoder之间传递信息的作用**

#### Position-wise Feed-Forward Networks

对每一个position（每一个词）都做一次MLP，MLP是一样的
$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$
输入和输出维度d_model=512，W1将输入投影到2048维，W2重新投影到512

**Attention已经完成了输入的序列信息的汇聚，再通过MLP投影成所需的语义空间中的向量。所以可以对每一个Position单独使用MLP**

##### 和RNN的区别

**Transformer和RNN都是用MLP实现语义空间的转换**

不同在于，RNN是把上一个时刻的信息输出作为当前时刻的输入；Transformer是将整个序列的信息作为MLP的输入



#### Embeddings and Softmax

- 对于每一个input token和output token，学习一个**d维向量表示（d=512）**
- 编码器，解码器和softmax前面都需要一个embedding，**这三个embedding使用相同的权重，使得训练更简单。**
- 对权重除以了根号d。这是因为 **学 embedding 的时候，会把每一个向量的 L2 Norm 学的比较小。维度d大的话，学到的权重就很小。但之后还需要加上positional encoding，所以通过除以根号d，使embedding 和  positional encoding 的 scale 差不多，可以做加法。** 

#### Positional Encoding

Attenion是不会保留时序信息的。因为输出是value的加权和，权重来自query和key之间的距离，和序列信息无关。一句话把顺序任意打乱之后，attention 出来的结果都是一样的。 在处理时序数据的时候，一句话里面的词完全打乱，那么语义肯定会发生变化，但是 attention 不会处理这个情况，所以需要 加入时序信息

**对于每一个位置数字信息的值，使用周期不一样的sin和cos函数计算成一个长为512的向量来表示**

输入进入 embedding 层之后，那么对**每个词都会拿到一个长为 512 的向量。positional encoding （这个词在句子中的位置），返回一个长为 512 的向量，表示这个位置**，然后把 embeding 和 positional encodding 加起来就行了。相加后的结果仍然是顺序不变的



### Why Self Attention

#### 比较了Table 1

![Attention和其他结构比较](.\img\Transformer\Attention和其他结构比较.png)

就计算复杂度而言，当序列长度 n 小于表示维度 d 时，自注意力层比循环层更快。

为了提高涉及很长序列的任务的计算性能，自注意力可以限制为仅考虑以相应输出位置为中心的输入序列中大小为 r 的邻域，但这会将最大路径长度增加到 O(n/r)。

实际上self-attention和CNN,RNN和时间复杂度差不多，但是Attention需要更多的数据才能达到和CNN、RNN一样的效果。

### Experiment

#### Tranning Data

WMT 2014 English-German dataset

WMT 2014 English-French dataset

#### Hardware and Schedule

base model：8块P100上训练了100,000 steps

big models：8块P100上训练了300,000 steps

#### Optimizer

Adam β1=0.9，β2=0.98，学习率是根据公式计算出来的，并使用了warm up技术。warmup_steps=4000

#### Regularization

##### Residual Dropout

- 对于每一个sub layer，在进入下一个sub layer和norm之前，会使用dropout
- 在输入embedding和positional encodings的和上也使用了dropout
- P_dropout=0.1

##### Label Smoothing

对于正确的词，置信度从1变为0.1

### Conclusion

- 提出了一个Transformer模型，用multi-head self-attention替换了RNN和CNN
- 在机器翻译任务上，Transformer训练更快
- 未来展望在图像，视频，音频中进一步研究



## VIT

### 论文地址

[vit]: file://E:/研究生学习/论文/vit.pdf

### Title

An Images Is Worth 16x16 Words:Transformers For Image Recognition At Scale

### Author

Google Research,Brain Team

### Abstract

Transformer已经在NLP中取得很好的发展，但在CV中应用有限。目前的CV里的attention主要是attention+CNN或者attention替换CNN部分模块但仍然保持CNN整体结构。

作者提出一个纯Transformer也可以在图像分类上取得很好的结果。

在大规模数据上训练的预训练VIT迁移到中小规模数据集上做分类时，可以媲美最好的CNN，且所需训练资源更少。

### Introduction

在NLP中，主流的方法是使用在大文本数据集上预训练模型，然后针对特定的任务数据集进行微调，且随着数据集和模型的增长，仍然没有出现饱和现象。

在CV中，CNN仍然占据主导地位。目前，已经有一些关于Attention的探索

- Wang et al.,2018：将ResNet的最后一个stage的14x14特征图拉平，此时序列长度为196，然后输入Transformer
- Ramachandran et al.,2019：提出孤立自注意力机制，用一个局部的window来控制Transformer的计算复杂度。
- Wang et al.,2020a：提出轴注意力机制。将2D的矩阵拆分为高和宽两个1D的向量。分别在高和宽两个dimension上做两次self-attention
- 以上技术理论上高效，但是无法在现代硬件上加速，因此很难训练一个大模型，可扩展性不高。
- 因此，state of the art仍然是以Resnet为basic的模型

**作者为了考虑Transformer的可扩展性，尽量不针对视觉任务而对Transformer进行修改，提出将图像分成patch（每个patch是16x16，一张224x224的图片就有14x14个patch，相当于一个维度为196x768的序列），将每一个patch经过一个FC Layer得到linear embedding sequence作为Transformer的输入。**



**由于Transformer和CNN相比缺少inductive biases（归纳偏置）。在CNN中inductive biases是locality 和平移不变性。**CNN的卷积核像一个template，同样的物体无论移动到哪里，遇到了相同的卷积核，则输出一致。

**CNN 有 locality 和 translation equivariance 归纳偏置--> CNN 有 很多先验信息 --> 需要较少的数据去学好一个模型。**

**Transformer 没有这些先验信息，只能 从更多的大量图片数据里，自己学习对视觉世界的感知。**

### Related Work

第一段讲了Transformer在NLP中的主流使用方法：BERT和GPT

第二段：

- 由于一张图片的pixel数量非常多，如果把所有pixel输入到Transformer中，那么这个计算量是远超Transformer在NLP中的使用情况的
- 提出了一些prior works对self-attention在CV中的研究

和VIT最接近的一项工作是Cordonnier(2020)提出的一个方法：把图像分成一个个patch，每个patch是2x2，然后使用self-attention。VIT和它的区别在于进一步提高了模型的scale。使用了大scale的patch，使得VIT能处理high resolution的images。

提出了一个VIT模型，除了初始的patch分块步骤，作者没有进将特定于图像的归纳偏置引入模型架构中。作者将一张图看作一个patches序列，使用标准的Transformer来做分类。在大数据集上的与训练效果很好，超过了state of the art

使用更多的额外数据集可以使得ViT超过state of the art

讲了别人做了哪些工作，你和他们的联系与区别是什么



### Method

#### VIT

![VIT结构](.\img\VIT\VIT结构.png)

<img src=".\img\VIT\VIT细节结构.png" alt="VIT细节结构" style="zoom: 50%;" />

- 首先将一张图分成一个个patch，每个patch是16x16x3，一张224x224的图片就有14x14个patch（**在代码实现中，使用卷积核大小为16，stride=16，卷积核个数=768）：224x224x3->14x14x768->196x768。这样就变为14x14个patch，每个patch是原来16x16个pixel的特征提取**
- 将每一个patch通过Flatten变为序列
- 每一个patch通过Linear Projection操作得到一个特征patch embedding
- 将patch embedding和position embedding相加
- 为了实现分类，在Transformer Encoder的输入前加入了一个额外的Learnable class embedding
- 将以上输入到Transformer得到输出**（在Transformer Encoder前有一个Dropout层，后面有一个LN层）（训练ImageNet-21K时由Linear+tanh+Linear，迁移到自己的数据集时，只有一个零初始化的Linear）**<img src=".\img\VIT\fine-tuning.png" alt="fine-tuning" style="zoom: 50%;" />
- 将结果输入到MLP中进行分类



步骤1：将一张224x224x3的图片分成16x16的patch，每个patch大小为16x16x3，一共有14x14=196个patch。所以原来的224x224x3的图片就变成了196x768的序列。

步骤2：线性投射层其实就是一个MLP，维度为768x768(D)。第一个768是根据输入图像计算得到的，第二个**D=768可以根据Transformer的改变而变大或变小。**

步骤3：步骤二得到的输出为196x768。相当于有196个token，每个token维度为768。**这就完成了2D图像到1D序列的转换。**

步骤4：由于加入了一个1x768的class embedding，所以**总体输出维度为197x768**，即**Embedded Patches的输入为197x768的tensor**

步骤5：**加入位置编码信息（实际上是编号）**

步骤6：进入Transformer模块，先做Layer Norm

步骤7：做Multi-Head Attention。输入的Q,K,V维度都是197x768。不过由于多头自注意力机制的头个数**h=12**，所以**每个头输入的维度应该为197x(768/12)=197x64**

步骤8：**将12个头的输出contact起来，仍然得到一个197x768的输出，再过一层Layer Norm**

步骤9：过MLP：197x768 ->197x3072->197x768

步骤10：经过L个Transformer Block

#### 代码实现图

以ViT-B/16为例（如果是自己的数据集，可以不用Pre-Logits）

<img src="E:\深度学习\网络模型笔记\img\VIT\VIT-B-16的代码实现图.png" alt="VIT-B-16的代码实现图" style="zoom:67%;" />

#### Class Token

往Patch Embedding中加入了一个**可学习的class token（即图中的0*)**，**这个token在Transformer的对应输出就当作是整个图像的特征y（即这个class token可以代表这张图片）。然后将y通过MLP进行分类预测**。



##### 为什么要加入class token

由于**Transformer**是为NLP任务设计的，它**期望输入数据中包含一个特殊的"类别"或"句子"标记，以指示所处理的文本属于哪个类别或句子**。**对于图像数据，没有明确的类别或句子标记，因此为了将图像数据与Transformer对话，ViT引入了一个特殊的向量，通常称为"类别标记"（class token）**

class token是一个额外的向量，它代表整个图像而不是特定的某个patch。它将与所有图像块的表示一起输入到Transformer模型。这样，**Transformer在处理每个图像块时都可以感知整个图像的信息，而不仅仅局限于特定的局部图像块**。**类别标记的添加有助于使Transformer模型能够理解整体图像的语义信息**，并在视觉任务中表现更好。

总的来说，就是**通过引入class token，使得图像数据能转换为Transformer可接受的序列格式，并且提供了一点额外的全局信息。**

#### Inductive bias

ViT相对于CNN缺少inductive bias。CNN具有locality（局部性）、two-dimensional neighborhood structure（二维邻域结构）和translation equivariance（平移不变性），这些信息贯穿于整个CNN网络中。然而在ViT中，只有MLP具有这些归纳信息，而self-attenion是全局的。

ViT仅仅在一开始的分patch阶段和fine-tuning修改position embeddings时使用到了图像的2D inductive bias。

#### Position Embeddings

仍然使用1D 的position embeddings

#### 公式描述

![VIT过程的公式描述](.\img\VIT\VIT过程的公式描述.png)

1.将每一个patch Xp输入到Linear Projection，将输出和class embedding拼接

2.将拼接结果和Position Embeddings相加，得到Transformer的输入Z0

3.循环L个Transformer Block 

4.**将Z_L<sup>0</sup>（class token所对应的输出）作为整体图像的特征，输入到MLP中完成分类。**



#### Fine-tuning And Higher Resolution

-  在大数据集上预训练，在下游任务上进行fine-tuing
- 在微调时使用大分辨率的图像效果会提高(Touvron et al.,2019)，但这会导致VIT产生的序列长度增加(patch size不变，H*W/P会变大)。
- 尽管VIT可以处理任意长度的序列，但是**由于提前预训练好的position embeddings可能就没有用了**
- **因此作者在pre-trained的position embeddings中采用2D插值。**
- **只有position embeddings的2D插值和patch extraction使用了图像的2D inductive bias**

#### 为什么强调只在position embedding的2D interpolation和patch extraction使用了图像的2D inductive信息？

主要是为了**突出ViT在处理图像时的特殊性和优势**

- 因为这种做法使得**Transformer模型能够利用图像的局部和全局信息**，而**不会受到CNN中固定的局部感受野大小的限制。**
- ViT通过使用**2D插值的方式对位置编码进行计算**，以便**在局部区域和全局区域都能获取合适的位置信息**。这是因为**在2D插值过程中，可以通过对附近像素的加权平均来估算图像块的位置编码，从而捕获图像中的空间关系，使得Transformer很好地处理图像上下文**
- 在patch提取阶段，ViT并没有使用传统的固定形状的卷积滤波器，而是将图像切分成均匀的图像块，这样做的目的是**使Transformer模型能够处理不同大小的图像，而不需要调整网络结构或参数（即patch size不变，但是sequence长度增大减小不影响Transformer）。这样，ViT可以在训练和推理过程中处理任意大小的图像**
- **这两种方法使得ViT具有更好的可扩展性和适应性，可以处理各种尺寸的图像，并且能够更好地捕捉图像中的全局上下文信息。因此，ViT强调了它只在这两个阶段使用了图像的2D归纳信息，以强调其在视觉任务中的特殊优势。**

### Experiments

- 对比 ResNet, ViT, Hybrid ViT (CNN 特征图，不是图片直接 patch 化) 的 representation learning capabilities 表征学习能力。
- 为了了解每个模型预训练好到底需要多少数据，在不同大小的数据集上预训练，然后在很多 benchmark tasks 做测试。
- 考虑模型预训练的计算成本时，ViT 表现很好， SOTA + fewer resource 训练时间更少
- ViT 的自监督训练，可行，效果也还不错，有潜力；一年之后，MAE 用自监督训练 ViT 效果很好

#### Datasets

ImageNet-1K: 1000 classes, 1.3M images

ImageNet-21K: 21000 classes, 14M images

JFG-300: 303M images Google 不开源

下游任务：分类 CFIAR etc.

#### Model Variants

![VIT Variants](.\img\VIT\VIT Variants.png)

**ViT-L/16代表着ViT-Large with 16x16 input patch size**



#### Metrics

微调上的准确率

few-shot：评估ViT在**小样本学习**（few-shot learning）任务上的表现的实验，并对其**泛化性能和适应性**进行测试

在ViT的few-shot实验中，通常采用的是一种叫做"**n-shot k-way**"的评估方式，其中**n代表每个类别的训练样本数量，k代表任务中的类别数量**。具体地说，给定n个样本的训练集，模型需要学会区分k个不同的类别。这个过程会多次重复，以评估模型在不同任务和数据集上的表现。

为了实现few-shot学习，通常会采用一些技术来辅助模型学习。例如，可以使用元学习（**meta-learning**）方法，如**MAML（Model-Agnostic Meta-Learning）或Prototypical Networks**，这些方法旨在帮助模型在少量样本上学习并快速适应新任务。此外，数据增强和特定的任务设计也可能用于增强模型的泛化性能

#### Comparison to state of the art

总结来说，就是ViT效果更好，预训练所需TPU核心天数更少

对于训练策略、优化器、学习率等其他影响因素的验证，作者在4.4节做了对比控制实验。

#### Pre-Train Data Requirements

##### 实验一

分别在ImageNet，ImageNet-21k和JFT-300M上进行了预训练

为了提高在小数据集上的性能，作者优化了weight decay,dropout和label smoothing三个参数。

试验结果表明，BiT在小数据集上比ViT更好，ViT在larger datasets更好

##### 实验二

分别在JFT-300M的9M，30M和90M子集上进行了实验。在这个实验中没有对小数据集进行额外的正则化。不过仍然使用了early-stopping来取得最佳精度。

在小数据集上，ResNets很有用，但是对于大数据集，直接从数据中学习相关模式的ViT更好。



#### Scaling study

- Vision Transformers 在性能/计算权衡方面优于 ResNets。要达到相同的性能，ViT 使用的计算量大约要少 2-4 倍（5 个数据集的平均值）
- 在计算预算较少的情况下，混合系统的性能略优于 ViT，但在较大的模型中，这种差异就会消失
- 仍然没有看到性能饱和的现象，模型仍然有进一步扩展和提高的空间。

#### 可视化

##### Patch embedding

![RGB embedding filters](.\img\VIT\RGB embedding filters.png)

在Patch embedding阶段（即Linear Projection），可以看出attention学到的特征主要包含颜色和纹理，这些成分可以作为基函数去描述图像的底层结构

##### Positional embedding

![pos embedding](.\img\VIT\pos embedding.png)

学到了**距离的概念**，可以看到自己和自己的位置处是黄色的，相似度最高，越远颜色就越蓝，即相似度越低。

此外，还学习到了行和列的规则，同行同列的颜色表示

##### Size of attended area

![Mean area](.\img\VIT\Mean area.png)

- **attention distance就类似于CNN中的receptive field**
- 可以看出，对于attention，在浅层layer中就已经学习到了较远距离的信息了。
- 随着网络加深，head都趋向于更远的attention distance了
- **模型趋向于关注和图像分类语义相关的区域了**

#### Self-superivision

作者使用类似BERT的方法进行了自监督学习探索：将某些patch进行mask，然后进行学习预测。

通过这种自监督预训练，ViT-B/16在ImageNet上取得了79.9%的准确率，比从头开始训练显著提高了2%，但是相对于supervised pre-training仍然低4%。

这是一个未来工作的展望。

#### Fine Tune

- SGD with a momentum of 0.9
- 学习率小梯度搜索技术
- 迁移ViT模型到其他数据集上时，移除了最后的两个linear layers，用一个零初始化的linear layer代替，输出维度为分类个数。这种方法比直接重新初始化最后的layer更具有鲁棒性。

#### Transformer Shape

ViT进行了对模型深度、宽度以及patch size缩放对性能影响的实验

- 缩放深度：通过增加模型的深度（即增加层数），可以明显地改进性能，尤其在增加了64层之前可以明显观察到这种改进。然而，在增加到16层之后，性能的改进变得有限。这表明增加模型深度对性能改进是有帮助的，但随着深度增加，性能的提升会逐渐减弱。
- 缩放宽度：增加模型的宽度（即增加每层的隐藏单元数）似乎对性能的改进影响较小。换句话说，增加隐藏单元数并不像增加深度那样显著改善性能。
- 缩小patch size：**通过减小图像块的大小，有效地增加了模型的序列长度。这样的操作在不引入额外参数的情况下，表现出惊人的鲁棒性改进**。换句话说，更小的图像块能够带来较好的性能提升，并且不会增加太多模型参数的数量。

#### 为什么缩小patch size可以提高ViT的鲁棒性？

- 当ViT减小patch size时，每个patch的pixel数减少，这意味着**patch的维度减少，那么此时patch的数量就会增加，相当于增加了输入序列的长度**。
- 在Transformer中，**序列长度决定了每个head在计算时可以考虑的位置距离。**
- Vit的position embedding**允许Transformer模型处理任意长度的输入序列**，并能够捕捉远距离位置之间的关系。因此，当减小patch size大小，从而增加输入序列的长度时，ViT仍然可以处理更长的上下文关系，而**不会引入额外的模型参数。**
- **对于Transformer，更长的输入序列长度意味着可以为模型提供更多全局信息，使得模型能够更好地理解图像的语义和结构。**

#### 三种不同位置编码的消融实验

- 无位置编码（No Positional Encoding）：在这个消融实验中，作者移除了位置编码，不将位置信息引入Transformer模型中。**这相当于将图像数据看作一系列无序的token，而不考虑其空间关系**。通过这个实验，作者想要验证位置编码对Transformer在处理图像数据时的重要性。
- 1D 位置编码：在这个实验中，作者使用1D位置编码，而不是原始的2D位置编码。1D位置编码是在图像块**序列维度**上独立地应用的，以一维方式捕捉位置信息。
- 原始的2D位置编码（Original 2D Positional Encoding）：这是ViT中默认和原始使用的位置编码。它涉及使用2D插值来生成位置嵌入，捕捉图像块之间的2D空间关系。
- 相对位置编码：对于给定的图像块对（其中一个作为查询，另一个作为注意机制中的键/值），我们有一个偏移量（pq − pk），其中每个偏移量都关联着一个嵌入（embedding）。然后，论文中进行额外的注意力运算，其中使用原始查询（查询内容），但是使用相对位置嵌入作为键。然后，将**相对注意力的逻辑值作为偏置项，并添加到主要注意力（基于内容的注意力）的逻辑值上，然后再应用softmax操作**

#### Attention maps

![attention maps](.\img\VIT\attention maps.png)

Attention maps是通过计算Transformer模型的注意力权重得到的。具体来说，这些attention maps显示了每个图像块与其他图像块之间的注意力分布。

注意力分布决定了每个图像块在处理其他图像块时的重要性。在Transformer中，**注意力是通过计算查询（query）和键（key）之间的相似度来得到的**。**较高的相似度意味着两个图像块之间的关联更强，因此，查询会更多地关注这些键**。

**为了获得注意力权重，通常会使用softmax函数将相似度值归一化为概率分布。这样，每个查询对所有键的注意力分布之和为1，这确保了每个图像块都能获得其他图像块的信息，同时保持了全局的一致性。**

注意力分布决定了Transformer中自注意力机制的输出。**在ViT中，这些自注意力权重可以直接可视化并显示为attention maps，以便了解模型在处理图像时对不同图像块之间的关注程度。**

在Figure 9中，ViT论文展示了不同层次的attention maps。这些maps显示了每个图像块与其他图像块的注意力分布，以及它们在不同层次的Transformer中的处理过程。这样的可视化有助于理解ViT模型如何在不同层次上学习图像中的信息，并捕捉图像块之间的关联

### Conclusion

- 提出了一个VIT模型
- 除了patch extraction step和position embedding，没有引入特定图像的归纳偏置，即没有使用到图像的2D信息。
- 在大数据集上pre-train可以取得超越state of art的性能
- 训练更cheap

展望

- VIT在检测和分割中的应用
- 探索自监督预训练方法



## Swin Transformer

### 论文地址

[Swin]: file://E:/研究生学习/论文/swin.pdf

### 期刊/会议

ICCV 2021

### Title

Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

### Author

Ze Liu, Yutong Lin, Yue Cao

### Abstract

#### 研究背景与问题

Transformer在CV中应用仍然存在两个问题

1.视觉实体规模尺度的变化

2.图像的resolution太大，计算量太大

#### 所提模型特点

提出了Shifted window（移动窗口）方法，将自注意力计算限制在非重叠的局部窗口，降低了序列长度。同时还允许cross-window connection（跨窗口连接），变相的带来了全局建模能力。

这种**分层架构**具有在各种尺度上建模的灵活性，并且具有相对于图像大小的线性计算复杂性，这为提出Swin V2做铺垫，可以在更大的分辨率图像上做预训练。

### Introduction

在CV中，仍然是以CNN为backbone的模型占主导地位，人们仍然通过扩大模型尺寸，使用更复杂的结构来提升CNN的性能。

引入Transformer到CV中，由于ViT已经做了这一点，作者指出可以将Transformer作为CV里所有任务通用的backbone。

#### Figure1

<img src=".\img\Swin\图1.png" alt="图1" style="zoom:67%;" />

对于**ViT**，**每一层Transformer block都是16倍下采样率，图像特征图是单一低分辨率**的。对于**多尺度的任务**（分割、检测等）**不太适用**。此外，**ViT的自注意力始终是在整张图上进行了计算，所以是一个全局建模，它的计算复杂度和图像关系成平方倍增长。**

对于Swin-T，它的特点：

- 它构建了**不同层级的特征图**。通过提出**patch merging（相邻的4个小patch合成1个大patch）**的方法，**在更深的layers中可以获得更大的感受野**。这样就获取了图像的多尺寸信息，就能**将这些多尺度特征图用于检测或者分割任务**中。

- **将self-attention限制在在局部小窗口中，其计算复杂度和图像大小是线性关系**。这一点利用了CNN的**locality的Inductive bias**（**局部性的先验知识**），即**同一个物体的不同部位**或者**语义相近的不同物体**会**大概率出现在相连的地方**，所以**在一个小范围local中计算的自注意力是够用的**。
- **使用shifted-window使得不同window之间的patch进行交互，提高模型的全局建模能力**。

#### Figure2

Swin-T设计的关键就是shifted window。

<img src=".\img\Swin\shifted window.png" alt="shifted window" style="zoom:50%;" />

**将每个window往右下角移动了2个patch。然后在新的window中再次分为四方格。这样使得window与window之间进行互动。因为原本的self-attention是限制在每个window中的，并且window之间是互不重叠的，因此它是无法和其他window中的patch产生关联的，这样就失去了全局建模能力。**

此外，**通过使用patch merging技术，在Transformer的后几层中，每个patch的感受野都变得很大了。再通过shift window，所谓的window local self-attention也就变相于全局self-attention了**。



**Swin-T利用了更多的视觉中的先验知识。在模型大一统上，仍然是ViT用的更好。ViT可以不加任何先验知识，直接利用Transformer在CV和NLP领域都用的很好，这样模型就能共享参数。**



### Related work

#### CNN and variants

AlexNet，VGG，GoogleNet，ResNet，DenseNet，HRNet，EfficientNet，深度卷积，可变性卷积

#### Self-attention based backbone architectures

一些工作使用self-attention替代CNN中的一部分结构，但是计算复杂度仍然很高。

#### Self-attention/Transformers to complement CNNs

使用self-attention的较好的长距离依赖建模能力来增强CNN

#### Transformer based vision backbone

ViT

DeiT

不适用于密集型的视觉任务中，且当图片分辨率很大时，计算复杂度会平方倍增长

### Method

#### Overall Atchitecture

![Swin-T结构](.\img\Swin\Swin-T结构.png)

- 假设输入图片尺寸是224x224x3。首先对图片进行**Patch Partition**。
- Patch Partition步骤中，**每个patch的size是4x4**。所以Patch Partition后就得到了**56x56x48**（56=224/4；48=4x4x3，即56x56个patch，每个patch_size=4x4）的特征图。
- 输入**Linear Embedding**，得到结果为**56x56xC（在Swin-T中C=96），展平后就变为3136x96**
- 由于3136这个序列对于Transformer来说太长了，所以Swin引入了window，只在window内做self-attention。**对于每个window，默认的patch数量是7x7=49个，所以序列长度只有49。**
- 通过**Swin Transformer Block计算的输出仍然为56x56x96**。
- 执行**Patch Merging**操作![Patch Merging示意图](.\img\Swin\Patch Merging示意图.png)

​	假设输入一个tensor为HxWxC，**如果下采样的倍数为2，那么Patch Merging就会每隔1个点选一个点进行Merging**，得到4个tensor，**输出的高宽缩小1倍，通道数变为4倍**。而**为了保持输出通道数是输入通道数的2倍 ，用了一个1x1的卷积将通道数缩小为2倍**。

- **执行完Patch Merging操作后，输出特征图变为28x28x192（即变成28x28个patch了，每个patch的size为8x8x3)**
- 重复多次，**得到7x7x768的特征图（即7x7个patch，每个patch的size为16x16x3）**
- **由于Swin-T没有像ViT那样加入CLS Token。所以它执行了全局平均pooling操作，将7x7取平均拉直为1，去做分类。**



#### Shifted Window based Self-Attention

##### Self-attention in non-overlapped windows

![self-attention in window](.\img\Swin\self-attention in window.png)

**假设输入tensor为56x56x96，那么图中每一个黄色格子就是一个window。每一个window包含MXM个patch（M默认=7）。**

##### MSA和W-MSA复杂度计算

**标准MSA的计算复杂度分析图**

![MSA复杂度计算](E:\深度学习\网络模型笔记\img\Swin\MSA复杂度计算.png)

- 如果输入是hw x C的tensor，那么q，k，v就是分别乘以3个C x C的系数矩阵得到的，每一个计算复杂度为hw x C<sup>2</sup>，那么计算复杂度就为3hwC<sup>2</sup>。
- 在计算Attention这一步，q和k<sup>T</sup>点乘，相当于乘上一个C x hw的矩阵，所以计算复杂度就为(hw)<sup>2</sup>C
- Attention和v的乘积，就是hw x hw * hw x C，那么计算复杂度为(hw)<sup>2</sup>C
- Proj层，hw x C * C x C，那么计算复杂度为hwC<sup>2</sup>
- 所以MSA总的计算复杂度就为4hwC<sup>2</sup>+2(hw)<sup>2</sup>C
- **公式表示**![W-MSA复杂度分析](.\img\Swin\W-MSA复杂度分析.png)

**对于W-MSA，此时h和w就变为M，那么带入MSA公式（1），可得一个窗口的MSA的计算复杂度**
$$
4M^2C^2+2M^4C
$$
**那么对于Swin-t，一共有hw/(MxM)个窗口，那么乘上以上公式，可得公式（2）**
$$
4hwC^2+2M^2hwC
$$
**对于MSA，复杂度随着hw的增大而呈平方关系倍数增大；而对于W-MSA，当M是一个固定值时，复杂度随着hw的增大而呈线性关系增大。**

**这样虽然减少了内存开销和计算量，但是这样的局部自注意力无法做到全局建模和通信**



**Swin Transformer的三个切入点**

- 为了解决CV中**层级式的问题**（不同尺度），提出了**Patch Merging操作**，像CNN一样**构建不同尺度的Transformer**。
- 为了**减少计算复杂度**，提出了**基于window的self-attention**操作。
- 为了**全局建模**和**不同window之间patch的交互**，提出了**shifted-window**的方法。





##### Shifted window partitioning in successive blocks

介绍了shifted window，主要是围绕图2介绍了交替使用W-MSA和SW-MSA的过程。

- **W-MSA负责将特征图划分为不同的窗口，例如8x8的特征图均匀划分为2x2=4个window，每个window有4x4(M=4)=16个patch**
- **SW-MSA将上一层的输入往右下角移动floor{M/2}个pixels（其实是patch，不是window)**



##### Efficient batch computation for shifted configuration

使用shifted window带来了2个问题：

1.窗口数量增加了

2.每个窗口的patch数量可能不一样。

**这样就无法把这些window压成一个batch去快速做self-attention**



**解决方法**

![Batch swmsa](.\img\Swin\Batch swmsa.png)

- 移位之后进行**cyclic shift，这样就保证了窗口数量不变**，**每个窗口的patch数量一致。**
- 但也产生了**新问题**，A,B,C三个窗口的有一部分patch是从原来相隔较远的window中移动过来的，它们之间不应该去做self-attention，所以做了一个**Masked MSA，让不相干的patch不进行self-attention计算（即图中一个window内不同颜色的patch不应该做self-attention**）。
- 做完Masked MSA后，将图重新**reverse cyclic shift**回去，为了**保持原来图片的相对位置和语义信息不变。**如果不把循环位移还原的话，那相当于在做Transformer的操作之中，一直在把图片往右下角移，不停的往右下角移，这样图片的语义信息很有可能就被破坏掉了。

##### Masked MSA

![Mask msa](.\img\Swin\Mask msa.png)



**虽然相隔较远的patch可能在某些层级上没有直接的self-attention联系，然后不对它们做self-attention可能会损失一部分全局信息。但是由于多尺度的存在，patch size会逐渐增加，到后面的layer中patch已经有了较大的感受野了（例如16x16），再利用shifted window融合不同window间patch的交互，仍然能捕捉到较好的全局信息。全局信息仍然能够在不同层级间传递和融合，使得模型整体上保留了一定程度的全局self-attention能力。这种折中的设计使得Swin Transformer在处理大尺寸图像时成为可能。**



#### Relative position bias

![relative pos idx](.\img\Swin\relative pos idx.png)
$$
Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V\\
B∈R^{M^2 * M^2};Q,K,V∈R^{M^2 * d}
$$


**相对位置索引，将4个图展平为行。但是作者实现时是用的一元索引。**

**二元转一元过程：**

1.偏移从0开始，行、列标加上M-1

2.行标乘上2M-1

3.行、列标相加

![relative pos index](E:\深度学习\网络模型笔记\img\Swin\relative pos index.png)

**relative position bias table**

每一个head的table

```python
 self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]
```

bias个数为(2M-1)X(2M-1)

<img src=".\img\Swin\pos 论文.png" alt="pos 论文" style="zoom:50%;" />

![relative pos table](E:\深度学习\网络模型笔记\img\Swin\relative pos table.png)

### Experiment

分别比较了Swin和其他state of art在ImageNet-1K 图像分类，COCO 目标检测和ADE20K语义分割任务上的性能。然后给出了消融实验。

#### Image Classfication on ImageNet 1K

##### Regular ImageNet-1K training

- AdamW optimizer
- 300 epochs using a cosine decay learning rate
- 20 epochs of linear warm-up
- batch size = 1024
- initial learning rate of 0.001
- weight decay = 0.05
- gradient clipping with a max norm of 1
- 正则化技术：RandAugment [17], Mixup [77], Cutmix [75], random erasing [82] and stochastic depth
- 对于更大的分辨率输入，在pre-train基础上微调训练30个epochs，学习率固定为10^-5，weight decay=10^-8



##### Pre-training on ImageNet-22K

- AdamW optimizer

- 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up

- batch size = 4096

- initial learning rate = 0.001

- weight decay = 0.01

-  In ImageNet-1K fine-tuning, we train the models for 30 epochs with a batch size of 1024, a constant learning rate of 10−5 , and a weight decay of 10−8

  

### Conclusion

1.所提的Swin-T可以构建层级式的特征图，并且计算复杂度相对于图像size是线性的。

2.希望 Swin Transformer 在各种视觉问题上的强大表现将促进视觉和语言信号的统一建模。

3.作为 Swin Transformer 的关键要素，基于shifted window的自注意力在视觉问题上被证明是有效且高效的，我们也期待研究其在自然语言处理中的使用



## GAN

### 论文地址

[GAN]: file://E:/研究生学习/论文/NIPS-2014-generative-adversarial-nets-Paper.pdf

### 期刊/会议



### Title

**Generative Adversarial Nets**

生成式对抗网络



### Author

Goodfellow



### Abstract

提出了一个新的框架 通过对抗的过程 来估计一个生成模型，同时训练2个模型。一个是生成模型G，G负责抓取数据分布；一个是判别模型D，负责估计样本是来自训练数据还是G生成的。

训练G是为了尽可能让D犯错。在任何函数空间中，都有一个唯一解，使得G能够生成的东西接近训练数据的分布，D基本上无法分辨。

在训练或生成样本期间不需要任何马尔可夫链或展开的近似推理网络。

### Introduction

深度学习在辨别模型中已经取得了很大进展，主要依赖于反向传播和dropout算法。

深度生成模型的影响较小，因为**难以近似最大似然估计**和**相关策略中出现的许多棘手的概率计算**，并且由于**难以在生成环境中利用分段线性单元的优势。**

介绍了GAN框架是什么

在GAN框架下的G和D都是一个MLP，G的输入是一个随机的noise（从高斯分布中取样）。G把这个noise映射到一个数据分布。



### Related work

#### 之前的工作

大多数都是去构造出一个分布函数，给这个函数一些参数使得它可以学习，并通过最大似然函数来训练。这样的问题是计算难度比较大，特别是高维的情况。

介绍了别人的工作：VAE

之前也有用辨别模型来训练生成模型：NCE。但NCE使用的损失函数更复杂，以至于在求解性能上没有GAN好。

介绍了GAN和对抗样本(Adversarial examples)的区别

Adversarial example：构造一些假的样本，跟真的样本很像，从而测试算法的稳定性。



### Adversarial nets

参考李宏毅的机器学习那部分讲解的笔记



### Theoretical Results

有一个全局最优解使得p_g = p_data

![理论1](.\img\GAN\理论1.png)

最终D_G会等于1/2



## BERT

### 论文地址

[BERT]: file://E:/研究生学习/论文/1810.04805BERT.pdf

### 期刊/会议



### Title

BERT:Pre-training of Deep Bidirectional Transformers Language Understanding

### Author

Jacob Devlin,Ming-Wei Chang

### Abstract

提出了BERT。BERT旨在通过**在所有层**中**联合调节左右上下文**来**预训练未标记文本的深度双向表示**

因此，BERT在fine-tune时只需要加一个额外的输出层即可获得很好的结果。

### Introduction

预训练语言模型对NLP的任务是很重要的，包括句子层面的任务：句子关系分析；词元层面的任务：命名实体识别，问答系统。

预训练模型在应用于特征表示的下游任务时，一般有两类方法：

- **feature_based：以ELMo为代表，针对每个任务构造相关的网络，把预训练的表示作为额外的特征，和输入一起进入网络中。ELMo使用了双向信息，但是它是基于RNN的，不适合fine-tuning**
- fine-tuning：以GPT为代表。把预训练好的模型用于下游任务上进行微调。**使用了Transformer的架构，但只能处理单向的信息**

这两种方法**使用相同的目标函数**，并且使用的是**单向的语言模型**来学习特征表示。

单向的语言模型限制了结构的选择。



**BERT随机mask输入的一些token（MLM），目标是预测出这些被masked的token。这样，MLM就能融合文本左右的信息。此外，作者还做了“next sentence prediction”的任务。**



### Related Work

#### Unsupervise Feature-based Approaches

介绍了word embeddings以及它的更粗粒度的扩展：sentence embeddings,paragraph embeddings

介绍了ELMo：双向语言模型

 

#### Unsupervised Fine-tuning Approaches

GPT

####  Transfer Learning from Supervised Data

在有标号的任务上进行迁移学习。

### BERT

#### BERT的两个步骤：

- pre-training：自监督训练，在没有标号的数据上进行了训练

- fine-tuning：用BERT预训练模型在有标号的数据上进行训练

![BERT两阶段](.\img\BERT\BERT两阶段.png)

#### 模型结构

BERT模型就是一个**多层的双向的Transformer编码器**。

##### Input/Output表示

BERT的输入可以是一个句子，也可以是一个句子对

使用了**WordPiece embeddings**。

- 首先使用**[CLS]**这个token。该token在每个sequence的**第一个位置**，**它的最终隐藏状态对应汇聚了序列的特征表示**
- 用**[SEP]**分开不同的sentences，然后给每个token添加一个可学习embedding

![BERT输入组合](.\img\BERT\BERT输入组合.png)

**BERT的输入包括token embeddings，segment embeddings（区分这个token属于哪个sentence）和Position Embeddings**



#### Pre-training BERT

随机MASK 15%的tokens，然后只预测这些被masked的words是什么，而不是恢复整个序列。

对于这15%的mask住的tokens，对其中的80%替换成[MASK]符号，10%替换成随机words，10%什么都不变



#### Fine-tuning BERT

BERT把整个sentence pair都放进去了，所以self-attention能看到两端的信息。

针对下游任务，只需要设计输入和输出

### Experiments



### Conclusion

无监督的预训练模型是许多语言理解系统任务中不可或缺的一部分。这样能使得训练资源不多的任务也能享受深度神经网络。BERT主要就是把前人的结果拓展到深的双向结构，使得一个pre-train模型能处理不同的NLP任务。

## MAE

### 论文地址

[MAE]: file://E:/研究生学习/论文/2111.06377MAE.pdf

### 期刊/会议

### Title

**Masked** Autoencoders Are **Scalable** Vision Learners

带掩码的自编码器是一个可拓展的视觉学习器

**取标题时（Scalable和Efficient二选一）**

### Author

Kaiming He，Xinlei Chen —— Facebook AI Research

### Abstract

MAE的方法：随机盖住输入图片的一些patches，然后重构这些patches

#### 两个核心的设计：

- 一个非对称的encoder-decoder结构，encoder只操作那些可见的patches（没有被masked）。一个轻量的decoder，负责从latent representation和mask tokens中恢复原始图片
- mask大量的patches可以得到一个非显在的且有意义的自监督任务
- 合并这两个设计可以高校训练大模型。

### Introduction

#### 示意图

<img src=".\img\MAE\MAE示意图.png" alt="MAE示意图" style="zoom:50%;" />

- 一张图片分成patches
- 将大量的patches盖住（灰色格子）
- 没有被盖住的patches被输入到encoders中得到特征
- 将盖住的patches和特征拼在一块（按原图顺序）
- 输入decoder进行重构恢复
- 在应用于下游任务时，只需要encoders，不需要decoder

CV已经在监督学习上可以训练大模型了，但在自监督学习上仍然没有很好的办法。

举例了NLP领域中基于自监督学习的大模型（GPT、BERT）

#### 是什么使得masked autoencoders在CV和NLP领域上处理也不一样呢

- 卷积神经网络的卷积窗口不好将mask tokens或者位置编码放入网络中（即**CNN无法将这个特定的mask tokens提出来，导致这个mask tokens在之后难以还原**）
- **信息密度差距**。在语言中，一个词有很丰富的语义和密集的信息。对于图片，由于像素之间存在冗余，所以一个被mask的patch可以通过附近的patch进行插值来恢复。作者就使用非常高比率将很多块给mask住。使得模型去学习全局的信息，而不是关注于局部的恢复
- decoder不同。在NLP中还原的是词，语义层面较高，使用一个MLP就能还原。而对于图片，还原的是pixel，是低层次的语义层面，仅靠一个MLP是不够的

**这篇文章的Introduction不仅讲了做了什么，还重点以问答方式讲了为什么这么做**

### Related Work

#### Masked language modeling

BERT GPT

#### Autoencoding

DAE

#### Masked image encoding

iGPT

BEiT

#### Self-supervised learning

Constrastive learning（对比学习），主要依靠数据增强

#### 总结

没有强调MAE和这些工作的区别，在写作时应该强调区别

### Approach

#### Masking

- 将图片分patches
- 随机采样一些patches进行保留，其余的全部mask

#### MAE encoder

一个ViT，但仅用于可见的patch，大量减少了训练开销

#### MAE decoder

输入是latent representation+mask token

所有的mask token是一个共享的、可学习的vector，它代表了要预测的missing patch

加入了positional embedding

用于其他任务时，不需要使用decoder

#### Reconstruction target

decoder的最后一层时一个linear layer。如果时是16x16，linear layer会投影到256的空间，然后reshape为重构图片。

损失函数为MSE

只在masked patches上计算loss





### Conclusion

作者的工作在ImageNet数据集上，通过一个自编码器可以学习到可以媲美有标号的效果

尽管图片的语义信息没有NLP那样明显，但是MAE仍然能取得很好的效果，它确实能够学习到一个隐藏的比较好的语义表达。

### 总结

- 利用ViT来做跟BERT一样的自监督学习
- ViT已经做过这个事了，MAE进行了拓展
  - MASK的块要更多，使得剩下的那些块，块与块之间的冗余度没那么大，使整个任务变得更加复杂
  - 使用Transformer的decoder，使得恢复信息更加简单
- 加上ViT之后的各种技术，使得MAE的训练更加鲁棒
- **简单的想法+非常好的结果+详细的实验**





## FGSM

### 论文地址

[fgsm]: file://E:/研究生学习/论文/研0/1412.6572_fgsm_goodfellow.pdf

### 期刊/会议

2015 ICLR

### Title

Explaining and harnessing adversarial examples

解释和利用对抗性样本

### Author

Ian.J.Goodfellow@google



### 论文主要创新点

- 创造出一种用于深度网络的通用攻击方法，同时针对该方法创立了对应的防御方案
- 作者通过分析现代神经网络内部结构，发现现有的网络结构大多数是线性或近似线性，而这样的网络对于样本的细微差别会有很大的反应。

### Abstract

包括神经网络在内的几种机器学习模型**很容易对adversarial examples进行错误分类**。这些对抗样本是**通过对数据集中的样本应用较小但蓄意的会导致最坏情况的扰动而形成的输入**，因此，**被扰动的输入导致模型以高置信度输出错误的答案**。

早期解释这种现象主要关注于非线性和过拟合。相反，**我们认为神经网络容易受到对抗性扰动的主要原因是它们的线性特性。**

这种解释得到新的定量结果的支持，同时对有关它们的最有趣的事实进行了首次解释：**它们在体系结构和训练集之间的概括**。而且，这种观点产生了一种**简单快速的生成对抗样本的方法**。使用这种方法**为对抗训练提供示例**，我们减少了MNIST数据集上maxout网络的测试集错误

### Introduction

Szegedy等人做出了一个有趣的发现：几种机器学习模型（包括最新的神经网络）**容易受到对抗样本的攻击**。也就是说，这些机器学习模型对这些样本进行了错误分类，这**些样本与从数据分布中得出的正确分类的样本仅稍有不同**。在许多情况下，在训练数据的不同子集上训练的、具有不同体系结构的各种模型都会将对抗样本错误分类。这表明**对抗样本暴露了我们训练算法中的基本盲点**。

这些对抗样本的原因仍然是一个谜团。一种推测性解释是由于深度神经网络的极端非线性性，也许是由于模型平均不足和纯监督学习问题的正则化不足所致。我们证明这些推测性假设是不必要的。**高维空间中的线性行为足以引起对抗样本**。这种观点**使我们能够设计一种快速生成对抗样本的方法**，从而使对抗性训练切实可行。我们表明，**对抗性训练可以提供dropout以外的正则化收益**。通用的正则化策略（例如dropout、预训练和模型平均）并不能显着降低模型对付对抗样本的脆弱性，但改用非线性模型族（如RBF网络）可以做到。

我们的解释表明，**设计模型因其线性而易于训练，而设计模型则利用非线性效应来抵抗对抗性扰动，这之间存在根本的矛盾**。从长远来看，**通过设计可以成功地训练更多非线性模型的更强大的优化方法，可以避免这种折衷**。

### Related work

Szegedy等人展示了神经网络和相关模型的多种有趣特性，与本文最相关的是：

- Box-constrained L-BFGS可以可靠地找到对抗样本；
- 在某些数据集（例如ImageNet）上，**对抗样本与原始样本是如此接近，以至于人眼无法区分这些差异**；
- 在许多情况下，**在训练数据的不同子集上训练的、具有不同体系结构的各种模型都会将对抗样本错误分类**；
- Shallow softmax回归模型也容易受到对抗样本的影响；
- 对抗样本的训练可以使模型正规化，但是，由于需要在内部循环中进行昂贵的约束优化，因此这在当时不切实际；

**现代机器学习模型——potemkin village（表面上很厉害，但是内部很脆弱）**

尽管线性分类器具有相同的问题，但通常将这些结果特别解释为深度网络中的缺陷。我们认为对这一缺陷的了解是修复它的机会。



### The Linear Explanation of Adversarial Examples

许多问题中，单个输入特征的精度受到限制。 例如，**数字图像通常每个像素仅使用8位，因此它们会丢弃低于动态范围1/255的所有信息**。

**如果扰动 η 的每个元素都小于特征的精度，分类器对输入x的响应与对抗输入 x~=x+η 的响应是不合理的**
$$
W_T\tilde{x}=W^Tx+W^T\eta
$$

$$
对抗性扰动会导致增长W^T\eta，通过赋予\eta=sign(w)，可以在\eta上受到最大范数约束的情况下最大化此增量\\
$$

**如果简单线性模型的输入具有足够的维度，则可以具有对抗样本。**



#### 总结

在该小节中，作者探讨了当输入数据空间被线性划分时，为什么机器学习模型容易受到对抗性示例的影响。具体来说，**如果一个机器学习模型是由一组线性函数组成的，那么输入空间会被分割成一系列的决策区域。然后，通过微小的扰动，可以将输入数据移动到不同的决策区域，从而导致模型产生不正确的输出**

这种现象在一些基于线性模型或者在输入空间被线性划分的情况下尤为明显。然而，即使是深度神经网络等复杂模型，**当其在高维空间中变得足够线性时，也可能容易受到对抗性示例的攻击**



### Linear Perturbation of Non-Linear Models

线性模型的廉价的分析性扰动也会破坏神经网络。

#### 最优最大范数约束下的扰动

**optimal max-norm constrained pertubation**
$$
\eta=\varepsilon sign(\nabla_xJ(\theta,x,y))\\
\theta为模型参数，x为模型输入，y为和x关联的target，J(\theta,x,y)是loss函数
$$
**作者将其称为生成对抗样本的”快速梯度符号方法“(fast gradient sign method,FGSM)**

![figure1](.\img\FGSM\figure1.png)

### 补充：Attack Approach

#### 基本原理

<img src=".\img\FGSM\attack approach2.png" alt="attack approach2" style="zoom:33%;" />

<img src=".\img\FGSM\attack approach.png" alt="attack approach" style="zoom:33%;" />

- 和训练一个模型一样，在每个iteration中都会计算梯度
- 不过此时的梯度不是模型的参数对于loss的梯度，而是输入的x（很长的vector）对于loss的梯度
- 不过x是有限制的，这里是d(x<sup>0</sup>,x)<=ε，因此需要判断
- 假设用的限制d是L-∞，x0在图中的位置，那么x_t的位置只能在蓝色框之内
- 当x_t的位置超出蓝色框时，就需要fix拉回来

#### 基于梯度的攻击方法很多，但它们都基于两个基本思路

- 对约束标准做改动，如L2-norm，L-infinity等

- 对优化方案做改动

![base_idea](.\img\FGSM\base_idea.png)



### FGSM原理

#### FGSM基本过程

FGSM方法通过计算输入数据的损失函数对输入数据的梯度，然后用符号函数sign将梯度值变为+1或者-1。将输入按照梯度方向加上一定的扰动，从而生成对抗样本。这些对抗样本人眼很难看出，但是很容易引起神经网络错误分类的高置信度。

#### 如何对样本添加噪音从而造成网络误分类

对于一个样本来说，其分类结果是由深度网络中大量参数和激活函数的形式所决定的，**如果以某种方式改变样本使得激活函数朝着反方向变化**，那么这种变化会形成“雪球效应”使得分类器改变最终的分类概率。

<img src=".\img\FGSM\fgsm示意图.png" alt="fgsm示意图" style="zoom:50%;" />

- FGSM将网络参数θ保持不变，损失函数L对原始输入x<sup>0</sup>进行求导，若值为正数，则取1；值为负数，则取-1，得到Δx
- 这样梯度要么为1，要么为-1，再将学习率替换为扰动约束ε（一个很大的值）
- 扰动约束ε为L-infinity时，扰动空间就是一个正方形
- 这样得到的x就一定落在四个角落上，一步到位，不会超出限制



#### 为什么这样做有攻击效果

就结果而言，攻击成功就是模型分类错误，**就模型而言，就是加了扰动的样本使得模型的loss增大**。 而**所有基于梯度的攻击方法都是基于让loss增大这一点来做的**

FGSM产生的对抗样本之所以能够使得模型误分类，是因为它利用了深度神经网络中的**线性性质和梯度信息**来**引导输入数据的微小扰动**，从而改变模型的预测结果。

FGSM的**核心思想**是在**保持对抗样本与原始样本相似性**的前提下，**找到能够最大程度地改变模型预测结果的最小扰动。**

#### 为什么不直接使用导数，而要用符号函数求得其方向

1. FGSM是典型的无穷范数攻击，那么我们**在限制扰动程度的时候，只需要使得最大的扰动的绝对值不超过某个阀值即可**。 而我们**对输入的梯度，对于大于阀值的部分我们直接clip到阀值**，对于小于阀值的部分，既然对于每个像素扰动方向只有+-两个方向，而现在方向已经定了，那么为什么不让其扰动的程度尽量大呢？ 因此**对于小于阀值的部分我们就直接给其提升到阀值，这样一来，相当于我们给梯度加了一个符号函数了**
2. 由于FGSM这个求导更新只进行一次，如果直接按值更新的话，可能生成的扰动改变就很小，无法达到攻击的目的，因此我们只需要知道这个扰动大概的方向

#### FGSM进一步解释

FGSM的原作者在论文中提到，神经网络之所以会受到FGSM的攻击是因为：

1. **扰动造成的影响在神经网络当中会像滚雪球一样越来越大，对于线性模型越是如此**。 而目前神经网络中倾向于使用Relu这种类线性的激活函数，使得网络整体趋近于线性。 
2. **输入的维度越大，模型越容易受到攻击**。

<img src=".\img\FGSM\fgsm-1.png" alt="fgsm-1" style="zoom:50%;" />

<img src="E:\深度学习\网络模型笔记\img\FGSM\fgsm-2.png" alt="fgsm-2" style="zoom:50%;" />

可以看到，对于一个简单的线性分类器，loss对于x的导数取符号函数后即w，**即使每个特征仅仅改变0.5，分类器对x的分类结果由以0.9526的置信概率判断为0变成以0.88的置信概率判断为1.**　　　



#### FGSM存在的问题

- 如果**激活函数是非线性函数**，那么，会导致**在ϵ的范围内，找不到合适的扰动骗过分类器**。其原因在于：线性函数或近似线性函数对于X的梯度是基本恒定的，因此只要沿着梯度方向，一定可以以最小的代价最快的速度增大loss，但是，**非线性函数对于每一点的梯度都不一样**。
- 对于界限ϵ，需要自己确定

#### FGSM的改良

- Iterative FGSM
- PGD：PGD是FGSM的一种迭代版本。它对抗性地优化输入数据，进行**多次迭代**，**每次都在一定范围内向着梯度的方向移动一小步**。PGD通常比FGSM更强大，因为它可以生成更具挑战性的对抗性样本。
- DeepFool：DeepFool是另一种有效的对抗性样本生成方法。它利用线性化的方式来寻找最小扰动，使得模型的输出从正确分类到错误分类。
- CW攻击：Carlini和Wagner攻击是一种强大的对抗性样本生成方法。它通过最小化添加的扰动的范数，并在约束条件下找到最小扰动，以使得模型误分类。

### Adersarial Training of Linear Models Versus Weight Decay

**线性模型与权重衰减的对抗训练**



### Adersarial Training of Deep Networks

Szegedy等人(2014b)表明，通过**训练对抗样本和干净样本的混合，可以对神经网络进行一定程度的正则化**。关于对抗样本的训练与其他数据增强方案有所不同。通常，人们会**使用转换（例如预期在测试集中实际发生的转换）来扩充数据**。相反，这种形式的数据增强使用了不太可能自然发生的输入，但暴露了以模型概念化其决策功能的方式的缺陷。

作者使用了两个方法提高了性能

- 加大模型容量，每层使用1600个单元。
- 在对抗性验证集错误上使用了early stopping，使用此标准来选择要训练的时期数，然后我们对所有60000个示例进行了再训练。使用随机种子生成器的五种不同种子进行五次不同的训练，这些随机数生成器用于选择小样本训练样本，初始化模型权重，并生成dropout掩码。

**模型还具有了对抗对抗样本的能力**。在没有对抗训练的情况下，基于快速梯度符号法的同类样本中，这种模型的错误率为89.4％。经过对抗训练，错误率降至17.9％。

对抗样本可在两个模型之间转移，但**对抗性训练的模型显示出更高的鲁棒性**。通过原始模型生成的对抗样本在对抗训练模型上的错误率为19.6％，而通过新模型生成的对抗样本在**原始模型（没有进行对抗训练**）上的错误率为40.9％。

当经过对抗训练的模型对一个对抗样本进行了错误分类时，其预测confidence仍然很高。错误分类示例的平均置信度为81.4％

 当数据受到对抗样本的干扰时，对抗训练过程可以看作是使最坏情况的错误最小化。

讨论了在输入加扰动还是在隐藏层加扰动比较好，作者认为如果模型具有对抗对抗样本的能力，那么在隐藏层中加入扰动也能起到正则化的作用，不然的话还是在输入加扰动更好。

### Conclusion

1. 对抗样本可以解释为高维点积的属性，它们是模型过于线性而不是非线性的结果；
2. 可以将对抗性示例在不同模型之间的泛化解释为：对抗性扰动与模型的权重向量高度一致，并且不同模型在训练以执行相同任务时会学习相似的功能；
3. 最重要的是扰动的方向，而不是空间中的特定点。空间中没有充满对抗样本的口袋，这些对抗样本像有理数一样精确地贴图了实数；
4. 因为这是最重要的方向，所以对抗性扰动会在不同的干净示例中进行概括；
5. 我们介绍了一系列用于生成对抗样本的快速方法；
6. 我们已经证明，对抗训练可以导致正则化； 比dropout更正则化；
7. 我们进行的控制实验无法使用更简单但效率更低的正则器（包括L1权重衰减和添加噪声）来重现此效果；
8. 易于优化的模型很容易受到干扰；
9. 线性模型缺乏抵抗对抗性扰动的能力。仅应训练具有隐藏层的结构（适用通用逼近定理），以抵抗对抗性扰动；
10. RBF网络可抵抗对抗样本；
11. 受过训练以模拟输入分布的模型不能抵抗对抗样本；
12. 集成模型不能抵抗对抗样本；



## PGD

### 论文地址

[pgd]: file://E:/研究生学习/论文/研0/1706.06083_pgd.pdf

### 期刊/会议

ICLR 2018

### Title

 **Towards Deep Learning Models Resistant to Adversarial Attacks**



### Author

MIT



### 论文主要创新点

### Abstract

为了解决神经网络易受对抗样本攻击的问题，作者通过鲁棒优化的角度研究了神经网络的对抗鲁棒性。

构建了一种具体的安全保证，可以防止对抗攻击。

这些方法使作者可以训练网络，使其对各种对抗性攻击的抵抗力大大提高。

提出了针对一阶攻击的安全概念

### Introduction

怎样训练一个对对抗攻击有较好鲁棒性的模型

如何训练对对抗样本具有鲁棒性的深度神经网络是一个比较重要的问题。现在有相当多的工作为对抗性环境提出各种攻击和防御机制：

- 防御性蒸馏（defensive distillation）
- 特征压缩（feature squeezing）
- 其他对抗样本检测方法

我们永远无法确定给定的攻击会在上下文中找到“最具对抗性”的例子，或者特定的防御机制会阻止某些定义明确的对抗性攻击的存在



作者从**鲁棒优化的角度研究神经网络的对抗鲁棒性**。使用自然的**鞍点（min-max）公式**来原则上捕获针对对抗攻击的安全性概念。这种表述使我们能够**精确地确定我们想要实现的安全保证的类型**，即**我们想要抵抗的广泛攻击类型（与仅防御特定的已知攻击相反）**。这种表述还使我们**能够将攻击和防御都放到一个通用的理论框架中**（不同于以往的用于对抗样本上）。**对抗训练直接对应于优化该鞍点问题**。

#### 主要贡献

- **对与该鞍点公式相对应的优化空间进行了仔细的实验研究**。尽管其组成部分具有非凸性和非凹性，但我们发现**根本的优化问题是可解决的**。特别是，我们提供了有力的证据，证明一阶方法可以可靠地解决此问题。我们用来自真实分析的思想补充这些见解，以进一步激发**投影梯度下降（Project Gradient Descent,PGD）**作为**通用的“一阶攻击”**，即，利用有关网络的本地一阶信息的最强攻击。
- **模型容量对对抗鲁棒性起着重要的作用**。**为了可靠地抵抗强大的对抗攻击，网络需要的容量要比仅正确分类良性样本的容量更大**。这表明，**鞍点问题的鲁棒决策边界比仅将良性数据点分开的决策边界要复杂得多**。
- 基于以上见解，我们在MNIST和CIFAR10上训练了网络，这些网络可抵抗各种对抗性攻击。我们的方法**以优化上述鞍点公式为基础**，并**将 PGD 作为可靠的一阶对抗攻击**

### An Optimization View on Adversarial

#### 对抗鲁棒性的优化视角

假设data服从D分布，标签y同理，loss function L(θ,x,y)是cross entropy，θ是model parameters。训练模型就是求解一个最优参数θ，使得loss function最小化，即最小化经验风险(Empirical risk minimization，ERM)

尽管ERM在很多任务上取得了成功，但是却不能增加模型的鲁棒性。

为了提高鲁棒性，需要适当增强ERM范式。我们的方法不是采用直接专注于提高针对特定攻击的鲁棒性的方法，而是**首先提出对抗性鲁棒模型应满足的具体保证，然后通过训练实现这一保证**。

取得这种保证的第一步是指定攻击模型，即**精确定义我们的模型应该抵抗的攻击**。对于每个数据点 x ，我们引入一组允许的扰动 S⊆R<sup>d</sup> ，以正规化算法的操纵力。在图像分类中，我们选择 S 以便捕获图像之间的感知相似性。

#### min-max最优化框架

接下来，通过结合算法来修改结构风险的定义，在输入中加入扰动，这就产生了以下鞍点问题（2.1）：

![鞍点问题](.\img\PGD\鞍点问题.png)

我们的观点源于将鞍点问题视为**内部最大化问题和外部最小化问题的组合**。

- **内部最大化旨在找到给定数据样本x的对抗样本δ（δ∈S），以获得尽可能高的loss（这正是攻击神经网络所期望的：loss越大，说明对抗攻击越有效）。**
- **外部最小化是旨在找到模型参数θ，以使得内部攻击问题给定的`对抗损失`最小化（这正是使用对抗训练技术训练鲁棒性模型所期望的）。**

**鞍点问题指定了理想鲁棒分类器应该实现的明确目标，以及对其鲁棒性的定量度量**。特别是，当参数 θ 产生（几乎）消失的风险时，相应的模型对于我们的攻击模型指定的攻击具有完全鲁棒性。

本文**在深度神经网络的背景下研究了该鞍点问题的结构**，然后，这些调查将我们带到了一种训练技术上，**该训练技术可以产生对各种对抗性攻击具有高度抵抗力的模型**。

相关工作上，攻击部分回顾了FGSM和I-FGSM，防御部分回顾了对抗训练。

#### PGD和BIM的区别

**Kurakin等人(2016)提出的BIM方法是FGSM的迭代版本. **

**Athalye等人（2018）发现的PGD攻击是BIM的变体**, **它以均匀的随机噪音作为初始化**,  并且是最强大的一阶攻击方法之一. 

**PGD只是比BIM添加了一个初始的随机扰动了**。



### Towards Universally Robust Network

#### 面向通用的鲁棒网络

当前关于对抗样本的工作通常集中于特定的防御机制，或针对这种防御的攻击。公式（2.1）的一个重要特征是，**获得较小的对抗损失即可保证没有攻击可以欺骗网络**。根据定义，没有对抗性扰动是可能的，因为对于我们的攻击模型所允许的所有干扰，损失很小。

不幸的是，尽管由鞍点问题提供的总体保证显然是有用的，但**我们能否在合理的时间内找到一个好的解决方案尚不清楚**。**解决鞍点问题（2.1）涉及同时解决非凸外部最小化问题和非凹内部最大化问题**。

#### 对抗样本的前景展望

为了更详细地了解内部最大化问题，作者研究了MNIST和CIFAR10上多个模型的局部最大值。作者使用的是PGD方法（一种大规模约束优化的标准方法）

作者的实验表明，从一阶方法的角度来看，内部最大化问题是可以解决的。

##### 一些实验观察到的现象

- 作者观察到，**当对 x+S 内随机选择的起始点执行投影的 L-∞ 梯度下降时，攻击算法所实现的损失会以相当一致的方式增加，并迅速达到平稳状态**。下图是从MNIST和CIFAR10评估数据集创建对抗样本时，交叉熵损失值的变化。

![loss变化](.\img\PGD\loss变化.png)

- **每次运行都在同一自然样本周围的L-∞球中的均匀随机点处开始。几次迭代后，攻击算法损失平稳**。**优化轨迹和最终损失值也相当聚集**，尤其是在CIFAR10上。而且，**经过对抗训练的网络上的最终损失值明显小于其标准对应网络上的损失；**

- 进一步研究最大值的集中度，我们观察到，**在大量的随机重新启动过程中，最终迭代的损失遵循高度集中的分布，而没有极端的异常值**。下图是对于MNIST和CIFAR10评估数据集中的五个样本，由交叉熵损失给出的局部最大值的值。对于每个样本，我们从样本周围的 L-∞ 球中的105个均匀随机点开始投影梯度下降（PGD），然后迭代PGD直到损失达到平稳。**蓝色直方图对应于标准网络上的损失值，而红色直方图对应于经过对抗训练的对应方。 对于经过对抗训练的网络，损失明显较小，并且最终损失值非常集中，没有任何异常值。**![loss集中问题](.\img\PGD\loss集中问题.png)
- 为了证明最大值明显不同，我们还测量了它们之间的L2 距离和角度，并观察到**距离分布接近 L∞ 球中两个随机点之间的预期距离**，**并且角度接近 90∘** 。**沿着局部最大值之间的线段，损失值是凸的，在端点处达到最大值，并在中间减小一个恒定因子。但是对于整个段，其损失值要比随机点的损失值高得多**。
- 最后，我们观察到最大值的分布表明最近开发的对抗样本子空间视图不能完全捕获攻击的丰富性。尤其是，我们观察到**对抗性扰动的负内积与样本的梯度有关**，并且随着扰动规模的增加，与梯度方向的总体相关性会降低。



#### 一阶攻击方法

**一阶攻击方法**是对抗性攻击中的一种类型，它**只使用了输入样本的一阶梯度信息**。**在一阶攻击中，攻击者仅考虑损失函数对输入样本的一阶导数，即关于输入样本的梯度**。例如FGSM

 我们的实验表明，**对于正常训练的网络和对抗训练的网络**，**PGD发现的局部最大值都具有相似的损失值**。这种集中现象提出了一个有趣的问题，即**针对PGD攻击的鲁棒性会产生针对所有一阶攻击算法的鲁棒性，即仅依赖一阶信息的攻击**。**只要对手仅使用损失函数相对于输入的梯度，我们就可以推测，它不会找到比PGD更好的局部最大值**。

**我们的实验表明，使用一阶方法很难找到这样更好的局部最大值：即使大量随机重启也找不到具有明显不同损失值的函数值**

**在某种意义上，依赖于一阶信息的攻击对于当前的深度学习实践来说是普遍的**

**如果我们训练网络使其对PGD 对抗具有强大的抵抗力，那么它将对包括所有当前方法的广泛攻击具有强大的抵抗力**。

在黑盒攻击环境下鲁棒性更好。

#### 对抗训练的下降方向

前面的讨论表明，通过应用PGD可以成功解决内部优化问题。接下来讨论了公式2.1外部最小化问题，即找到使“对抗性损失”（内部最大化问题的值）最小的模型参数。

**计算外部问题的梯度 ∇θρ(θ) 的一种自然方法是在内部问题的最大值处计算损失函数的梯度。这相当于用输入点的对应对抗性扰动代替输入点，并通常在扰动的输入上训练网络**。



![外部问题loss变化](.\img\PGD\外部问题loss变化.png)

训练过程中对抗样本的交叉熵损失。这些图显示了在针对PGD攻击训练MNIST和CIFAR10网络期间，训练样本中的攻击损失如何演变。CIFAR10图中的急剧下降对应于训练步长的减小。这些图说明，我们可以不断降低鞍点公式（2.1）的内部问题的值，从而产生越来越强大的分类器。

### Network Capacity and Adversarial Robustness

#### 网络容量和对抗鲁棒性

 成功地从等式（2.1）解决问题不足以保证稳健而准确的分类。

对抗性强健的深度学习模型。对于可能扰动的固定集合 S ，问题的值完全取决于我们正在学习的分类器的体系结构。因此，**模型的架构能力成为影响其整体性能的主要因素**。在高层次上，以鲁棒的方式对样本进行分类需要更强大的分类器，因为对抗样本的存在将问题的决策边界更改为更复杂的样本

![模型容量](.\img\PGD\模型容量.png)

上图标准与对抗性决策边界的概念性说明。左：一组点，可以用一个简单的（在这种情况下为线性）决策边界轻松分离。中：简单决策边界不会将数据点周围的 L∞ 球（此处为正方形）分开。因此，有一些对抗样本（红色星号）将被错误分类。右图：分离 L∞ 球需要一个更加复杂的决策边界。所得的分类器对于带有有限 L∞ 范数扰动的对抗样本具有鲁棒性。

对于MNIST数据集，我们考虑一个简单的卷积网络，并研究其行为如何随着不断扩大的网络规模（即，将卷积过滤器的数量和全连接的层的大小增加一倍）而针对不同的攻击而发生变化。初始网络具有一个带2个过滤器的卷积层，然后是另一个具有4个过滤器的卷积层，以及一个具有64个单元的全连接的隐藏层。卷积层之后是2×2最大池化层，而对抗样本的 =0.3 。 结果如图4所示

![模型容量实验结果](.\img\PGD\模型容量实验结果.png)

 **对于CIFAR10数据集，我们使用了ResNet模型。**我们使用**随机裁剪和翻转以及每个图像标准化来执行数据增强**。**为了增加容量，我们修改了网络，该网络合并了较宽的层。这将导致网络中包含5个残差单元，每个残差单元分别具有（16、160、320、640） 个滤波器**。 使用自然样本进行训练后，该网络可以达到95.2％的准确性，对抗样本的 ε=8 。

作者观察到的一些现象：

- **容量的作用**。我们观察到，仅使用自然样本进行训练时（除了提高这些样本的准确性之外），**增加网络的容量会提高针对单步扰动的鲁棒性**。当考虑具有较小  ε的对抗样本时，此效果会更大；
- **FGSM生成的样本不能增加鲁棒性**。当使用FGSM生成的对抗样本训练网络时，我们观察到**网络对这些对抗样本过拟合**。这种行为被称为标签泄漏，它源于以下事实：攻击算法产生了非常有限的一组对抗样本，网络会过拟合。
- 对于小容量网络，尝试针对强大的攻击（PGD）进行训练**会阻止网络学习任何有意义的信息**。即使可以通过标准训练收敛到准确的分类器，网络仍然会收敛到预测固定的分类。**网络的小容量迫使训练过程牺牲自然样本的性能，以提供对抗对抗输入的任何鲁棒性**。
- 随着容量的增加，鞍点问题的损失减小。固定攻击的模型并对其进行训练，随着容量的增加，（2.1）的值会下降，这表明该模型可以更好地拟合对应的对抗样本
- 更大的容量和更强大的攻击算法会降低可转移性。要么增加网络的容量，要么对内部优化问题使用更强大的方法，都会降低所传递对抗性输入的有效性。

### Experiment：Adversarially Robust Deep Learning Models

 到目前为止，我们的实验表明，我们需要关注两个关键要素：**a）训练足够高的容量的网络，b）使用最强大的攻击算法**

对于MNIST和CIFAR10，选择的攻击算法将是随机梯度下降（PGD），从围绕自然实例的随机扰动开始。这对应于我们的“完整”一阶攻击的概念，**该算法可以仅使用一阶信息就可以有效地最大化样本的损失**。由于我们正在为多个epoch训练模型，因此每批多次重新启动PGD并没有好处。下次遇到每个样本时，都将选择新的开始。

 在针对该攻击方法进行训练时，**我们观察到对抗样本的训练损失不断减少。此行为表明我们确实在训练过程中成功解决了最初的优化问题**

#### 所对比的模型

![不同的模型](.\img\PGD\不同的模型.png)

#### MNIST上的实验

- 40 iterations of PGD
- step size = 0.01
- ε=0.3
- 两层卷积结构（分别有32和64个filters），每层跟随一个2x2 的最大池化，最后的fully connected layer 有1024个单元

![MNIST上的实验](.\img\PGD\MNIST上的实验.png)

#### CIFAR10上的实验

- orginal ResNet and its 10x wider variant
- 7 steps of size 2
- ε = 8
- 对于最厉害的adversary，用了20 steps

![CIFAR10上的实验](.\img\PGD\CIFAR10上的实验.png)

#### 补充了L-∞和L-2对比的实验

L-∞训练出来的模型更加鲁棒

![L2和L∞对比](E:\深度学习\网络模型笔记\img\PGD\L2和L∞对比.png)



### Conclusion

我们的发现提供了证据，表明**可以使深度神经网络抵抗对抗攻击**。正如我们的理论和实验所表明的，我们**可以设计可靠的对抗训练方法**。背后的关键见解之一是**基础优化任务的意外规则结构**：即使相关问题对应于具有许多不同局部最大值的高度非凹函数的最大化，它们的值也高度集中。总体而言，我们的发现使我们希望，对抗性强健的深度学习模型可能在当前范围之内

对于MNIST数据集，我们的网络非常鲁棒，可以针对各种强大的 L-∞ 约束攻击算法和大扰动实现高精度。我**们在CIFAR10上的实验尚未达到相同的性能水平**。但是，我们的结果已经表明，我们的技术大大提高了网络的健壮性。我们认为，进一步探索这一方向将为该数据集带来强大的对抗网络。
